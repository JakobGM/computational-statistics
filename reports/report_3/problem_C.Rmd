# Problem C: The EM-algorithm and bootstrapping

```{r, include = FALSE}
# Include parent to get its headers
library(knitr)
set_parent("base.Rmd")
```

## Description

Let $\vec{x} := [x_1, ..., x_n]$ and $\vec{y} := [y_1, ..., y_n]$ be two collections of independent random variables from two independent exponential distributions

\begin{align*}
  x_1, ..., x_n &\sim \text{exp}(\lambda_0) \\
  y_1, ..., y_n &\sim \text{exp}(\lambda_1).
\end{align*}

Denote the probability density functions of these distributions as $f_x(x | \lambda_0)$ and $f_y(y | \lambda_1)$ respectively.
Now assume that we do not observe $\vec{x}$ and $\vec{y}$ directly, but rather $\vec{z} := [z_1, ..., z_n]$ and $\vec{u} := [u_1, ..., u_n]$, which are constructed as

\begin{align*}
  z_i &= \text{max}(x_i, y_i) \\
  u_i &= I(x_i \leq y_i) \\
\end{align*}

for $i = 1, ..., n$ and where $I$ is the indicator function.

## Complete data log likelihood function

The likelihood function for the complete data $(\vec{x}, \vec{y})$ is

\begin{gather*}
  L(\lambda_0, \lambda_0; \vec{x}, \vec{y}) \\
  =
    \prod_{i = 1}^n f_x(x_i | \lambda_0)
    \cdot
    \prod_{j = 1}^n f_y(y_j | \lambda_1) \\
  =
    \prod_{i = 1}^n \lambda_0 e^{-\lambda_0 x_i}
    \cdot
    \prod_{j = 1}^n \lambda_1 e^{-\lambda_1 y_i} \\
  =
    (\lambda_0 \lambda_1)^n
    \cdot
    \text{exp}
      \left\{
      -\lambda_0 \sum_{i = 1}^n x_i
      \right\}
    \cdot
    \text{exp}
      \left\{
      -\lambda_1 \sum_{j = 1}^n y_j
      \right\}
\end{gather*}

Thus the log likelihood becomes

\begin{gather}
  l(\lambda_0, \lambda_1; \vec{x}, \vec{y})
  :=
  \ln{L(\lambda_0, \lambda_1; \vec{x}, \vec{y})} \nonumber \\
  =
  n (\ln{\lambda_0} + \ln{\lambda_1})
  - \lambda_0 \sum_{i = 1}^n x_i
  - \lambda_1 \sum_{j = 1}^n y_j.
  \label{eq:loglik}
\end{gather}

We can now calculate the expected value of the log likelihood given $\vec{z}$, $\vec{u}$, $\lambda_0^{(t)}$, $\lambda_1^{(t)}$.
First we calculate the conditional expectation of the first term of \eqref{eq:loglik} consisting of constants:

\begin{equation}
  \label{eq:first}
  \text{E} \left[
    n (\ln{\lambda_0} + \ln{\lambda_1})
    ~|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  =
  n (\ln{\lambda_0} + \ln{\lambda_1}).
\end{equation}

For the conditional expectation of $x_i$, which is not directly observed, notice that

\begin{align*}
  \text{E} \left[
    x_i
    ~|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  =
  \begin{cases}
    z_i &\text{if } u_i = 1 \\
    %
    \text{E}\left[
      x_i ~|~ x_i < z_i, \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
    \right]
    &\text{otherwise.}
  \end{cases} \\
  %
  =
    u_i z_i
    +
    (1 - u_i)
    \text{E}\left[
      x_i ~|~ x_i < z_i, \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
    \right]
\end{align*}

Here we have rewritten the cases by using $u_i$.
For the $u_i = 0$ case we can calculate the expected value by integration:

\begin{gather*}
  \text{E}\left[
    x_i
    ~\middle|~
    x_i < z_i, \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right] \\
  =
    \frac{
      \int_{x = 0}^{x = z_i} x f_x(x ~|~ \lambda_0^{(t)})\text{d}x
    }{
      \int_{x = 0}^{x = z_i} f_x(x ~|~ \lambda_0^{(t)})\text{d}x
    } \\
  %
  =
    \frac{
      \int_{x = 0}^{x = z_i} \lambda_0 x e^{-\lambda_0 x} \text{d}x
    }{
      \int_{x = 0}^{x = z_i} \lambda_0 e^{-\lambda_0 x} \text{d}x
    } \\
  %
  =
    \frac{
      1 - (\lambda_0^{(t)} z_i + 1) e^{-\lambda_0^{(t)} z_i}
    }{
      \lambda_0^{(t)}
    }
    \cdot
    \frac{1}{1 - e^{-\lambda_0^{(t)} z_i}} \\
  =
    \frac{1}{\lambda_0^{(t)}}
    -
    \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
\end{gather*}

Using $u_i$ we can now write the conditional expectation of the second term in \eqref{eq:loglik} as

\begin{equation}
  \label{eq:second}
  E\left[
    \lambda_0 \sum_{i = 1}^n x_i
    ~\middle|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  =
    \lambda_0 \sum_{i = 1}^n \left[
      u_i z_i
      +
      (1 - u_i)
      \left(
        \frac{1}{\lambda_0^{(t)}}
        -
        \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
      \right)
    \right]
\end{equation}

Using the symmetry of the problem we can find the conditional expectation of the third term in \eqref{eq:loglik} as well

\begin{equation}
  \label{eq:third}
  E\left[
    \lambda_1 \sum_{j = 1}^n y_j
    ~\middle|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  =
    \lambda_1 \sum_{j = 1}^n \left[
      (1 - u_i) z_i
      +
      u_i
      \left(
        \frac{1}{\lambda_1^{(t)}}
        -
        \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}
      \right)
    \right]
\end{equation}

We can now combine equations \eqref{eq:first}, \eqref{eq:second}, and \eqref{eq:third} in order to get the conditional expectation of the log likelihood

\begin{align*}
  Q(\lambda_0, \lambda_1)
  :=
  E\left[
    l(\lambda_0, \lambda_1; \vec{x}, \vec{y})
    ~\middle|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  &=
    n (\ln{\lambda_0} + \ln{\lambda_1}) \\
  &-
    \lambda_0 \sum_{i = 1}^n \left[
      u_i z_i
      +
      (1 - u_i)
      \left(
        \frac{1}{\lambda_0^{(t)}}
        -
        \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
      \right)
    \right] \\
  &-
    \lambda_1 \sum_{j = 1}^n \left[
      (1 - u_i) z_i
      +
      u_i
      \left(
        \frac{1}{\lambda_1^{(t)}}
        -
        \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}
      \right)
    \right] \\
  :&=
    n (\ln{\lambda_0} + \ln{\lambda_1}) - \lambda_0 S_0 - \lambda_1 S_1
\end{align*}

\newcommand{\Q}[0]{\ensuremath{Q(\lambda_0, \lambda_1)}}

Here we have denoted this conditional expectation of the log likelihood as $\Q$, and the first and second sum in the equation as $S_0$ and $S_1$, respectively.
This will simplify the notation going forwards.


## EM algorithm

In order to maximize $\Q$ with respect to $\lambda_0$ and $\lambda_1$ we must solve

\newcommand{\uvec}[1]{\boldsymbol{\hat{\textbf{#1}}}}

\begin{gather*}
  \nabla \cdot \Q = \vec{0}, \\
  %
  \nabla := \left[
    \uvec{i} \frac{\partial}{\partial \lambda_0},~
    \uvec{j} \frac{\partial}{\partial \lambda_1}
  \right].
\end{gather*}

This equation can be explicitly solved for $\lambda_0$ and $\lambda_1$ as

\begin{align*}
  \frac{n}{\lambda_0} = S_0 \implies \lambda_0 &= \frac{n}{S_0}, \\
  \frac{n}{\lambda_1} = S_1 \implies \lambda_1 &= \frac{n}{S_1}. \\
\end{align*}



```{r}
library(tidyverse)
u <- scan("data/u.txt", double()) %>% as.logical()
z <- scan("data/z.txt", double())
n <- length(z) 
```

```{r}
expectation_maximization <- function (z, u, lambda, tol) {
  lambdas <- lambda
  n <- length(u)

  delta <- Inf
  while (delta > tol) {
    sum_0 <- sum(z[u]) + sum(1 / lambda[1] - z[!u] / expm1(lambda[1] * z[!u]))
    sum_1 <- sum(z[!u]) + sum(1 / lambda[2] - z[u] / expm1(lambda[2] * z[u]))
    new_lambda <- c(n / sum_0, n / sum_1)

    delta <- max(abs(lambda - new_lambda))
    lambda <- new_lambda

    lambdas <- c(lambdas, lambda)
  }

  lambdas <- tibble(
    iteration = 1:(length(lambdas) / 2),
    lambda_0 = lambdas[c(TRUE, FALSE)],
    lambda_1 = lambdas[c(FALSE, TRUE)]
  )

  final_lambda <- tibble(
    parameter = c("lambda_0", "lambda_1"),
    value = lambda
  )
  return(list(lambdas = lambdas, final_lambda = final_lambda))
}
```


```{r}
expectation <- expectation_maximization(z, u, lambda = c(1, 1), tol = 0.001)
final_lambda <- expectation$final_lambda
lambdas <- expectation$lambdas
print(final_lambda)
```

```{r}
library(scales)
lambdas %>%
  gather(lambda, value, c("lambda_0", "lambda_1")) %>%
  ggplot(aes(x = iteration)) +
  geom_line(aes(y = value, col = lambda)) +
  geom_hline(
    data = final_lambda,
    aes(yintercept = value, col = parameter),
    alpha = 0.5,
    linetype = "dashed"
  ) +
  scale_x_continuous(breaks=1:dim(lambdas)[1]) +
  ylim(0, NA)
```


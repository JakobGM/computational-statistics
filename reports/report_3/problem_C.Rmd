# Problem C: The EM-algorithm and bootstrapping

```{r, include = FALSE}
# Include parent to get its headers
library(knitr)
set_parent("base.Rmd")
```

## Description

Let $\vec{x} := [x_1, ..., x_n]$ and $\vec{y} := [y_1, ..., y_n]$ be two collections of independent random variables from two independent exponential distributions
\begin{align*}
  x_1, ..., x_n &\sim \text{exp}(\lambda_0) \\
  y_1, ..., y_n &\sim \text{exp}(\lambda_1).
\end{align*}

Denote the probability density functions of these distributions as $f_x(x | \lambda_0)$ and $f_y(y | \lambda_1)$ respectively.
Now assume that we do not observe $\vec{x}$ and $\vec{y}$ directly, but rather $\vec{z} := [z_1, ..., z_n]$ and $\vec{u} := [u_1, ..., u_n]$, which are constructed as
\begin{align*}
  z_i &= \text{max}(x_i, y_i) \\
  u_i &= I(x_i \leq y_i) \\
\end{align*}

for $i = 1, ..., n$ and where $I$ is the indicator function.

## Complete data log likelihood function

The likelihood function for the complete data $(\vec{x}, \vec{y})$ is
\begin{gather*}
  \mathcal{L}(\lambda_0, \lambda_1; \vec{x}, \vec{y}) \\
  =
    \prod_{i = 1}^n f_x(x_i | \lambda_0)
    \cdot
    \prod_{j = 1}^n f_y(y_j | \lambda_1) \\
  =
    \prod_{i = 1}^n \lambda_0 e^{-\lambda_0 x_i}
    \cdot
    \prod_{j = 1}^n \lambda_1 e^{-\lambda_1 y_i} \\
  =
    (\lambda_0 \lambda_1)^n
    \cdot
    \text{exp}
      \left\{
      -\lambda_0 \sum_{i = 1}^n x_i
      \right\}
    \cdot
    \text{exp}
      \left\{
      -\lambda_1 \sum_{j = 1}^n y_j
      \right\}
\end{gather*}

Thus the log likelihood becomes
\begin{align}
  \ell(\lambda_0, \lambda_1; \vec{x}, \vec{y})
  &:=
  \ln{\mathcal{L}(\lambda_0, \lambda_1; \vec{x}, \vec{y})} \nonumber \\
  &=
  n (\ln{\lambda_0} + \ln{\lambda_1})
  - \lambda_0 \sum_{i = 1}^n x_i
  - \lambda_1 \sum_{j = 1}^n y_j.
  \label{eq:loglik}
\end{align}

We can now calculate the expected value of the log likelihood given $\vec{z}$, $\vec{u}$, $\lambda_0^{(t)}$, $\lambda_1^{(t)}$.
First we calculate the conditional expectation of the first term of \eqref{eq:loglik} consisting of constants:
\begin{equation}
  \label{eq:first}
  \text{E} \left[
    n (\ln{\lambda_0} + \ln{\lambda_1})
    ~|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  =
  n (\ln{\lambda_0} + \ln{\lambda_1}).
\end{equation}

For the conditional expectation of $x_i$, which is not directly observed, notice that
\begin{align*}
  \text{E} \left[
    x_i
    ~|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  =
  \begin{cases}
    z_i &\text{if } u_i = 1 \\
    %
    \text{E}\left[
      x_i ~|~ x_i < z_i, \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
    \right]
    &\text{otherwise.}
  \end{cases} \\
  %
  =
    u_i z_i
    +
    (1 - u_i)
    \text{E}\left[
      x_i ~|~ x_i < z_i, \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
    \right]
\end{align*}

Here we have rewritten the cases by using $u_i$.
For the $u_i = 0$ case we can calculate the expected value by integration:
\begin{gather*}
  \text{E}\left[
    x_i
    ~\middle|~
    x_i < z_i, \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right] \\
  =
    \frac{
      \int_{x = 0}^{x = z_i} x f_x(x ~|~ \lambda_0^{(t)})\text{d}x
    }{
      \int_{x = 0}^{x = z_i} f_x(x ~|~ \lambda_0^{(t)})\text{d}x
    } \\
  %
  =
    \frac{
      \int_{x = 0}^{x = z_i} \lambda_0 x e^{-\lambda_0 x} \text{d}x
    }{
      \int_{x = 0}^{x = z_i} \lambda_0 e^{-\lambda_0 x} \text{d}x
    } \\
  %
  =
    \frac{
      1 - (\lambda_0^{(t)} z_i + 1) e^{-\lambda_0^{(t)} z_i}
    }{
      \lambda_0^{(t)}
    }
    \cdot
    \frac{1}{1 - e^{-\lambda_0^{(t)} z_i}} \\
  =
    \frac{1}{\lambda_0^{(t)}}
    -
    \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
\end{gather*}

Using $u_i$ we can now write the conditional expectation of the second term in \eqref{eq:loglik} as
\begin{equation}
  \label{eq:second}
  E\left[
    \lambda_0 \sum_{i = 1}^n x_i
    ~\middle|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  =
    \lambda_0 \sum_{i = 1}^n \left[
      u_i z_i
      +
      (1 - u_i)
      \left(
        \frac{1}{\lambda_0^{(t)}}
        -
        \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
      \right)
    \right]
\end{equation}

Using the symmetry of the problem we can find the conditional expectation of the third term in \eqref{eq:loglik} as well
\begin{equation}
  \label{eq:third}
  E\left[
    \lambda_1 \sum_{j = 1}^n y_j
    ~\middle|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  =
    \lambda_1 \sum_{j = 1}^n \left[
      (1 - u_i) z_i
      +
      u_i
      \left(
        \frac{1}{\lambda_1^{(t)}}
        -
        \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}
      \right)
    \right]
\end{equation}

We can now combine equations \eqref{eq:first}, \eqref{eq:second}, and \eqref{eq:third} in order to get the conditional expectation of the log likelihood
\begin{align*}
  Q(\lambda_0, \lambda_1)
  :=
  E\left[
    \ell(\lambda_0, \lambda_1; \vec{x}, \vec{y})
    ~\middle|~
    \vec{z}, \vec{u}, \lambda_0^{(t)}, \lambda_1^{(t)}
  \right]
  &=
    n (\ln{\lambda_0} + \ln{\lambda_1}) \\
  &-
    \lambda_0 \sum_{i = 1}^n \left[
      u_i z_i
      +
      (1 - u_i)
      \left(
        \frac{1}{\lambda_0^{(t)}}
        -
        \frac{z_i}{e^{\lambda_0^{(t)} z_i} - 1}
      \right)
    \right] \\
  &-
    \lambda_1 \sum_{j = 1}^n \left[
      (1 - u_i) z_i
      +
      u_i
      \left(
        \frac{1}{\lambda_1^{(t)}}
        -
        \frac{z_i}{e^{\lambda_1^{(t)} z_i} - 1}
      \right)
    \right] \\
  :&=
    n (\ln{\lambda_0} + \ln{\lambda_1}) - \lambda_0 S_0 - \lambda_1 S_1
\end{align*}

\newcommand{\Q}[0]{\ensuremath{Q(\lambda_0, \lambda_1)}}

Here we have denoted this conditional expectation of the log likelihood as $\Q$, and the first and second sum in the equation as $S_0$ and $S_1$, respectively.
This will simplify the notation going forward.


## EM algorithm

In order to maximize $\Q$ with respect to $\lambda_0$ and $\lambda_1$ we must solve

\newcommand{\uvec}[1]{\boldsymbol{\hat{\textbf{#1}}}}

\begin{gather*}
  \nabla \Q = [0, 0], \\
  %
  \nabla := \left[
    \uvec{i} \frac{\partial}{\partial \lambda_0},~
    \uvec{j} \frac{\partial}{\partial \lambda_1}
  \right].
\end{gather*}

This equation can be explicitly solved for $\vec{\lambda} := [\lambda_0, \lambda_1]$ as
\begin{align}
  \label{eq:em_lambda}
  \frac{n}{\lambda_0} = S_0 \implies \lambda_0 &= \frac{n}{S_0}, \nonumber \\
  \frac{n}{\lambda_1} = S_1 \implies \lambda_1 &= \frac{n}{S_1}.
\end{align}

We start by importing the provided data set containing $\vec{z}$ and $\vec{u}$:

```{r}
library(tidyverse)
data <- tibble(
  u = scan("data/u.txt", double()) %>% as.logical(),
  z = scan("data/z.txt", double())
)
n <- length(data$z) 
```

We can now implement the EM algorithm, using equation \eqref{eq:em_lambda} in the M-step.
We will use

$$
  || \vec{\lambda}^{(t)} - \vec{\lambda}^{(t-1)}||_1
  = 
  \text{max} \left(
    |\lambda_0^{(t)} - \lambda_0^{(t-1)}|,~
    |\lambda_1^{(t)} - \lambda_1^{(t-1)}|
  \right)
  <
  \texttt{TOL}
$$

as the stopping criterion.

```{r}
expectation_maximization <- function (data, lambda = c(1, 1), tol = 1e-4) {
  # Problem parameters
  z <- data$z
  u <- data$u
  n <- length(u)

  # Data structure used to save each lambda result for every iteration
  lambdas <- lambda

  # Stopping criterion initialized at infinity
  delta <- Inf
  while (delta > tol) {
    # Maximum likelihood estimates
    sum_0 <- sum(z[u]) + sum(1 / lambda[1] - z[!u] / expm1(lambda[1] * z[!u]))
    sum_1 <- sum(z[!u]) + sum(1 / lambda[2] - z[u] / expm1(lambda[2] * z[u]))
    new_lambda <- c(n / sum_0, n / sum_1)

    # Calculate convergence criterion
    delta <- max(abs(lambda - new_lambda))

    # Save results before next iteration
    lambda <- new_lambda
    lambdas <- c(lambdas, lambda)
  }

  lambdas <- tibble(
    iteration = 0:(length(lambdas) / 2 - 1),
    lambda_0 = lambdas[c(TRUE, FALSE)],
    lambda_1 = lambdas[c(FALSE, TRUE)]
  )

  final_lambda <- tibble(
    parameter = c("lambda_0", "lambda_1"),
    value = lambda
  )
  return(list(lambdas = lambdas, final_lambda = final_lambda))
}
```

We can now invoke the EM-algorithm with $\texttt{TOL} = 10^{-4}$ and $\vec{\lambda}^{(t)} = [1, 1]$, using the provided data:

```{r}
orig_result <- expectation_maximization(data = data)
print(orig_result$final_lambda$value)
```

The EM-estimator for the rate parameters is $\vec{\hat{\lambda}}_{\text{EM}} \approx [3.466, 9.353]$.
We can plot the convergence of the EM-algorithm:

```{r, fig.pos = "!h", fig.height = 3, fig.cap = "\\label{fig:em_convergence}Convergence of the EM-algorithm. $\\lambda_0$ is shown in red, while $\\lambda_1$ in shown in blue. The converged values are plotted as horizontal, dashed lines with their respective colors. The initial value for the EM-algorithm is iteration 0 and marked with a black dot."}
library(scales)
orig_result$lambdas %>%
  gather(lambda, value, c("lambda_0", "lambda_1")) %>%
  ggplot(aes(x = iteration)) +
  geom_line(aes(y = value, col = lambda)) +
  geom_hline(
    data = orig_result$final_lambda,
    aes(yintercept = value, col = parameter, linetype = "Converged value"),
    alpha = 0.5
  ) +
  geom_point(
    data = tibble(x = 0, y = 1),
    aes(x = x, y = y, shape = "Initial value")
  ) +
  scale_x_continuous(breaks=0:dim(orig_result$lambdas)[1]) +
  scale_linetype_manual(name = "", values = c(2, 2)) +
  ylim(0, NA)
```

\newpage

As we can see in Figure \ref{fig:em_convergence} the EM-algorithm converges quite rapidly.


## Bootstrapping

We now want to apply bootstrapping in order to find estimates for the standard deviation, bias, and correlation of $\vec{\hat{\lambda_0}}$ and $\vec{\hat{\lambda_1}}$.
The following pseudocode describes the actual implementation:

\begin{itemize}
  \item Assume that we have an initial data set $\vec{z}$ and $\vec{u}$, defined as before.
  \item Construct the pairwise observation set $\Omega = \{ \omega_1, \omega_2, ..., \omega_n \} := \{ (z_1, u_1), (z_2, u_2), ..., (z_n, u_n) \}$. These observation pairs are now independet and identically distributed. Denote this distribution as $F$.
  \item Construct the empirical distribution $\hat{F}$ which assigns mass $\frac{1}{n}$ to each observation pair in $\Omega$. This distribution is supposed to approximate $F$.
  \item Draw $B$ \textit{bootstrap samples} from $\hat{F}$. Each bootstrap sample is of size $n$, and is denoted $\Omega^{(b)} = \{ \omega_1^{(b)}, ..., \omega_n^{(b)} \} = \{ (z_1^{(b)}, u_1^{(b)}), ..., (z_n^{(b)}, u_n^{(b)}) \}$ for $b = 1, ..., B$. In practice, this means sampling $n$ elements from $\Omega$ \textit{with} replacement.
  \item For $b = 1, 2, ..., B$:
  \begin{itemize}
    \item Estimate $\lambda_0^{(b)}$ and $\lambda_1^{(b)}$ using the EM-algorithm as derived above, but this time use the bootstrap replicate $\Omega^{(b)}$ instead of the original data set $\Omega$. These are called bootstrap replicates of $\lambda_0$ and $\lambda_1$, respectively.
  \end{itemize}
\end{itemize}

The important part here is that we draw observation *pairs*, as $z_i$ and $u_j$ are dependent for $i = j$ and independent for $i \neq j$.
It is this algorithm we now implement, using $B = 2000$:

```{r}
library(rsample)  # For bootstraps() function
library(magrittr)  # For assignment pipe
set.seed(0)
B <- 2000
bs_data <- bootstraps(data, times = B)
bs_results <- tibble(lambda_0 = double(), lambda_1 = double())

for (bootstrap in bs_data$splits) {
  expectation <- expectation_maximization(data = analysis(bootstrap))
  lambda <- expectation$final_lambda$value
  bs_results %<>% add_row(lambda_0 = lambda[1], lambda_1 = lambda[2])
}
```

The idea is now that the bootstrap replicates $[(\hat{\lambda_0}^{(1)}, \hat{\lambda_1}^{(1)}), ..., (\hat{\lambda_0}^{(B)}, \hat{\lambda_1}^{(B)})]$ can be considered to be approximate samples of the distribution of $\vec{\lambda}_{\text{EM}}$.
We can therefore estimate the standard deviation, bias, and correlation of $\vec{\lambda}_{\text{EM}}$ from the bootstrap replicates in order to make inferences from the original EM-result.

```{r, fig.pos = "!h"}
library(corrr)
library(kableExtra)
bs_stats <- list(
  cor = cor(bs_results$lambda_0, bs_results$lambda_1),
  std = bs_results %>% map(sd) %>% unlist(),
  mean = bs_results %>% colMeans()
)
bs_stats$bias <- bs_stats$mean - orig_result$final_lambda$value
bs_stats[-1] %>%
  as_tibble() %>%
  t() %>%
  kable(
    escape = FALSE,
    col.names = c("$\\hat{\\lambda_0}$", "$\\hat{\\lambda_1}$"),
    caption = "\\label{tab:em_results}Estimated EM-estimator statistics from bootstrap results.",
    format = "pandoc"
  )
```

We can see from table \ref{tab:em_results} that the bias is relatively small.
We also have $\hat{\text{SD}}(\hat{\lambda_0}) < \hat{\text{SD}}(\hat{\lambda_1})$, which may be explained by the fact that $\hat{\lambda_0} < \hat{\lambda_1}$ in combination with the fact that the variance and rate of an exponential distribution are equal.
Additionally:

```{r}
print(bs_stats$cor)
```

We have $\hat{\text{Corr}}(\hat{\lambda_0}, \hat{\lambda_1}) \approx 0.0015$ which can be considered very close to $0$.
This makes sense, as they should be independent.

These results can be visualized in a histogram:

```{r, fig.fullwidth = TRUE, fig.pos = "!h", fig.cap = "\\label{fig:em_histogram}Histogram of the bootstrap replicates of $\\hat{\\lambda_0}$ (red) and $\\hat{\\lambda_1}$ (blue). The mean of the bootstrap replicates are show as solid, black, vertical lines. Mean $\\pm$ Std of the bootstrap replicates are shown in dashed, black, vertical lines. Finally, the original EM-estimators are shown as green vertical lines."}
binwidth <- 0.07
bs_results %>%
  gather(lambda, value, c("lambda_0", "lambda_1")) %>%
  ggplot(aes(x = value, fill = lambda)) +
  geom_histogram(aes(y = binwidth * ..density..), binwidth = binwidth) +
  geom_vline(xintercept = bs_stats$mean) +
  geom_vline(xintercept = bs_stats$mean + bs_stats$std, linetype = "dashed") +
  geom_vline(xintercept = bs_stats$mean - bs_stats$std, linetype = "dashed") +
  geom_vline(
    data = orig_result$final_lambda,
    aes(xintercept = value, color = "Original estimates")
  ) +
  scale_color_manual(values = c("green")) +
  ylab("Relative frequency") +
  xlab("Parameter value") +
  theme(legend.position = "bottom")
```

Figure \ref{fig:em_histogram} shows the greater estimated variance and bias in $\hat{\lambda_1}$ compared to $\hat{\lambda_0}$.
It is possible to construct a \textit{bias corrected} estimate, $\hat{\lambda_1}_c$, for $\lambda_1$ on the form
\begin{equation*}
  \hat{\lambda_1}_c := \hat{\lambda_1} - \widehat{\text{Bias}}(\hat{\lambda}),
\end{equation*}

and likewise for $\lambda_0$.
This may indeed decrease the bias of our estimators, but will not necessarily yield an overall better estimator as
\begin{equation*}
  \text{Var}(\hat{\lambda_i}_c) \geq \text{Var}(\hat{\lambda_i}), ~~~ i = 1, 2
\end{equation*}

due to the estimator consisting of additional terms.
It is therefore preferable to \textit{not} apply bias correction if the bias is small.
As we consider our bias to be within acceptable bounds, we will not opt for bias correction.



## Analytical formula for $f_{Z_i, U_i}(z_i, u_i ~|~ \lambda_0, \lambda_1)$

\newcommand{\od}[1]{\,\text{d}#1}

We now want to find an analytical expression for $f_{Z_i, U_i}(z_i, u_i ~|~ \lambda_0, \lambda_1)$.
Start by finding $F_{Z}(z_i ~|~ u_i = 1)$:
\begin{align*}
  F_Z(z_i ~|~ u_i = 1)
  &=
  P(X_i \leq z_i, Y_i \leq X_i) \\
  &=
  \int \limits_{0}^{z_i} \int \limits_{0}^{x} f_{X}(x ~|~ \lambda_0) f_{Y}(y ~|~ \lambda_1) \od{y} \od{x} \\
  &=
  \int \limits_{0}^{z_i} \lambda_0 e^{-\lambda_0 x} \int \limits_{0}^{x} \lambda_1 e^{-\lambda_1 y} \od{y} \od{x} \\
  &=
  \int \limits_{0}^{z_i} \lambda_0 e^{-\lambda_0 x} \left( 1 - e^{-\lambda_1 x} \right) \od{x} \\
  &=
  \frac{\lambda_0}{\lambda_0 + \lambda_1} \left( e^{-(\lambda_0 + \lambda_1) z_i} - 1 \right) - e^{-\lambda_0 z_i} + 1
\end{align*}

We can now find $f_{Z}(z_i ~|~ u_i = 1)$ by differentiating:
\begin{align*}
  f_{Z}(z_i ~|~ u_i = 1)
  &=
  \frac{\od{F_Z}(z_i ~|~ u_i = 1)}{\od{z_i}} \\
  &=
  -\lambda_0 e^{-(\lambda_0 + \lambda_1) z_i} + \lambda_0 e^{-\lambda_0 z_i} \\
  &=
  \lambda_0 e^{-\lambda_0 z_i} \left( 1 - e^{-\lambda_1 z_i} \right)
\end{align*}

Again, by the symmetry of the problem, we can find $f_{Z}(z_i ~|~ u_i = 0)$:
\begin{equation*}
  f_{Z}(z_i ~|~ u_i = 1)
  =
  \lambda_1 e^{-\lambda_1 z_i} \left( 1 - e^{-\lambda_0 z_i} \right)
\end{equation*}

The joint distribution for $\vec{z}$ and $\vec{u}$ and inversely the likelihood of $\lambda_0$ and $\lambda_1$, depending on what you consider to be the variables, can now be found:
\begin{align*}
  f_{Z, U}(\vec{z}, \vec{u} ~|~ \lambda_0, \lambda_1)
  &=
  \prod \limits_{i:u_i = 0} f_{Z}(z_i ~|~ u_i = 0)
  \prod \limits_{i:u_i = 1} f_{Z}(z_i ~|~ u_i = 1) \\
  &=
  \prod \limits_{i = 1}^{n} \left(
    u_i \lambda_0 e^{-\lambda_0 z_i} \left( 1 - e^{-\lambda_1 z_i} \right)
    +
    (u_i - 1) \lambda_1 e^{-\lambda_1 z_i} \left( 1 - e^{-\lambda_0 z_i} \right)
  \right) \\
  &=
  \mathcal{L}(\lambda_0, \lambda_1; \vec{z}, \vec{u})
\end{align*}

We can now find the log likelihood:
\begin{align*}
  \ell(\lambda_0, \lambda_1; \vec{z}, \vec{u})
  &:=
  \ln{\mathcal{L}(\lambda_0, \lambda_1; \vec{z}, \vec{u})} \\
  &=
    \sum \limits_{i: u_i = 0} \left(
      \ln{(\lambda_1)} - \lambda_1 z_i + \ln{(1 - e^{- \lambda_0 z_i})}
    \right)
  +
    \sum \limits_{i: u_i = 1} \left(
      \ln{(\lambda_0)} - \lambda_0 z_i + \ln{(1 - e^{- \lambda_1 z_i})}
    \right) \\
  &=
    n_1 \ln{(\lambda_0)}
    +
    n_0 \ln{(\lambda_1)}
    +
    \sum \limits_{i: u_i = 0} \left(\ln{(1 - e^{-\lambda_0 z_i})} - \lambda_1 z_i \right)
    +
    \sum \limits_{i: u_i = 1} \left(\ln{(1 - e^{-\lambda_1 z_i})} - \lambda_0 z_i \right)
\end{align*}

Where we have defined:
\begin{align*}
  n_0 := \sum \limits_{i = 1}^{n} (1 - u_i), 
  ~~~~~~
  n_1 := \sum \limits_{i = 1}^{n} u_i
\end{align*}

In order to find the maximum likelihood estimators $\hat{\lambda_0}$ and $\hat{\lambda_1}$, we derive the *score vector*:
\begin{align*}
  \vec{s}(\lambda_0, \lambda_1; \vec{z}, \vec{u})
  :&=
  \begin{bmatrix}
    \frac{\partial \ell(\lambda_0, \lambda_1; \vec{z}, \vec{u})}{\partial \lambda_0} \\
    \frac{\partial \ell(\lambda_0, \lambda_1; \vec{z}, \vec{u})}{\partial \lambda_1}
  \end{bmatrix} \\
  &=
  \begin{bmatrix}
    \frac{n_1}{\lambda_0} + \sum \limits_{i: u_i = 0} \frac{z_i}{e^{\lambda_0 z_i} - 1} - \sum \limits_{i: u_i = 1} z_i\\
    \frac{n_0}{\lambda_1} + \sum \limits_{i: u_i = 1} \frac{z_i}{e^{\lambda_1 z_i} - 1} - \sum \limits_{i: u_i = 0} z_i
  \end{bmatrix}
\end{align*}

We can now solve the optimization problem
\begin{equation*}
  \hat{\lambda_0}, \hat{\lambda_1}
  =
  \underset{\lambda_0, \lambda_1}{\text{argmax}}~\mathcal{L}(\lambda_0, \lambda_1; \vec{z}, \vec{u}),
\end{equation*}

by solving the dual problem of
\begin{equation*}
  \vec{s}(\lambda_0, \lambda_1; \vec{z}, \vec{u}) = \vec{0},
\end{equation*}

with respect to $\lambda_0$ and $\lambda_1$.

Now, notice that $s_1 = \frac{\od{\ell}}{\od{\lambda_0}}$ is a function of only $\lambda_0$, not $\lambda_1$, and vice versa for $s_2$.
The Hessian of $\ell(\lambda_0, \lambda_1; \vec{z}, \vec{u})$ is therefore diagonal with the following diagonal elements:
\begin{align*}
  \frac{\partial^2 \ell}{{\partial \lambda_0}^2}
  =
    - \frac{n_1}{{\lambda_0}^2}
    - \sum \limits_{i: u_i = 0} \frac{{z_i}^2 e^{\lambda_0 z_i}}{(e^{\lambda_0 z_i} - 1)^2} 
  < 0
  \\
  \frac{\partial^2 \ell}{{\partial \lambda_1}^2}
  =
    - \frac{n_0}{{\lambda_1}^2}
    - \sum \limits_{i: u_i = 1} \frac{{z_i}^2 e^{\lambda_1 z_i}}{(e^{\lambda_1 z_i} - 1)^2} 
  < 0
\end{align*}

The Hessian is therefore negative definite.
In conclusion, there is only one maxima, which is the global maxima.
We can therefore be sure that if we solve the optimization problem, we have in fact found maximum likelihood estimators $\hat{\lambda_0}$ and $\hat{\lambda_1}$.

```{r}
score <- function (lambda, data, index) {
  #' Evaluate the log likelihood score function for a given lambda
  #' lambda = double value type for lambda_{0,1}
  #' data = Data frame with `z` and `u` columns
  #' index = Either 0 or 1, indicating if lambda_0 or lambda_1 is of interest

  if (index == 0) {
    indeces <- data$u
  } else {
    indeces <- !data$u
  }

  return(
    sum(indeces) / lambda
    + sum(data$z[!indeces] / expm1(lambda * data$z[!indeces]))
    - sum(data$z[indeces])
  )
}
```

```{r}
lambda_mle <- c(
  lambda_0 = uniroot(f = score, interval = c(1e-3, 100), data = data, index = 0)$root,
  lambda_1 = uniroot(f = score, interval = c(1e-3, 100), data = data, index = 1)$root
)
print(lambda_mle)
```

The result of the log likelihood estimation is $\vec{\lambda}_{\text{MLE}} \approx [3.466, 9.353]$.
Comparing this to the result of the EM-algorithm, $\vec{\hat{\lambda}}_{\text{EM}} \approx [3.466, 9.353]$, they yield the same result to three decimal digits.
The exact difference, $\vec{\hat{\lambda}}_{\text{EM}} - \vec{\lambda}_{\text{MLE}}$, is

```{r}
orig_result$final_lambda$value - lambda_mle
```

So both values are equal to at least 5 decimals, which strengthen our belief in the correctness of both implementations.

Some of the advantages of optimizing the likelihood directly compared to the EM algorithm are:

\begin{itemize}
  \item Only one optimization is necessary, compared to the EM algorithm where you might have to optimize differently parametrized functions iteratively. However, the EM-algorithm is rarely applied when explicit expressions for the minimizers can not be found. 
  \item The Fisher information matrix can be calculated in order to infer the degree of information encoded in the maximum likelihood.
  \item You optimize the function of interest directly, instead of only approximating it. 
\end{itemize}

---
output:
  pdf_document: default
  html_document: default
---
```{r, include=FALSE}
set.seed(0)
knitr::set_parent("base.Rmd")

library(magrittr)
library(tidyverse)
library(purrr)
```

## Problem A

In this problem we will analyze the data \texttt{data3A\$x} provided in the file \texttt{probAdata.R}.

```{r}
source(here::here("reports", "report_3", "data", "probAdata.R"))
```

The data is assumed to be distributed according to the AR(2) model defined by the relation

$$
x_t = \beta_1 x_{t -1} + \beta_2 x_{t - 2} + e_t
$$

where $e_t$ are iid random variables with zero mean and constant variance, with no assumption on the exact distribution.This makes $\vec{x}$ a non-Gaussian time-series. Our sequence is of length $T = 100$.

We will use two methods in order to estimate $\vec{\beta} := [\beta_1, \beta_2]$:

\begin{align*}
    \hat{\vec{\beta}}_{LS}
    &=
    \underset{\beta_1, \beta_2}{\text{argmin}}~Q_{LS}(\vec{x}),
    %
    & Q_{LS}(\vec{x})
    :=
    \sum \limits_{t = 3}^{T} \left( x_t -\beta_1 x_{t - 1} - \beta_2 x_{t - 2} \right)^2.
  \\
    \hat{\vec{\beta}}_{LA}
    &=
    \underset{\beta_1, \beta_2}{\text{argmin}}~Q_{LA}(\vec{x}),
    %
    & Q_{LA}(\vec{x})
    :=
    \sum \limits_{t = 3}^{T} \left| x_t -\beta_1 x_{t - 1} - \beta_2 x_{t - 2} \right|.
\end{align*}

With other words, optimizing the sum of squared residuals (LS) or the absolute residuals (LA), respectively.
The minimizers can be calculated with the \texttt{R}-function \texttt{ARp.beta.est} provided in the file \texttt{probAhelp.R}.

```{r}
source(here::here("reports", "report_3", "data", "probAhelp.R"))
```

We start off by using the supplied function to estimate $\vec{\beta}_{LS}$ and $\vec{\beta}_{LA}$.

```{r}
x <- data3A$x
beta <- ARp.beta.est(x, 2)
```

The innovations $\hat{e}_t$ can be estimated by 

$$
\hat{e}_t = x_t - \hat{\beta}_1x_{t-1} - \hat{\beta}_2x_{t-2},\ t = 3,\ldots,T
$$

The innovations are estimated using either $\hat{\vec{\beta}}_{LS}$ or $\hat{\vec{\beta}}_{LA}$. We will denote the different estimated innovations by $\hat{e}_t^{LS}$ and $\hat{e}_t^{LA}$.

The estimated innovations $\hat{\epsilon}_t$ are then centered by calculating 

$$
\hat{\epsilon}_t = \hat{e}_t - \bar{e}
$$

This is implemented by the function `estimate_pseudo_innovations`.

```{r}
estimate_pseudo_innovations <- function(beta, x) {
  L <- length(x)
  xs <- list(
    xt = x[seq(3, L)],
    xt1 = x[seq(2, L - 1)],
    xt2 = x[seq(1, L - 2)]
  )

  i <- xs %>%
    pmap_dbl(function(xt, xt1, xt2)
      xt - beta[1] * xt1 - beta[2] * xt2)

  pi <- i %>%
    subtract(mean(i))
  return(pi)
}

ls_pi <- estimate_pseudo_innovations(beta$LS, x)
la_pi <- estimate_pseudo_innovations(beta$LA, x)
```

The residuals are plotted in Figure \ref{fig:resids}. We observe that the residuals are evenly distributed around 0, except for one residual which is much larger than all others. 

```{r, fig.cap="\\label{fig:resids} Estimated centered residuals using the two estimators."}
residuals_df <- map2_dfr(
  list(ls_pi, la_pi), list("ls", "la"),
  ~tibble(residual = .x,
          index = seq(1, length(.x)),
          type = .y)
)

residuals_df %>%
  ggplot(aes(x=index, y=residual)) + 
  geom_point(bins=50) +
  facet_wrap(~type)
```


We proceed by obtaining $B = 1500$ bootstrap samples of the data sequence for each of the estimators $\hat{\vec{\beta}}_{LS}$ and $\hat{\vec{\beta}}_{LA}$. Each bootstrap sample is obtained by first picking a random consecutive sequence of length 2 from the original data sequence $\vec{x}$ and calculating the remaining $T - 2$ data points by using the relation 

$$
x_t = \hat{\beta}_1x_{t-1} + \hat{\beta}_2x_{t-2} + \epsilon.
$$
$\epsilon$ is chosen randomly from either the set $\{\hat{\epsilon}_3^{LS}, \ldots, \hat{\epsilon}_T^{LS}\}$ or $\{\hat{\epsilon}^{LA}_3, \ldots, \hat{\epsilon}_T^{LA}\}$, depending on which of the two estimators $\hat{\vec{\beta}}_{LS}$ and $\hat{\vec{\beta}}_{LA}$ we're using to obtain the sample.

```{r}
calculate_xt <- function(beta, xt1, xt2, pi) {
  beta[1] * xt1 + beta[2] * xt2 + pi
}
```

For each of the B resampled data sequences we estimate $\vec{\beta}_{LS}$ and $\vec{\beta}_{LA}$.

```{r}
resample_x <- function(pseudo_innovations, beta, x) {
  L <- length(x)
  rs_pi <- sample(pseudo_innovations,
    L - 2,
    replace = TRUE
  )

  init_index <- sample(seq(1, L - 1), 1)
  init_x <- x[init_index:(init_index + 1)]
  xt2 <- init_x[1]
  xt1 <- init_x[2]
  resampled_x <- init_x
  for (i in 1:(L - 2)) {
    xt <- calculate_xt(beta, xt1, xt2, rs_pi[i])
    resampled_x <- c(resampled_x, xt)
    xt2 <- resampled_x[i + 1]
    xt1 <- resampled_x[i + 2]
  }

  return(resampled_x)
}

B <- 1500
resampled_xs_ls <- B %>% rerun(resample_x(ls_pi, beta$LS, x))
resampled_xs_la <- B %>% rerun(resample_x(la_pi, beta$LA, x))

calculate_beta_ls <- as_mapper(~ ARp.beta.est(.x, 2)$LS)
calculate_beta_la <- as_mapper(~ ARp.beta.est(.x, 2)$LA)

bootstrapped_beta_ls <- resampled_xs_ls %>%
  map(calculate_beta_ls) %>%
  map(purrr::set_names, nm = c("beta1", "beta2")) %>%
  transpose() %>%
  as_tibble() %>%
  unnest()

bootstrapped_beta_la <- resampled_xs_la %>%
  map(calculate_beta_la) %>%
  map(purrr::set_names, nm = c("beta1", "beta2")) %>%
  transpose() %>%
  as_tibble() %>%
  unnest()
```

A histogram showing the obtained values are shown in Figure $\ref{fig:hist_ls}$ and $\ref{fig:hist_la}$.

```{r, fig.cap="\\label{fig:hist_ls} Histogram of $\\hat{\\beta}_{LS}$ obtained from bootstrap samples of the data sequence."}
bootstrapped_beta_ls %>%
  gather() %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 100) +
  facet_wrap(~key, scales = "free_x")
```

```{r, fig.cap = "\\label{fig:hist_la} Histogram of $\\hat{\\beta}_{LA}$ obtained from bootstrap samples of the data sequence."}
bootstrapped_beta_la %>%
  gather() %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 100) +
  facet_wrap(~key, scales = "free_x")
```

The \textit{bias} of our estimator is defined as

\begin{equation*}
  \text{bias}_F\left(\hat{\beta}, \beta\right) = E_F\left[\hat{\beta}\right] - \beta,
\end{equation*}

where $F$ is the real AR(2) distribution of our model.
Since we do not know the parametrization of the underlying model, this must be estimated with our bootstrap replicates $\hat{\beta}^{\star}$

$$
\widehat{\text{bias}}_B = \hat{\beta}^{\star}(\cdot) - \hat{\beta},
$$

where we have defined

\begin{equation*}
  \hat{\beta}^{\star}(\cdot) := \frac{1}{B} \sum \limits_{b = 1}^{B} \hat{\beta}^{\star}(b).
\end{equation*}

The estimated bias for the two components of the two different estimators, $\hat{\vec{\beta}}_{LS}$ and $\hat{\vec{\beta}}_{LA}$, can now be calculated.

```{r}
la_bias <- list(
  beta1_bias = bootstrapped_beta_la %>%
    pull(beta1) %>% 
    mean() %>% 
    subtract(beta$LA[[1]]),
  beta2_bias = bootstrapped_beta_la %>%
    pull(beta2) %>% 
    mean() %>% 
    subtract(beta$LA[[2]])  
)

ls_bias <- list(
  beta1_bias = bootstrapped_beta_ls %>%
    pull(beta1) %>% 
    mean() %>% 
    subtract(beta$LS[[1]]),
  beta2_bias = bootstrapped_beta_ls %>%
    pull(beta2) %>% 
    mean() %>% 
    subtract(beta$LS[[2]])  
)

ls_bias %>% 
  as_tibble() %>%
  knitr::kable(
    caption = "\\label{tab:ls_bias} Estimated bias of $\\hat{\\vec{\\beta}}_{LS}$",
    col.names = c("Bias of $\\hat{\\beta}_1^{LS}$", "Bias $\\hat{\\beta}_2^{LS}$")
    )

la_bias %>% 
  as_tibble() %>%
  knitr::kable(
    caption = "\\label{tab:la_bias}  Estimated bias of $\\hat{\\vec{\\beta}}_{LA}$",
    col.names = c("Bias of $\\hat{\\beta}_1^{LA}$", "Bias of $\\hat{\\beta}_2^{LA}$")
  )
```

The bias estimation results are shown in Table $\ref{tab:ls_bias}$ and $\ref{tab:la_bias}$.

The variance can be estimated by

$$
\widehat{\text{Var}}_B = \frac{1}{B-1} \sum_{b=1}^B\left(\hat{\beta}^{\star}(b) - \hat{\beta}^{\star}(\cdot)\right)^2,
$$

which we also implement in the following code:

```{r}
bootstrapped_beta_ls %>%
  select(beta1, beta2) %>%
  summarize(
    beta1_var = var(beta1),
    beta2_var = var(beta2)
    ) %>%
  knitr::kable(
    caption="\\label{tab:ls_var} Estimated variance of $\\hat{\\vec{\\beta}}_{LS}$",
    col.names = c("Variance of $\\hat{\\beta}_1^{LS}$", "Variance $\\hat{\\beta}_2^{LS}$")
    )

bootstrapped_beta_la %>%
  select(beta1, beta2) %>%
  summarize(
    beta1_var = var(beta1),
    beta2_var = var(beta2)
    ) %>%
  knitr::kable(
    caption= "\\label{tab:la_var} Estimated variance of $\\hat{\\vec{\\beta}}_{LA}$",
    col.names = c("Variance of $\\hat{\\beta}_1^{LA}$", "Variance $\\hat{\\beta}_2^{LA}$")
    )

```

The results can be seen in Table $\ref{tab:ls_var}$ and $\ref{tab:la_var}$.

The estimated variance for $\hat{\beta}_{LS}$ is a lot higher than for $\hat{\beta}_{LA}$. In addition, the bias seems to be very similar for the two estimators. It's therefore clear that the LS estimator is not optimal for the non-Gaussian time-series we're considering in this
problem, seeing as the LA estimator performs better. 


### A2

In order to obtain a prediction interval for $x_{101}$ we create a prediction for each of the bootstrap samples by sampling a random centered estimated innovation from the set $\{\hat{\epsilon}_3, \ldots, \hat{\epsilon}_T\}$. For a specific bootstrap sample $\hat{\vec{\beta}}^{\star}$ we obtain the prediction by using the relation 

$$
x_{101} = \hat{\beta}^{\star}_1x_{100} + \hat{\beta}^{\star}_2x_{99} + \epsilon.
$$

Again, $\epsilon$ denotes a random sample from the corresponding set of centered estimated innovations.

```{r}
sample_x <- function(beta1, beta2, pi, x) { 
  return(beta2 * x[99] + beta1 * x[100] + sample(pi, 1))
}

ls_sample_x <- partial(sample_x, pi=ls_pi, x = x)
x_samples <- map2_dbl(bootstrapped_beta_ls$beta1, bootstrapped_beta_ls$beta2, ls_sample_x)

x_df <- enframe(x, name="index") %>% 
  bind_rows(
    tibble(
      index=rep(101, length(x_samples)),
      value = x_samples
    )
  ) 
```

After obtaining the predictions we calculate the 2.5th and 97.5th percentiles in order to obtain a $95\%$ confidence interval.

```{r}
x_summary_df <- x_df %>%
  group_by(index) %>%
  summarise(
    mean = mean(value),
    lq = quantile(value, 0.025),
    uq = quantile(value, 0.975)
  )

x_summary_df %>% 
  filter(index == 101) %>%
  select(lq, uq) %>%
  knitr::kable(
    caption = "\\label{tab:conf_int_ls} Confidence interval, LS",
    col.names = c("2.5th percentile", "97.5 percentile")
  )
```

The resulting confidence intervals are shown in Table $\ref{tab:conf_int_ls}$ and $\ref{tab:conf_int_la}$.

```{r, fig.cap="\\label{fig:pred_hist_ls} Predicted values of $x_{101}$ using $\\hat{\\vec{\\beta}}_{LS}$. The black lines show the confidence interval."}
enframe(x_samples, name="index") %>%
  ggplot() +
  geom_histogram(aes(x=value), bins=50) +
  geom_vline(aes(xintercept = x_summary_df %>%
                   filter(index == 101) %>%
                   pull(lq))) +
  geom_vline(aes(xintercept = x_summary_df %>%
                   filter(index == 101) %>%
                   pull(uq)))
```

The histogram showing the distribution of the predictions can be seen in Figure $\ref{fig:pred_hist_ls}$ and $\ref{fig:pred_hist_la}$.

```{r, fig.cap="\\label{fig:pred_hist_la} Predicted values of $x_{101}$ using $\\hat{\\vec{\\beta}}_{LA}$. The black lines show the confidence interval."}
la_sample_x <- partial(sample_x, pi=la_pi, x = x)
  
x_samples <- map2_dbl(bootstrapped_beta_la$beta1, bootstrapped_beta_la$beta2, ls_sample_x)

x_df <- enframe(x, name="index") %>% 
  bind_rows(
    tibble(
      index=rep(101, length(x_samples)),
      value = x_samples
    )
  ) 

x_summary_df <- x_df %>%
  group_by(index) %>%
  summarise(
    mean = mean(value),
    lq = quantile(value, 0.025),
    uq = quantile(value, 0.975)
  )

x_summary_df %>%
  filter(index == 101) %>%
  select(lq, uq) %>%
  knitr::kable(
    caption = "\\label{tab:conf_int_la} Confidence interval, LA",
    col.names = c("2.5th percentile", "97.5 percentile")
  )

enframe(x_samples, name="index") %>%
  ggplot() +
  geom_histogram(aes(x=value), bins=50) +
  geom_vline(aes(xintercept = x_summary_df %>%
                   filter(index == 101) %>%
                   pull(lq))) +
  geom_vline(aes(xintercept = x_summary_df %>%
                   filter(index == 101) %>%
                   pull(uq)))
```

The value of $x_{100}$ is `r x[100]`, and the predictions based on both estimators suggests that $x_{101}$ most likely will increase or decrease by less than $\sim 8$. It's worth noting that the large residual observed in Figure \ref{fig:resids} affects the predicted values by contributing to extreme outliers such as the one seen to the right in Figure \ref{fig:pred_hist_la} and \ref{fig:pred_hist_ls}.

---
output:
  pdf_document: default
  html_document: default
---
```{r, include=FALSE}
set.seed(0)
knitr::set_parent("base.Rmd")

library(tidyverse)
library(broom)
library(tibble)
```

## Problem B

The concentration of bilirubin (mg/dL) in blood samples from three young men was taken by JÃ¸rgensen in 1993.

\begin{table}[htb]
  \begin{tabular}{c|ccccccccccc}
    \hline
    Individual & \multicolumn{11}{c}{Concentration (mg/dL)}                                 \\ \hline
    \texttt{p1} & 0.14 & 0.20 & 0.23 & 0.27 & 0.27 & 0.34 & 0.41 & 0.41 & 0.55 & 0.61 & 0.66 \\
    \texttt{p2} & 0.20 & 0.27 & 0.32 & 0.34 & 0.34 & 0.38 & 0.41 & 0.41 & 0.48 & 0.55 &      \\
    \texttt{p3} & 0.32 & 0.41 & 0.41 & 0.55 & 0.55 & 0.62 & 0.71 & 0.91 &      &      &      \\ \hline
  \end{tabular}
  \caption{Blood concentration of bilirubin from three young, male individuals.}
  \label{tab:bilirubin_data}
\end{table}

The resulting data is shown in table \ref{tab:bilirubin_data}, and provided in the file \texttt{bilirubin.txt}.

```{r}
bilirubin <- here::here("reports", "report_3", "data", "bilirubin.txt") %>%
  read.table(header=T) %>%
  as_tibble()
```

### B1

We start off by creating boxplot in order to explore how logarithm of the concentration is distributed across the three different individuals.

```{r, fig.cap="\\label{fig:boxplot} Boxplot for each of the different groups"}
bilirubin %>%
  mutate(log_meas = log(meas)) %>%
  ggplot(aes(x=pers, y=log_meas)) +
  geom_boxplot()
```

The boxplot is shown in Figure $\ref{fig:boxplot}$. The plots suggest that there might be a difference between the persons, seeing as the variance is considerably lower for \texttt{p2} than it is for the other individuals. We can also observe that the mean is considerably higher for \texttt{p3} than it is for the other individuals. 

Assume the following distribution of the bilirubin concentration $Y$ of individual $i$ and sample $j$:

\begin{align*}
  \log{Y_{ij}} = \beta_i + \epsilon_{ij},
  & \text{with } i = 1, 2, 3 \text{ and } j = 1, ..., n,
\end{align*}

where $n_1 = 11$, $n_2 = 10$, and $n_3 = 8$, and $\epsilon_{ij} \overset{iid}{\sim} \mathcal{N}(0, \sigma^2)$.
We proceed by fitting this linear model to the data, estimating the coefficients $\beta_i,\ i = 1,2,3$ associated with each of the individuals.

```{r}
linear_model <- lm(log(meas)~pers, data=bilirubin)
summary(linear_model)
```

The resulting linear model uses *dummy variable encoding* with estimated regression coefficients
\begin{align*}
  \hat{\beta}_0 &\approx -1.094, \\
  \hat{\beta}_{\texttt{p2}} &\approx 0.064, \\
  \hat{\beta}_{\texttt{p3}} &\approx 0.465.
\end{align*}

The model therefore predicts
$$
  \mathrm{E}\left[Y_{ij} \mid i = 1\right]
  <
  \mathrm{E}\left[Y_{ij} \mid i = 2\right]
  <
  \mathrm{E}\left[Y_{ij} \mid i = 3\right].
$$

So the linear model does in fact distinguish between the individuals to some degree, but is this a good enough reason to conclude that $\beta_i \neq \beta_j$ given $i \neq j$?

In order to investigate this, we want to test whether this model performs significantly better than the null-model, i.e$.$ the model that only uses the mean of the data.
The $F$-test statistic can be used in order to test the hypothesis:
\begin{align*}
  \mathrm{H_0} &: \beta_1 = \beta_2 = \beta_3 \\
  \mathrm{H_1} &: \beta_i \neq \beta_j, \text{ for some } i \neq j
\end{align*}

This test is now performed, and the resulting F-test statistic is saved to `Fval`:

```{r}
Fstat <- linear_model %>%
  glance() %>%
  select(statistic, p.value)

Fval <- Fstat$statistic
Fstat %>% kable(format = "pandoc", caption = "\\label{tab:Fstat}Result of F-test.")
```

The null-hypothesis must be rejected at a significance level of $\alpha = 0.05$, seeing as the $p$-value is less than 0.05.


### B2

A function that randomly assigns the data to the three different groups, fits a linear model to the data and returns the F-statistics is implemented `permTest()` below.

```{r}
permTest <- function(df){
  perm_df <- tibble(
    meas = df %>% pull(meas) %>%sample(),
    pers = df %>% pull(pers)
  )
  lm(log(meas)~pers, data=perm_df) %>% 
    glance() %>% 
    pull(statistic)
}
```

### B3

We now apply `permTest()` in order to generate $999$ samples of the F-statistic:

```{r}
permTest_df <- tibble(
    run = seq(1, 999),
    F_stat = 999 %>% rerun(permTest(bilirubin))
  ) %>%
  unnest()
```

The null-hypothesis for the permutation test is now

$$
  \mathrm{H}_0: \text{All data are from the same distribution}.
$$

The F-statistic resulting from fitting a linear model to the data is suitable for use with the permutation test.
This is because the F-statistic is able to capture some of the information about the difference between the three groups.
The permutation test is performed below.

```{r, fig.cap="\\label{fig:Fhist}Histogram showing the F-statistic obtained from the different permutations. The red line is the 95th percentile while the blue represents the value of the F-statistic computed for the original data."}

perc_95 <- permTest_df %>%
  pull(F_stat) %>% 
  quantile(probs = c(0.95))

permTest_df %>%
  ggplot(aes(x=F_stat)) +
  geom_histogram(bins=50) +
  geom_vline(aes(xintercept=perc_95), color="red") +
  geom_vline(aes(xintercept=Fval), color="blue")

p_value <- permTest_df %>%
  pull(F_stat) %>%
  map_lgl(~.x >= Fval) %>%
  mean()

p_value %>% knitr::kable(
    caption="\\label{tab:pval} The $p$-value of the permutation test.",
    col.names="P-value",
  ) %>%
  kable_styling(latex_options=c("hold_position"))
```

The resulting F-statistics are shown in figure $\ref{fig:Fhist}$.

The associated $p$-value can be seen in table $\ref{tab:pval}$ and can be seen to be below 0.05.
We therefore reject the null-hypothesis at significance level of $\alpha = 0.05$, which agrees with the result obtained from the F-test conducted earlier.

The permutation test is a more robust test, as it requires no model assumptions.
It is not dependent on the residuals $\epsilon_{ij}$ being normally distributed for instance.
This strengthens our belief in the distributions being different, since the conclusion is made with less assumptions.

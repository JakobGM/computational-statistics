## Problem D: Rejection sampling and importance sampling

We now consider a specific recombination rate in genetics given by the data of Rao, C. R. [-@Roa].
197 counts are classified into four categories, $\vec{y} = (y_1, y_2, y_3, y_4)$.
These are assumed to be multinomially distributed, and the data is given in Table 1.

```{r, echo=FALSE}
library(ggplot2)
library(tibble)
library(knitr)
library(kableExtra)
library(purrr)
data <- tibble(
  cellCount = c("$y_1 = 125$", "$y_2 = 18$", "$y_3 = 20$", "$y_4 = 34$"),
  probability = c("$\\frac{1}{2} + \\frac{\\theta}{4}$", "$\\frac{1 - \\theta}{4}$", "$\\frac{1 - \\theta}{4}$", "$\\frac{\\theta}{4}$")
)
data %>%
  kable(
    col.names = c("Cell count", "Probability"),
    caption = "Table 1: Genetic linkage data"
  ) %>%
  kable_styling()
```

The multinomial mass function is given, proportionally, by

$$
f(\vec{y} | \theta) \propto (2 + \theta)^{y_1} (1-\theta)^{y_2 + y_3} \theta^{y_4}.
$$

Using Beta(1, 1) as a prior for $\theta$, i.e. a uniform prior, yields the following posterior density

$$
\begin{align}
  f(\theta | \vec{y}) =& d \cdot h(\theta | \vec{y})& \\
  h(\theta) :=& (2 + \theta)^{y_1} (1-\theta)^{y_2 + y_3} \theta^{y_4}& \\
  d :=& \int_0^1 h(\theta | \vec{y}) \text{d}\theta > 0
\end{align}
$$

Here we have introduced the unknown normalizing constant $d$ for $h(\theta | \vec{y})$.

### 1. Rejection sampling algorithm

We will now implement the rejection sampling algorithm for $f(\theta|\vec{y})$.
The proposal density, $g(\theta | \vec{y})$, is chosen to be the uniform distribution $\mathcal{U}(0, 1)$.
Thus,

$$
g(\theta | \vec{y}) \equiv 1.
$$

The acceptance probability thus becomes

$$
\alpha = \frac{1}{c} \cdot \frac{f(\theta)}{g(\theta)} = \frac{d}{c} \cdot h(\theta | \vec{y}) \in [0, 1].
$$

Notice that neither $c$ nor $d$ are known to us, but we can numerically approximate their proportion as

$$
\frac{1}{\beta} := \frac{c}{d} = \max_{\theta} h(\theta | \vec{y})
$$

Such that the acceptance probability can be written as
$$
\alpha = \beta \cdot h(\theta | \vec{y})
$$

We now implement a function constructor, which given $\vec{y}$, returns $\hat{f}(\theta)$ such that
$\max_{\theta} \hat{f}(\theta) = 1$.

```{r}
construct_f <- function(y) {
  #' Return proportional distribution function for f which satisfies max(f) = 1

  # Proportional function of f in log-space
  unscaled_log_f <- function(theta) {
    y[1] * log(2 + theta) + (y[2] + y[3]) * log(1 - theta) + y[4] * log(theta)
  }

  # Find the maxima of this function in log-space
  log_maxima <- optimize(
    f = unscaled_log_f,
    maximum = TRUE,
    interval = c(0, 1)
  )$objective
  
  # Scale f to have max value 1
  # NB! This does not integrate to 1, so it is not a proper density function!
  # You can use make_density for that purpose.
  scaled_f <- function(theta) {
    return(exp(unscaled_log_f(theta) - log_maxima))
  }
  return(scaled_f)
}
```

Remember that this is not a proper density function, as it does not integrate to $1$.
Another function constructor can be implemented to normalize $\hat{f}$. We will use
this function later for plotting comparisons.

```{r}
make_density <- function(f, lower = 0, upper = 1) {
  #' Given f, returns a new function which integrates to 1 over the given interval
  normalizer = integrate(f, lower = lower, upper = upper)$value
  normalized_function <- function(...) {
    return(f(...) / normalizer)
  }
  return(normalized_function)
}
```

We can now implement the rejection sampling algorithm, using the constructor function.
The function will also return the total number of generated proposals, which will be
used in subtask c).

```{r}
sample_theta <- function(n, y) {
  #' Generate n theta samples from f function, given 4-vector y
  #' Returns a list with thetas key to $theta,
  #' and the number of tries keyed to $tries.
  f_density <- construct_f(y = y)
  found <- vector()
  tries <- 0
  while(length(found) < n) {
    # Number of samples that remain to be found
    remaining <- n - length(found)
    tries <- tries + remaining

    # These are proposed values that might be accepted...
    x <- runif(remaining)

    # ... with acceptance probability
    alpha <- f_density(x)

    # Append the values that get accepted
    u <- runif(remaining)
    success <- u <= alpha
    found <- c(found, x[success])
  }
  return(list(theta = found, tries = tries))
}
```

We can now plot the acceptance probability, $\alpha$, as a function of $\theta$.
We will here use $\vec{y} = (125, 18, 20, 34)$, as in the provided dataset.

```{r}
y <- c(125, 18, 20, 34)
f_scaled_density <- construct_f(y = y)
df <- enframe(rnorm(1))
ggplot(data = df) + aes(x = value) +
  stat_function(
    fun = f_scaled_density,
    xlim = c(0, 1),
    mapping = aes(col = 'Acceptance probability'),
    geom = "area",
    fill = "gray"
  ) +
  geom_hline (
    yintercept = 1,
    mapping = aes(col = 'Uniform distribution')
  ) +
  scale_y_continuous(
    name = expression(alpha)
  ) +
  scale_x_continuous(
    name = expression(theta)
  ) +
  labs(
    caption = "Acceptance probability as a function of proposed parameter theta."
  )
```

We can now observe that the rejection algorithm will sample values mostly within the interval $[0.5, 0.75]$.
Realized samples will be investigated in the following section.

### 2. Posterior mean by Monte-Carlo integration

Now we sample ten million samples, $\theta_i$, from the implemented rejection algorithm.

```{r, cache=TRUE}
M <- 10000000
theta_sampling_result <- sample_theta(n = M, y = y)
theta_samples <- enframe(theta_sampling_result$theta)
```

The posterior mean can be practically derived from the samples as

$$
E\big[\theta~|~p(\theta) \sim \mathcal{U}(0, 1)\big] \approx \frac{1}{M} \sum_{i = 1}^M \theta_i
$$
Or equivalently, in R

```{r}
thetaSampleMean <- mean(theta_samples$value)
```

In order to check the correctness of the rejection sampling algorithm, we can approximate the posterior mean
by numerically solving the integral

$$
E\big[\theta~|~p(\theta) \sim \mathcal{U}(0, 1)\big] = \int_0^1 \theta \cdot f(\theta | \vec{y})~\text{d}\theta.
$$

This is done by using the $\mathtt{integrate}$ R function

```{r}
f_density <- make_density(f_scaled_density)
thetaExpectedMean <- integrate(
  f = function(theta) theta * f_density(theta),
  lower = 0,
  upper = 1
)$value
```

All these results can now be shown in a comparison plot.

```{r}
means <- tibble(
  xint = c(thetaSampleMean, thetaExpectedMean),
  grp = c("Sample mean", "Numerical mean")
)
binWidth = 0.001
theta_samples %>%
  ggplot() +
  geom_histogram(
    mapping = aes(x = value, y = ..density..),
    binwidth = binWidth,
    boundary = 0,
    size = 0
  ) +
  geom_vline(
    data = means,
    aes(
      xintercept = xint,
      col = grp,
      linetype = c("dashed", "dotted")
    )
  ) +
  stat_function(
    fun = f_density,
    xlim = c(0, 1),
    aes(col = 'Theoretical density')
  ) +
  guides(linetype = FALSE) +
  scale_x_continuous(
    name = expression(theta),
    limits = c(0, 1)
  ) +
  labs(
    title = "Histogram of parameter samples",
    caption = "The histogram of the samples is colored in grey."
  )
```

The sample histogram, which is colored in grey, perfectly coincides with the theoretical density.
This is what we would expect with ten million samples. The same can be said of the sample mean
and numerical mean. We can therefore conclude that the rejection sampling algorithm has been 
correctly implemented.

The calculated posterior means are shown in the following table.

```{r}
means %>%
  kable(
    col.names = c("Posterior mean", "Method"),
    caption = "Table 2: Posterior mean calculation comparison"
  ) %>%
  kable_styling()
```

These values are close enough to conclude that the implementation is correct.

### 3. Required iterations for one sample

The overall acceptance rate for the rejection sampling algorithm is $c^{-1}$.
We would therefore expect, on average, to generate $c$ samples before one is
accepted. Since $\mathcal{U}(0, 1)$ is used as the proposal density, we can
numerically calculate $c$ as

$$
c = \max_{\theta \in [0, 1]} f(\theta | \vec{y})
$$

We can compare this theoretical result with the numerical one, calculated earlier

```{r}
average_theta_tries <- theta_sampling_result$tries / M
cNumeric = optimize(
  f = f_density,
  interval = c(0, 1),
  maximum = TRUE
)$objective
attempts <- tibble(
  method = c("Sampling", "Theoretical"),
  attempts = c(average_theta_tries, cNumeric)
)
attempts %>%
  kable(
    caption = "Table 3: Required proposals for each accepted sample"
    col.names = c("Method", "Required proposals")
  ) %>%
  kable_styling()
```

The acceptance rate is close to the theoretical optimal.
We have now confirmed both the validity and optimality of the implemented
algorithm within the bounds of the assigned problem.


### 4. New prior

```{r}
alpha <- 1
beta <- 5
tibble(x = c(0, 1)) %>%
  ggplot(aes(x)) +
  stat_function(
    fun = partial(dgamma, shape = alpha, scale = beta),
    aes(col = "Beta(1, 5)")
  )
```

```{r}
posteriorMean <- function(theta_samples, beta = 5) {
  weights <- (1 - theta_samples) ** (beta - 1)
  importanceThetaMean <- sum(theta_samples * weights) / sum(weights)
  return(importanceThetaMean)
}
importanceThetaMean <- posteriorMean(theta_samples = theta_samples$value)

generatePosterior <- function(y, alpha = 1, beta = 5) {
  log_unscaled = function(theta) {
    y[1] * log(2 + theta) + (y[2] + y[3] + beta - 1) * log(1 - theta) + (y[4] + alpha - 1) * log(theta)
  }
  normalizingConstant <- integrate(
    f = function(theta) exp(log_unscaled(theta)),
    lower = 0,
    upper = 1
  )$value
  print(normalizingConstant)
  scaledPosterior <- function(theta) {
    return(exp(log_unscaled(theta)) /normalizingConstant)
  }
  return(scaledPosterior)
}
newPosterior <- generatePosterior(y = y, beta = 5)
newPosteriorIntegralMean <- integrate(
  f = function(theta) theta * newPosterior(theta),
  lower = 0,
  upper = 1
)$value

oldPosterior <- generatePosterior(y = y, beta = 1)
oldPosteriorIntegralMean <- integrate(
  f = function(theta) theta * oldPosterior(theta),
  lower = 0,
  upper = 1
)$value

meanResults <- tibble(
  posteriorMeans=c(
    mean(theta_samples$value),
    importanceThetaMean,
    oldPosteriorIntegralMean,
    newPosteriorIntegralMean
  ),
  grp=c(
    "Old posterior asymptotic",
    "New posterior asymptotic",
    "Old posterior numerical integration",
    "New posterior numerical integration"
  )
)

# Plot posterior mean progression as samples increase
row_seq <- seq(0, 10000, 10)
thetaSubsets <- lapply(row_seq, function(k) theta_samples$value[1:k])
meanProgression <- tibble(
  iteration = row_seq,
  oldPosterior = sapply(thetaSubsets, mean),
  newPosterior = sapply(thetaSubsets, posteriorMean)
)

meanProgression %>%
  ggplot(aes(x = iteration, posteriorMean)) +
  geom_line(
    aes(y = oldPosterior, col="Old posterior")
  ) +
  geom_line(
    aes(y = newPosterior, col="New posterior")
  ) +
  geom_hline(
    data = meanResults,
    aes(
      yintercept = posteriorMeans,
      col = grp
    ),
    linetype = c("solid", "solid", "dashed", "dashed"),
    alpha = 0.8
  ) +
  ylim(0.59, 0.625) +
  theme(legend.position = "bottom") +
  guides(col=guide_legend(nrow=3, byrow = FALSE)) +
  ylab("Posterior mean") +
  xlab("Iteration") +
  scale_y_continuous(
    breaks = c(
      seq(0.59, 0.63, 0.01),
      round(oldPosteriorIntegralMean, digits = 3),
      round(newPosteriorIntegralMean, digits = 3)
    )
  )
```

## Problem D: Rejection sampling and importance sampling

We now consider a specific recombination rate in genetics given by the data of Rao, C. R. [-@Roa].
197 counts are classified into four categories, $\vec{y} = (y_1, y_2, y_3, y_4)$.
These are assumed to be multinomially distributed, and the data is given in Table 1.

```{r, echo=FALSE}
library(ggplot2)
library(tibble)
library(knitr)
library(kableExtra)
library(purrr)
data <- tibble(
  cellCount = c("$y_1 = 125$", "$y_2 = 18$", "$y_3 = 20$", "$y_4 = 34$"),
  probability = c("$\\frac{1}{2} + \\frac{\\theta}{4}$", "$\\frac{1 - \\theta}{4}$", "$\\frac{1 - \\theta}{4}$", "$\\frac{\\theta}{4}$")
)
data %>%
  kable(
    col.names = c("Cell count", "Probability"),
    caption = "Table 1: Genetic linkage data"
  ) %>%
  kable_styling()
```

The multinomial mass function is given (proportionally) by

$$
f(\vec{y} | \theta) \propto (2 + \theta)^{y_1} (1-\theta)^{y_2 + y_3} \theta^{y_4}
$$

Using Beta(1, 1) as a prior for $\theta$, i.e. a uniform prior, yields the following posterior density

$$
\begin{align}
  f(\theta | \vec{y}) =& d \cdot h(\theta | \vec{y})& \\
  h(\theta) :=& (2 + \theta)^{y_1} (1-\theta)^{y_2 + y_3} \theta^{y_4}& \\
  d :=& \int_0^1 h(\theta | \vec{y}) \text{d}\theta > 0
\end{align}
$$

Here we have introduced the unknown normalizing constant $d$ for $h(\theta | \vec{y})$.

### 1. Rejection sampling algorithm

We will now implement the rejection sampling algorithm for $f(\theta|\vec{y})$ with $g(\theta | \vec{y}) = 1 \sim \mathcal{U}(0, 1)$ as the proposal density.
The acceptance probability thus becomes

$$
\alpha = \frac{1}{c} \cdot \frac{f(\theta)}{g(\theta)} = \frac{d}{c} \cdot h(\theta | \vec{y}) \in [0, 1].
$$
Notice that neither $c$ nor $d$ are known to us, but we can approximate their proportion as

$$
\frac{1}{\beta} := \frac{c}{d} = \max_{\theta} h(\theta | \vec{y})
$$

Such that the acceptance probability can be written as
$$
\alpha = \beta \cdot h(\theta | \vec{y})
$$

```{r}
construct_f <- function(y) {
  #' Construct f function which is scaled to max(f) = 1
  unscaled_log_f <- function(theta) {
    y[1] * log(2 + theta) + (y[2] + y[3]) * log(1 - theta) + y[4] * log(theta)
  }
  log_maxima <- optimize(
    f = unscaled_log_f,
    maximum = TRUE,
    interval = c(0, 1)
  )$objective
  
  # Scale f to have max value 1
  # NB! This does not integrate to 1, so it is not a proper density function!
  # You can use make_density for that purpose.
  scaled_f <- function(theta) {
    return(exp(unscaled_log_f(theta) - log_maxima))
  }
  return(scaled_f)
}

sample_theta <- function(n, y) {
  #' Generate n theta samples from f function, given 4-vector y
  #' Returns a list with thetas key to $theta,
  #' and the number of tries keyed to $tries.
  f_density <- construct_f(y = y)
  found <- vector()
  tries <- 0
  while(length(found) < n) {
    # Number of samples that remain to be found
    remaining <- n - length(found)
    tries <- tries + remaining

    # These are proposed values that might be accepted...
    x <- runif(remaining)

    # ... with acceptance probability
    alpha <- f_density(x)

    # Append the values that get accepted
    u <- runif(remaining)
    success <- u <= alpha
    found <- c(found, x[success])
  }
  return(list(theta = found, tries = tries))
}
```

```{r}
y <- c(125, 18, 20, 34)
f_scaled_density <- construct_f(y = y)
df <- enframe(rnorm(1))
ggplot(data = df) + aes(x = value) +
  stat_function(
    fun = f_scaled_density,
    xlim = c(0, 1),
    mapping = aes(col = 'Density'),
    geom = "area",
    fill = "gray"
  ) +
  geom_hline (
    yintercept = 1,
    mapping = aes(col = 'Uniform distribution')
  )
```


```{r, cache=TRUE}
N <- 10000000
theta_sampling_result <- sample_theta(n = N, y = y)
theta_samples <- enframe(theta_sampling_result$theta)
theta_tries <- theta_sampling_result$tries
```
```{r}
make_density <- function(f, lower = 0, upper = 1) {
  normalizer = integrate(f, lower = lower, upper = upper)$value
  normalized_function <- function(...) {
    return(f(...) / normalizer)
  }
  return(normalized_function)
}
f_density <- make_density(f_scaled_density)
```

```{r}
thetaSampleMean <- mean(theta_samples$value)
thetaExpectedMean <- integrate(
  f = function(theta) theta * f_density(theta),
  lower = 0,
  upper = 1
)$value
means <- tibble(
  xint = c(thetaSampleMean, thetaExpectedMean),
  grp = c("Sample mean", "Expected mean")
)

binWidth = 0.001
theta_samples %>%
  ggplot() +
  geom_histogram(
    mapping = aes(x = value, y = ..density..),
    binwidth = binWidth,
    boundary = 0,
    size = 0
  ) +
  geom_vline(
    data = means,
    aes(
      xintercept = xint,
      col = grp,
      linetype = c("dashed", "dotted")
    )
  ) +
  stat_function(
    fun = f_density,
    xlim = c(0, 1),
    aes(col = 'f')
  ) +
  xlim(0, 1) +
  guides(linetype = FALSE)
```


### 2. Posterior mean by Monte-Carlo integration



### 3. Required iterations for one sample

```{r}
theta_tries / N
```

```{r}
cInverse = optimize(
  f = f_density,
  interval = c(0, 1),
  maximum = TRUE
)$objective
cInverse
```


### 4. New prior

```{r}
alpha <- 1
beta <- 5
tibble(x = c(0, 1)) %>%
  ggplot(aes(x)) +
  stat_function(
    fun = partial(dgamma, shape = alpha, scale = beta),
    aes(col = "Beta(1, 5)")
  )
```

```{r}
posteriorMean <- function(theta_samples, beta = 5) {
  weights <- (1 - theta_samples) ** (beta - 1)
  importanceThetaMean <- sum(theta_samples * weights) / sum(weights)
  return(importanceThetaMean)
}
importanceThetaMean <- posteriorMean(theta_samples = theta_samples$value)

generatePosterior <- function(y, alpha = 1, beta = 5) {
  log_unscaled = function(theta) {
    y[1] * log(2 + theta) + (y[2] + y[3] + beta - 1) * log(1 - theta) + (y[4] + alpha - 1) * log(theta)
  }
  normalizingConstant <- integrate(
    f = function(theta) exp(log_unscaled(theta)),
    lower = 0,
    upper = 1
  )$value
  print(normalizingConstant)
  scaledPosterior <- function(theta) {
    return(exp(log_unscaled(theta)) /normalizingConstant)
  }
  return(scaledPosterior)
}
newPosterior <- generatePosterior(y = y, beta = 5)
newPosteriorIntegralMean <- integrate(
  f = function(theta) theta * newPosterior(theta),
  lower = 0,
  upper = 1
)$value

oldPosterior <- generatePosterior(y = y, beta = 1)
oldPosteriorIntegralMean <- integrate(
  f = function(theta) theta * oldPosterior(theta),
  lower = 0,
  upper = 1
)$value

meanResults <- tibble(
  posteriorMeans=c(
    mean(theta_samples$value),
    importanceThetaMean,
    oldPosteriorIntegralMean,
    newPosteriorIntegralMean
  ),
  grp=c(
    "Old posterior asymptotic",
    "New posterior asymptotic",
    "Old posterior numerical integration",
    "New posterior numerical integration"
  )
)

# Plot posterior mean progression as samples increase
row_seq <- seq(0, 10000, 10)
thetaSubsets <- lapply(row_seq, function(k) theta_samples$value[1:k])
meanProgression <- tibble(
  iteration = row_seq,
  oldPosterior = sapply(thetaSubsets, mean),
  newPosterior = sapply(thetaSubsets, posteriorMean)
)

meanProgression %>%
  ggplot(aes(x = iteration, posteriorMean)) +
  geom_line(
    aes(y = oldPosterior, col="Old posterior")
  ) +
  geom_line(
    aes(y = newPosterior, col="New posterior")
  ) +
  geom_hline(
    data = meanResults,
    aes(
      yintercept = posteriorMeans,
      col = grp
    ),
    linetype = c("solid", "solid", "dashed", "dashed"),
    alpha = 0.8
  ) +
  ylim(0.59, 0.625) +
  theme(legend.position = "bottom") +
  guides(col=guide_legend(nrow=3, byrow = FALSE)) +
  ylab("Posterior mean") +
  xlab("Iteration") +
  scale_y_continuous(
    breaks = c(
      seq(0.59, 0.63, 0.01),
      round(oldPosteriorIntegralMean, digits = 3),
      round(newPosteriorIntegralMean, digits = 3)
    )
  )
```

## Problem D: Rejection sampling and importance sampling

We now consider a specific recombination rate in genetics given by the data of Rao, C. R. [-@Roa].
197 counts are classified into four categories, $\vec{y} = (y_1, y_2, y_3, y_4)$.
These are assumed to be multinomially distributed, and the data is given in Table 1.

```{r, echo=FALSE}
library(ggplot2)
library(tibble)
library(knitr)
library(kableExtra)
data <- tibble(
  cellCount = c("$y_1 = 125$", "$y_2 = 18$", "$y_3 = 20$", "$y_4 = 34$"),
  probability = c("$\\frac{1}{2} + \\frac{\\theta}{4}$", "$\\frac{1 - \\theta}{4}$", "$\\frac{1 - \\theta}{4}$", "$\\frac{\\theta}{4}$")
)
data %>%
  kable(
    col.names = c("Cell count", "Probability"),
    caption = "Table 1: Genetic linkage data"
  ) %>%
  kable_styling()
```

The multinomial mass function is given (proportionally) by

$$
f(\vec{y} | \theta) \propto (2 + \theta)^{y_1} (1-\theta)^{y_2 + y_3} \theta^{y_4}
$$

Using Beta(1, 1) as a prior for $\theta$, i.e. a uniform prior, yields the following posterior density

$$
\begin{align}
  f(\theta | \vec{y}) =& d \cdot h(\theta | \vec{y})& \\
  h(\theta) :=& (2 + \theta)^{y_1} (1-\theta)^{y_2 + y_3} \theta^{y_4}& \\
  d :=& \int_0^1 h(\theta | \vec{y}) \text{d}\theta > 0
\end{align}
$$

Here we have introduced the unknown normalizing constant $d$ for $h(\theta | \vec{y})$.

### 1. Rejection sampling algorithm

We will now implement the rejection sampling algorithm for $f(\theta|\vec{y})$ with $g(\theta | \vec{y}) = 1 \sim \mathcal{U}(0, 1)$ as the proposal density.
The acceptance probability thus becomes

$$
\alpha = \frac{1}{c} \cdot \frac{f(\theta)}{g(\theta)} = \frac{d}{c} \cdot h(\theta | \vec{y}) \in [0, 1].
$$
Notice that neither $c$ nor $d$ are known to us, but we can approximate their proportion as

$$
\frac{1}{\beta} := \frac{c}{d} = \max_{\theta} h(\theta | \vec{y})
$$

Such that the acceptance probability can be written as
$$
\alpha = \beta \cdot h(\theta | \vec{y})
$$

```{r}
construct_f <- function(y) {
  unscaled_log_f <- function(theta) {
    y[1] * log(2 + theta) + (y[2] + y[3]) * log(1 - theta) + y[4] * log(theta)
  }
  log_maxima <- optimize(
    f = unscaled_log_f,
    maximum = TRUE,
    interval = c(0, 1)
  )$objective
  
  # Scale f to have max value 1
  # NB! This does not integrate to 1, so it is not a proper density function!
  # You can use make_density for that purpose.
  scaled_f <- function(theta) {
    return(exp(unscaled_log_f(theta) - log_maxima))
  }
  return(scaled_f)
}

sample_theta <- function(n, y) {
  f_density <- construct_f(y = y)
  found <- vector()
  while(length(found) < n) {
    x <- runif(1)
    alpha <- f_density(x)
    u <- runif(1)
    if (u <= alpha) {
      found <- c(found, x)
    }
  }
  return(found)
}
```

```{r}
y <- c(125, 18, 20, 34)
f_density <- construct_f(y = y)
df <- enframe(rnorm(10000))
ggplot(data = df) + aes(x = value) +
  stat_function(
    fun = f_density,
    xlim = c(0, 1),
    mapping = aes(col = 'Density'),
    geom = "area",
    fill = "gray"
  ) +
  geom_hline (
    yintercept = 1,
    mapping = aes(col = 'Uniform distribution')
  )
```


```{r, cache=TRUE}
N <- 100000
theta_samples <- enframe(sample_theta(n = N, y = y))
```
```{r}
fNormalizer <- integrate(f_density, lower = 0, upper = 1)$value
f_normalizedDensity <- function(theta) {
  return(f_density(theta) / fNormalizer)
}
make_density <- function(f, lower = 0, upper = 1) {
  normalizer = integrate(f, lower = lower, upper = upper)$value
  normalized_function <- function(...) {
    return(f(...) / normalizer)
  }
  return(normalized_function)
}
```

```{r}
thetaMean <- mean(theta_samples$value)
binWidth = 0.005
theta_samples %>%
  ggplot() +
  geom_histogram(
    mapping = aes(x = value, y = ..density..),
    binwidth = binWidth,
    boundary = 0,
    size = 0
  ) +
  geom_vline(
    xintercept = thetaMean,
    show.legend = TRUE,
    aes(col = 'Sample mean')
  ) +
  stat_function(
    fun = make_density(f_density),
    xlim = c(0, 1),
    aes(col = 'f')
  ) +
  xlim(0, 1)
```


### 2. Posterior mean by Monte-Carlo integration

(Answer)


### 3. Required iterations for one sample

(Answer)


### 4. New prior

(Answer)

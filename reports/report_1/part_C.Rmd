## Problem C: The Dirichlet distribution: simulating using known relations

```{r, echo=FALSE}
library(kableExtra)
```

Define the $K$ dimensional stochastic vector $\vec{x} := (x_1, ..., x_K)$ where $x_k \in [0, 1]$ and $\sum_{x = 1}^{K} x_k = 1$.
Since $x_K$ is determined by $(x_1, ..., x_{K - 1})$, define $\tilde{x} := (x_1, ..., x_{K - 1})$.
Also, define the the parameter vector $\vec{\alpha} = (\alpha_1, ..., \alpha_K).

The vector $\vec{x}$ is said to have a Dirichlet distribution with parameter $\vec{\alpha}$ when the probability density for $\tilde{x}$ is given by

$$
f(\tilde{x}; \vec{\alpha}) = \frac{\Gamma(\sum_{k = 1}^{K} \alpha_k)}{\prod_{k = 1}^{K}{\alpha_k}} \cdot \bigg( \prod_{k = 1}^{K - 1}{x_k^{\alpha_k - 1}} \bigg) \cdot \bigg(1 - \sum_{k = 1}^{K - 1} x_k \bigg)^{\alpha_K - 1}
$$

for $x_1, ..., x_{K - 1} > 0$ and $\sum_{k = 1}^{K - 1} x_k < 1$.

### 1. Theory

Define a new $K$-dimensional stochastic vector $\vec{z} := (z_1, ... z_K)$ where $z_k \sim \text{gamma}(\alpha_k, 1)$, where $z_k$ are independently distributed. We will now show that the transformation

$$
x_k = \frac{z_k}{\sum_{k=1}^{K} z_k}
$$
results in $\vec{x}$ being Dirichlet distributed with parameter vector $\vec{\alpha}$.

Since the $z_k$'s are identically and independently distributed, the joint distribution can be found by

$$
\begin{align}
  f_{z_k}(z_k; \alpha_k) &= \frac{z_k^{\alpha_k - 1} e^{-z_k}}{\Gamma(\alpha_k)} \\
  \implies f_z(\vec{z}; \vec{\alpha}) &= \prod_{k = 1}^{K} f_{z_k}(z_k; \alpha_k)
  = \prod_{k = 1}^{K} \left( \frac{z_k^{\alpha_k - 1}}{\Gamma(\alpha_k)} \right) e^{-v} \\
  v :&= \sum_{k = 1}^{K} z_k.
\end{align}
$$

Now, perform a transformation, $h$, of the variable $\vec{z}$ to $(x_1, ..., x_{K - 1}, v)$, with $v$ as defined above. This yields

$$
\begin{align}
  x_k &= \frac{z_k}{v} \\
  \implies z_k &=
  \begin{cases}
    h_k(x_k, v) = v \cdot x_k &, k \in \left{1,~...,~K - 1\right} \\
    h_K(\tilde{x}, v) = v \cdot (1 - \sum_{k = 1}^{K - 1} x_k), &, k = K
  \end{cases} \\
  \implies \vec{z} &= v \vec{x}
  = \left(v \cdot x_1,~...,~v \cdot x_{K - 1}, v \cdot \left[1 - \sum_{k = 1}^{K - 1} x_k\right]\right).
\end{align}
$$

The change-of-variables formula gives the new joint distribution as

$$
f_x(\tilde{x}, v; \vec{\alpha}) = f_z(\vec{z}; \vec{\alpha}) \cdot |J|,
$$

where $|J|$ is the determinant of the Jacobian. It is defined by

$$
J_{j, k} = \frac{\text{d}z_k}{\text{d}x_j},
$$

for row $j$ and column $k$. For brevity's sake we've defined $x_K := v$ in order to make the above expression hold for all $j, k$.

The elements of the Jacobian can be found by calculating $\frac{dh_k}{dx_j}$ in four distinct cases.

$$
\frac{dh_k}{dx_j} = \frac{d}{dx_j} vx_j = \begin{cases} v,\ k = j \\ 0,\ k \neq j\end{cases}, k, j \in \{1,\ldots, K - 1\} \\

\frac{dh_K}{dx_j} = \frac{d}{dx_j}v\left(1 - \sum_{k=1}^{K-1}x_k\right) =-v,\ j \in \{1,\ldots, K - 1\} \\

\frac{dh_k}{dv} =  \frac{d}{dv} vx_k = x_k,\ k \in \{1,\ldots, K - 1\} \\

\frac{dh_K}{dv} =  \frac{d}{dv}v\left(1 - \sum_{k=1}^{K-1}x_k\right) = 1 - \sum_{k=1}^{K-1}x_k
$$

The Jacobian matrix can be reduced to upper triangular form by $K - 1$ row additions
without changing its determinant.
The determinant of an upper triangular matrix is the product of its diagonal entries, which results in

$$
\det
\begin{bmatrix}
  v  & 0  & \dots  & 0  & x_1 \\
  0  & v  & \dots  & 0  & x_2 \\
  \vdots  & \vdots  & \ddots  & \vdots  & \vdots \\
  0  & 0  & \dots  & v  & x_{K - 1} \\
  -v  & -v  & \dots  & -v  & 1 - \sum_{k = 1}^{K - 1} x_k
\end{bmatrix}
= 
\det
\begin{bmatrix}
  v  & 0  & \dots  & 0  & x_1 \\
  0  & v  & \dots  & 0  & x_2 \\
  \vdots  & \vdots  & \ddots  & \vdots  & \vdots \\
  0  & 0  & \dots  & v  & x_{K - 1} \\
  0  & 0  & \dots  & 0  & 1
\end{bmatrix}
=
v^{k - 1}
$$
Inserting the expressions $z_k,\ k = 0,\ldots,K$ into $f_z(\vec{z}; \vec{\alpha})$ yields 

$$
f_z(\tilde{x}, v ; \vec{\alpha}) = \frac{\prod_{k=1}^{K-1}x_k^{\alpha_k - 1}}{\prod_{k=1}^K \Gamma(\alpha_k)}\left(1 - \sum_{k=1}^{K-1}x_k\right)^{\alpha_K - 1}\frac{v^{\sum_{k=1}^{K}\alpha_k}}{v^K}e^{-v}
$$
Multiplying by this by the Jacobian leaves us with 

$$
f_z(\tilde{x}, v ; \vec{\alpha}) = \frac{\prod_{k=1}^{K-1}x_k^{\alpha_k - 1}}{\prod_{k=1}^K \Gamma(\alpha_k)}\left(1 - \sum_{k=1}^{K-1}x_k\right)^{\alpha_K - 1}v^{\sum_{k=1}^{K}\alpha_k - 1}e^{-v}
$$
We recognize the two latter terms as the kernel of the gamma function, and by integrating out $v$ we obtain

$$
f(\tilde{x} ; \vec{\alpha}) = \int_0^{\infty}f_z(\tilde{x}, v ; \vec{\alpha}) dv \\
= \frac{\prod_{k=1}^{K-1}x_k^{\alpha_k - 1}}{\prod_{k=1}^K \Gamma(\alpha_k)}\left(1 - \sum_{k=1}^{K-1}x_k\right)^{\alpha_K - 1}\int_0^{\infty}v^{\sum_{k=1}^{K}\alpha_k - 1}e^{-v} dv \\
= \frac{\prod_{k=1}^{K-1}x_k^{\alpha_k - 1}}{\prod_{k=1}^K \Gamma(\alpha_k)}\left(1 - \sum_{k=1}^{K-1}x_k\right)^{\alpha_K - 1}\Gamma\left(\sum_{k=1}^{K}\alpha_k\right)
$$
This is the Dirichlet distribution, as we wanted to show.


### 2. Implementation

We will now implement a random sampling algorithm for the Dirichlet distribution, as explained above.

```{r}
rdirichlet <- function(n, alpha) {
  # Dimension of Dirichlet distribution
  K <- length(alpha)
  
  # A tibble which will contain samples from Gamma(alpha_k, 1)
  zValues <- tibble(n = seq(1, n))
  
  for (k in seq_along(alpha)) {
    # Generate n samples from Gamma(alpha_k, 1)
    z <- random_sample_from_f(
      n = n,
      alpha = alpha[k],
      beta = 1
    )
    
    # Add these values to column named x_k,
    # since they will be transformed z_k -> x_k later
    zValues <- add_column(
      zValues,
      !!(paste("x_", toString(k), sep = "")) := z
    )
  }
  
  # Delete unnecessary column named "n"
  zValues <- zValues[-1]
  
  # Find v, the sum of the z_k's
  v <- rowSums(zValues)
  
  # Perform variable transformation z_k -> x_k
  xValues <- zValues / v
  
  # Return these values, they are now Dirichlet distributed
  return(xValues)
}
```

In order to validate the algorithm, we will draw 10 million samples with $\vec{\alpha} = (1, 3, 7, 10)$.
The sample mean and variance can be compared to the theoretical mean and variance, which are given without proof

$$
\begin{align}
  \text{E}[X_k] &= \frac{\alpha_k}{\alpha_0} \\
  \text{Var}(X_k) &= \frac{\alpha_k (\alpha_0 - \alpha_k)}{\alpha_0^2 (\alpha_0 + 1)} \\
  \alpha_0 :&= \sum_{k = 1}^K \alpha_k
\end{align}
$$

```{r}
N <- 10000000
alpha <- c(1, 3, 7, 10)
alphaSum <- sum(alpha)
expectedMean <- alpha / alphaSum
expectedVariance <- alpha * (alphaSum - alpha) / (alphaSum ** 2 * (alphaSum + 1))

xValues <- rdirichlet(n = N, alpha = alpha)
sampleMean <- colMeans(xValues)
sampleVariance <- sapply(xValues, var)

comparison <- tibble(
  x = c("$x_1$", "$x_2$", "$x_3$", "$x_4$"),
  sampleMean = sampleMean,
  expectedMean = expectedMean,
  sampleVariance = sampleVariance,
  expectedVariance = expectedVariance
)

comparison %>%
  kable(
    caption = "Dirichlet sample statistics comparison",
    col.names = c(
      "$x$",
      "Sample mean",
      "Theoretical mean",
      "Sample variance",
      "Theoretical variance"
    )
  ) %>%
  kable_styling()
```

Both the sample mean and variance is well within acceptable bounds from the theoretical values. We therefore conclude that the implementation is correct.

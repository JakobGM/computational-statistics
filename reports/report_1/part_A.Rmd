## Problem A: Stochastic simulation by the probability integral transform and bivariate techniques


### 1. Sampling from the exponential distribution

We want to generate $n$ samples the exponential distribution with rate parameter $\lambda$.

```{r}
library(tidyverse)
library(magrittr)
library(numbers)
```

```{r}
rexp <- function(n, rate = 1) {
  uniformly_distributed <- runif(n=n)
  exponentially_distributed <- -log(uniformly_distributed) / rate
  return(enframe(exponentially_distributed))
}
```

```{r}
samples <- 10000
rate <- 1
exponential_samples <- rexp(n = samples, rate = rate)
ggplot() + 
  geom_histogram(
    data = exponential_samples,
    mapping = aes(x=value, y=..density..),
    binwidth = 0.1,
    boundary = 0
  ) +
  geom_vline(
    aes(xintercept = mean(exponential_samples$value))
  ) +
  stat_function(
    fun = dexp,
    args=list(rate = rate),
    aes(col='Theoretical density')
  )
```


### 2. Probability density function

We now consider the probability density function
$$
g(x) =
\begin{cases}
  cx^{\alpha - 1}, & 0 < x < 1, \\
  ce^{-x}, & 1 \leq x, \\
  0, & \text{otherwise,}
\end{cases}
$$
where $c$ is the normalising constant and $\alpha \in (0, 1)$.
First we determine the normalising constant by integration,

$$
1
= \int_\mathbb{R} g(x) \text{d}x
= \int_0^1 cx^{\alpha - 1} \text{d}x + \int_1^\infty ce^{-x} \text{d}x
= c \frac{\alpha + e}{\alpha e} \\
\implies c = \frac{\alpha e}{\alpha + e}
$$
Inserting $c$ into the density function yields

$$
g(x) =
\begin{cases}
  \frac{\alpha e}{\alpha + e} x^{\alpha - 1}, & 0 < x < 1, \\
  \frac{\alpha e}{\alpha + e} e^{-x}, & 1 \leq x, \\
  0, & \text{otherwise.}
\end{cases}
$$

#### (a) Cumulative distribution

The cumulative distribution can now be found

$$
G(x)
= \int_0^x g(y) \text{d}y
= \begin{cases}
  0, & x \leq 0 \\
  \frac{e}{\alpha + e} x^{\alpha}, & 0 < x < 1, \\
  1 - \frac{\alpha e}{\alpha + e} e^{-x}, & 1 \leq x.
\end{cases}
$$
Now we find the inverse of the cumulative distribution function. First for the case when $G(x) < \frac{e}{\alpha + e}$

$$
\begin{align}
  G(x) &= \frac{e}{\alpha + e} x^{\alpha} \\
  \implies x &= \sqrt[\alpha]{\frac{\alpha + e}{e} G(x)}
\end{align}
$$
And for $G(x) > \frac{e}{\alpha + e}$ we have

$$
\begin{align}
  G(x) &= 1 - \frac{\alpha e}{\alpha + e}e^{-x} \\
  \implies x &= - \ln\left(\frac{\alpha + e}{\alpha e}(1 - G(x))\right)
\end{align}
$$
The inverse cumulative function thus becomes

$$
G^{-1}(x) =
\begin{cases}
  \sqrt[\alpha]{\frac{\alpha + e}{e} x}, &0 \leq x < \frac{e}{\alpha + e}, \\
  - \ln\left(\frac{\alpha + e}{\alpha e}(1 - x)\right), &\frac{e}{\alpha + e} \leq x \leq 1.
\end{cases}
$$
The expectation is given by $E(X) = \int_{-\infty}^{\infty}xg(x)dx = c\int_0^1x^{\alpha}dx + c\int_1^{\infty}xe^{-x} = c\left(\frac{1}{\alpha + 1} + \frac{2}{e}\right)$. 

The variance is given $Var(X) = E(X^2) - E(X)^2$, where $E(X^2) = \int_{-\infty}^{\infty}x^2g(x)dx = c\int_0^1x^{\alpha+1}dx + c\int_1^{\infty}x^2e^{-x} = c\left(\frac{1}{\alpha + 2} + \frac{5}{e}\right)$. 

The expression for the variance becomes $Var(X) = c\left(\frac{1}{\alpha + 2} + \frac{5}{e}\right) - c^2\left(\frac{1}{\alpha + 1} + \frac{2}{e}\right)^2$

#### (b) Sampling from $g(x)$

We now want to generate random samples from $g(x)$. Since we know the inverse of the cumulative distribution, we can use the inverse transform technique.

```{r}
rg <- function(n, alpha = 1) {
  u <- runif(n = n)
  boundary <- exp(1) / (alpha + exp(1))
  left <- u < boundary
  right <- !left
  u[left] <- (u[left] / boundary) ** (1 / alpha)
  u[right] <- -log((1 - u[right]) / (boundary * alpha))
  return(enframe(u))
}
```

We also implement the density function for the purpose of comparison

```{r}
dg <- function(x, alpha = 1) {
  normalizing_constant <- alpha * exp(1) / (alpha + exp(1))
  d <- rep(0, length(x))
  left_indices <- 0 < x & x < 1
  right_indices <- 1 <= x
  d[left_indices] <- normalizing_constant * (x[left_indices] ** (alpha - 1))
  d[right_indices] <- normalizing_constant * exp(-x[right_indices])
  return(d)
}
```

We now compare one million random samples generated with this sampling technique and compare
it with the theoretical density

```{r}
samples <- 1000000
alpha <- 0.7
g_samples <- rg(n = samples, alpha = alpha)
ggplot() +
  geom_histogram(
    data = g_samples,
    mapping = aes(x=value, y=..density..),
    binwidth = 0.01,
    boundary = 0
  ) + stat_function(
    fun = dg,
    args = list(alpha = alpha),
    aes(col = 'Theoretical density.')
  ) +
  geom_vline(
    aes(
      xintercept = mean(g_samples$value),
      col = 'Empirical mean'
    )
  ) +
  ylim(0, 1) +
  xlim(0, 5)
```
```{r}
c <- alpha*exp(1)/(alpha + exp(1))
mean <- c*(1/(alpha + 1) + 2/exp(1))
second_moment <- c*(1/(alpha+2) + 5/exp(1))
variance <- second_moment - mean^2

results <- list(
  mean=mean,
  sample_mean = mean(g_samples %>% use_series(value)),
  variance = variance,
  sample_variance = var(g_samples %>% use_series(value)))

print(results)
```



### 3. Box-Muller algorithm for standard normal distribution

```{r}

box_muller <- function(n){
  x1 <- runif((n+1)/2)*2*pi
  x2 <- rexp((n+1)/2, rate=1/2) %>% use_series(value)
  y1 <- map2_dbl(x2, x1, ~sqrt(.x)*cos(.y))
  y2 <- map2_dbl(x2, x1, ~sqrt(.x)*sin(.y))
  return(c(y1, y2)[1:n])
}

dnorm_std <- partial(dnorm, mean = 0, sd = 1)
n <- 1000000
x <- box_muller(n)

sample <- tibble(value = x)
results <- list(mean = 0,
                sample_mean = mean(x), 
                variance = 1,
                sample_variance = var(x))

box_muller_plot <- sample %>%
  ggplot() + 
  geom_histogram(aes(x=value, y=..density..), binwidth=0.05) + 
  stat_function(fun=dnorm_std, color="red", size=1)

print(box_muller_plot)
print(results)
```



### 4. Arbitrary normal distribution

```{r}
multivariate_normal <- function(mean, covariance, n){
  d <- length(mean)
  A <- chol(covariance)
  
  xs <- rerun(n, box_muller(d))
  y <- map(xs, ~A%*%.x + mean)
  
  return(y)
}
```

The implementation is tested with $d=3$, $\mu = \left[1, 4, 2\right]^T$ and $\Sigma = 3\mathbb{I}_3$. The empirical mean and covariance matrix is shown to agree with the specified theoretical mean and covariance matrix. 


```{r}
mean <- c(1,4,2)
covariance <- diag(3)*3
n <- 10000

sample <- multivariate_normal(mean, covariance, n)

sample_tbl <- sample %>%
  map(t) %>%
  map(as_tibble) %>%
  bind_rows()

sample_mean <- sample_tbl %>% 
  colMeans()

sample_covariance <- sample_tbl %>% 
  cov()

results <- list(mean=mean,
                sample_mean=as.numeric(sample_mean),
                covariance=covariance,
                sample_covariance=as.matrix(sample_covariance))
colnames(results$sample_covariance) <- NULL
rownames(results$sample_covariance) <- NULL
print(results)
```


## Problem A: Stochastic simulation by the probability integral transform and bivariate techniques


### 1. Sampling from the exponential distribution

We want to generate $n$ samples the exponential distribution with rate parameter $\lambda$.

```{r}
library(tidyverse)
```

```{r}
rexp <- function(n, rate = 1) {
  uniformly_distributed <- runif(n=n)
  exponentially_distributed <- -log(uniformly_distributed) / rate
  return(enframe(exponentially_distributed))
}
```

```{r}
samples <- 10000
rate <- 1
exponential_samples <- rexp(n = samples, rate = rate)
ggplot() + 
  geom_histogram(
    data = exponential_samples,
    mapping = aes(x=value, y=..density..),
    binwidth = 0.1,
    boundary = 0
  ) +
  geom_vline(
    aes(xintercept = mean(exponential_samples$value))
  ) +
  stat_function(
    fun = dexp,
    args=list(rate = rate),
    aes(col='Theoretical density')
  )
```


### 2. Probability density function

We now consider the probability density function
$$
g(x) =
\begin{cases}
  cx^{\alpha - 1}, & 0 < x < 1, \\
  ce^{-x}, & 1 \leq x, \\
  0, & \text{otherwise,}
\end{cases}
$$
where $c$ is the normalising constant and $\alpha \in (0, 1)$.
First we determine the normalising constant by integration,

$$
1
= \int_\mathbb{R} g(x) \text{d}x
= \int_0^1 cx^{\alpha - 1} \text{d}x + \int_1^\infty ce^{-x} \text{d}x
= c \frac{\alpha + e}{\alpha e} \\
\implies c = \frac{\alpha e}{\alpha + e}
$$
Inserting $c$ into the density function yields

$$
\newcommand{c}[]{\frac{\alpha e}{\alpha + e}}
g(x) =
\begin{cases}
  \c x^{\alpha - 1}, & 0 < x < 1, \\
  \c e^{-x}, & 1 \leq x, \\
  0, & \text{otherwise.}
\end{cases}
$$

#### (a) Cumulative distribution

The cummulative distribution can now be found

$$
G(x)
= \int_0^x g(y) \text{d}y
= \begin{cases}
  0, & x \leq 0 \\
  \frac{e}{\alpha + e} x^{\alpha}, & 0 < x < 1, \\
  1 - \c e^{-x}, & 1 \leq x.
\end{cases}
$$
Now we find the inverse of the cumulative distribution function. First for the case when $G(x) < \frac{e}{\alpha + e}$

$$
\begin{align}
  G(x) &= \frac{e}{\alpha + e} x^{\alpha} \\
  \implies x &= \sqrt[\alpha]{\frac{\alpha + e}{e} G(x)}
\end{align}
$$
And for $G(x) > \frac{e}{\alpha + e}$ we have

$$
\begin{align}
  G(x) &= 1 - \frac{\alpha e}{\alpha + e}e^{-x} \\
  \implies x &= - \ln\left(\frac{\alpha + e}{\alpha e}(1 - G(x))\right)
\end{align}
$$
The inverse cumulative function thus becomes

$$
G^{-1}(x) =
\begin{cases}
  \sqrt[\alpha]{\frac{\alpha + e}{e} x}, &0 \leq x < \frac{e}{\alpha + e}, \\
  - \ln\left(\frac{\alpha + e}{\alpha e}(1 - x)\right), &\frac{e}{\alpha + e} \leq x \leq 1.
\end{cases}
$$

#### (b) Sampling from $g(x)$

We now want to generate random samples from $g(x)$. Since we know the inverse of the cumulative distribution, we can use the inverse transform technique.

```{r}
rg <- function(n, rate = 1) {
  u <- runif(n = n)
  boundary <- exp(1) / (rate + exp(1))
  left <- u < boundary
  right <- !left
  u[left] <- (u[left] / boundary) ** (1 / rate)
  u[right] <- -log((1 - u[right]) / (boundary * rate))
  return(enframe(u))
}
```

We also implement the density function for the purpose of comparison

```{r}
dg <- function(x, rate = 1) {
  normalizing_constant <- rate * exp(1) / (rate + exp(1))
  d <- rep(0, length(x))
  left_indices <- 0 < x & x < 1
  right_indices <- 1 <= x
  d[left_indices] <- normalizing_constant * (x[left_indices] ** (rate - 1))
  d[right_indices] <- normalizing_constant * exp(-x[right_indices])
  return(d)
}
```

We now compare one million random samples generated with this sampling technique and compare
it with the theoretical density

```{r}
samples <- 1000000
rate <- 0.7
g_samples <- rg(n = samples, rate = rate)
ggplot() +
  geom_histogram(
    data = g_samples,
    mapping = aes(x=value, y=..density..),
    binwidth = 0.01,
    boundary = 0
  ) + stat_function(
    fun = dg,
    args = list(rate = rate),
    aes(col = 'Theoretical density.')
  ) +
  geom_vline(
    aes(
      xintercept = mean(g_samples$value),
      col = 'Empirical mean'
    )
  ) +
  ylim(0, 1) +
  xlim(0, 5)
```

### 3. Box-Muller algorithm for standard normal distribution

(Answer)


### 4. Arbitrary normal distribution

(Answer)

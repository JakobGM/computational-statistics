---
output:
  pdf_document:
    fig_caption: yes
  html_document:
    fig_caption: yes
---
# Exercise 2 - Implementation of the MCMC sampler

\providecommand{\kappau}{\kappa_u}
\providecommand{\kappav}{\kappa_v}
\providecommand{\uvec}{\boldsymbol{u}}
\providecommand{\Rmat}{\boldsymbol{R}}
\providecommand{\yvec}{\boldsymbol{y}}
\providecommand{\Evec}{\boldsymbol{E}}
\providecommand{\etavec}{\boldsymbol{\eta}}

## Importing the data set

We will now implement an MCMC sampler for the parameters discussed in exercise 1.
First, let's import the libraries which will be used in the implementation.

```{r}
# Lots of helper functions and new data types
library(tidyverse)

# For sparse matrix support
library(spam)

# Spatial data library
library(fields, warn.conflict=FALSE)

# Custom color scheme
library(colorspace)
```

The data of interest is contained in the `Oral` dataset.

```{r}
attach(Oral)
```

The neighbourhood structure matrix, $\boldsymbol{R}$, is provided in the data file `tma4300_ex2_Rmatrix.Rdata`.

```{r}
load("data/tma4300_ex2_Rmatrix.Rdata")
```

We will now define a list named \texttt{problem}, which will contain the global state for our problem.
It will contain constants for our problem, such as $\yvec$, $\boldsymbol{E}$, $n$, $\boldsymbol{R}$, $\alpha_u$, $\alpha_v$, $\beta_u$, and $\beta_v$.
These variables will remain constant through all the iterations of the MCMC algorithm, and will be passed as the `problem` parameter to all functions.

```{r}
problem = list(
  y = Oral$Y,
  E = Oral$E,
  n = length(Oral$Y),
  R = R,
  alpha_u = 1,
  alpha_v = 1,
  beta_u = 0.01,
  beta_v = 0.01
)
```

## Implementing full conditional sampling functions

We start by implementing full conditional samplers for $\boldsymbol{\kappa}^{(m)}_u$, $\boldsymbol{\kappa}^{(m)}_v$, and $\boldsymbol{u}^{(m)}$.

### Sampling $\boldsymbol{\kappa}^{(m)}_u$ and $\boldsymbol{\kappa}^{(m)}_v$

$\kappa_u^{(m)}$ will be drawn from the following full conditional

$$
\kappa_u^{(m)}~|~\boldsymbol{y}, \kappa_v^{(m-1)}, \boldsymbol{\eta}^{(m - 1)}, \boldsymbol{u}^{(m -  1)}
\sim \text{Gamma}\left(\frac{n - 1}{2} + \alpha_u, \frac{1}{2} {\boldsymbol{u}^{(m - 1)}}^{T} \boldsymbol{R} \boldsymbol{u}^{(m - 1)} + \beta_u \right)
$$

Which can be implemented with the \texttt{rgamma} R-function.

```{r}
draw_kappa_u <- function(u, problem) {
  #' Return one random sample from the full conditional for kappa_u
  shape <- (problem$n - 1) / 2 + problem$alpha_u
  rate <- 0.5 * t(u) %*% problem$R %*% u + problem$beta_u
  sample <- rgamma(shape = shape, rate = rate, n = 1)[[1]]
  return(sample)
}
```

Likewise, for $\kappa_v^{(m)}$ we draw from the full conditional

$$
\kappa_v^{(m)}~|~\boldsymbol{y}, \kappa_u^{(m)}, \boldsymbol{\eta}^{(m - 1)}, \boldsymbol{u}^{(m -  1)}
\sim \text{Gamma}\left(\frac{n}{2} + \alpha_v, \frac{1}{2} \left(\boldsymbol{\eta}^{(m - 1)} - \boldsymbol{u}^{(m - 1)} \right)^{T} \left(\boldsymbol{\eta}^{(m - 1)} - \boldsymbol{u}^{(m - 1)} \right) + \beta_v \right),
$$

Which will implemented in the same way

```{r}
draw_kappa_v <- function(eta, u, problem) {
  #' Return one random samplef from the full conditional for kappa_v
  shape <- problem$n / 2 + problem$alpha_v
  rate <- 0.5 * t(eta - u) %*% (eta - u) + problem$beta_v
  sample <- rgamma(shape = shape, rate = rate, n = 1)[[1]]
  return(sample)
}
```

### Full conditional sampler for $\uvec$

$\boldsymbol{u}^{(m)}$ is drawn from the following full conditional distribution

$$
\boldsymbol{u}^{(m)}~|~\boldsymbol{\eta}^{(n - 1)}, \kappa_u^{(m)}, \kappa_v^{(m)}
\sim
\mathcal{N}\left(
  \left(\kappa_v^{(m)} I + \kappa_u^{(m)} \boldsymbol{R}\right)^{-1} \kappa_v^{(m)} \boldsymbol{\eta}^{(m - 1)},~
  \left(\kappa_v^{(m)} I + \kappa_u^{(m)} \boldsymbol{R}\right)^{-1}
\right)
$$
Now, notice that the adjacency matrix $R$ is a sparse matrix. Thus, $\left(\kappa_v^{(n - 1)} I + \kappa_u^{(n - 1)} R\right)$ is also a sparse matrix.
It is therefore preferable to store this matrix as a *sparse* matrix, using the `spam` R-library.
Additionally, we use the `rmvnorm.canonical` function in order to formulate the normal distribution in form of the sparse *precision* matrix instead of the covariance matrix. With other words,

\begin{align*}
Q \leftarrow &\kappa_v^{(m)} I + \kappa_u^{(m)} R \\
b \leftarrow &\kappa_v^{(m)} \boldsymbol{\eta}^{(m - 1)}
\end{align*}

The implementation of the full conditional sampler for $\boldsymbol{u}$ therefore becomes

```{r}
draw_u <- function(kappa_v, kappa_u, eta, problem) {
  #' Return one sample from the full conditional distribution of u
  Q <- diag.spam(x = kappa_v, nrow = problem$n) + kappa_u * problem$R
  b <- kappa_v * eta
  sample <- c(rmvnorm.canonical(n = 1, b = b, Q = Q))
  return(sample)
}
```

### Metropolis-Hastings step for $\etavec$

The sampling for $\boldsymbol{\eta}$ requires a Metropolis-Hastings step, since the distribution is not in a well-known form.

#### Proposal density

We will draw a proposal, $\boldsymbol{\eta}^*$, from the taylor expansion, $q(\boldsymbol{\eta}^* ~|~ ...)$, of $p(\boldsymbol{\eta}^* ~|~ ...)$ around $z = \boldsymbol{\eta}^{(m - 1)}$.

$$
q \left(
  \boldsymbol{\eta^*}
  ~|~
    \boldsymbol{z} = \boldsymbol{\eta}^{(m - 1)},
    \boldsymbol{y},
    \boldsymbol{u}^{(m)},
    \kappa_u^{(m)},
    \kappa_v^{(m)}
\right)
\\
\propto
\exp \left\{
  -\frac{1}{2} {\boldsymbol{\eta}^*}^T
    \left(
      \kappa_v^{(m)} \textbf{I} + \text{diag}\left(c\left(\boldsymbol{\eta}^{(m - 1)}\right)\right) 
    \right)
    \boldsymbol{\eta}^*
  + {\boldsymbol{\eta}^*}^T \left( \kappa_v^{(m)} \boldsymbol{u}^{(m)} + b\left(\boldsymbol{\eta}^{(m - 1)}\right) \right)
\right\},
$$

with $b(\boldsymbol{z})$ and $c(\boldsymbol{z})$ defined as in exercise 1.
Again, we can use the sparse, canonical representation in the implementation

\begin{align*}
  Q \leftarrow &\kappa_v^{(m)} \textbf{I} + \text{diag}(c(\boldsymbol{\eta}^{(m - 1)})) \\
  b \leftarrow &\kappa_v^{(m)} \boldsymbol{u}^{(m)} + b(\boldsymbol{\eta}^{(m - 1)}).
\end{align*}

So the proposal sample drawer is therefore implemented as follows

```{r}
draw_proposal_eta <- function(z, u, kappa_v, problem) {
  b <- problem$y + problem$E * exp(z) * (z - 1)
  c <- problem$E * exp(z)
  
  canonical_b <- kappa_v * u + b
  Q <- diag.spam(x = kappa_v, nrow = problem$n) + diag.spam(c)
  
  sample <- c(rmvnorm.canonical(n = 1, b = canonical_b, Q = Q))
  return(sample)
}
```

#### Acceptance probability

The acceptance probability, $\boldsymbol{\alpha}^{(m)}$, for setting $\boldsymbol{\eta}^{(m)} \leftarrow \boldsymbol{\eta}^*$ is as follows

$$
\boldsymbol{\alpha}^{(m)}
=
\min \left(
  1,
  \frac{
    p \left(
      \boldsymbol{\eta^*}
      ~|~
      \boldsymbol{y},
      \boldsymbol{u}^{(m)},
      \kappa_u^{(m)},
      \kappa_v^{(m)}
      \right)
  }{
    p \left(
      \boldsymbol{\eta^{(m-1)}}
      ~|~
      \boldsymbol{y},
      \boldsymbol{u}^{(m)},
      \kappa_u^{(m)},
      \kappa_v^{(m)}
      \right)
  }
  \cdot
    \frac{
      q \left(
        \boldsymbol{\eta^{(m-1)}}
        ~|~
        \boldsymbol{z} = \boldsymbol{\eta}^*,
        \boldsymbol{y},
        \boldsymbol{u}^{(m)},
        \kappa_u^{(m)},
        \kappa_v^{(m)}
        \right)
    }{
      q \left(
        \boldsymbol{\eta^*}
        ~|~
        \boldsymbol{z} = \boldsymbol{\eta}^{(m - 1)},
        \boldsymbol{y},
        \boldsymbol{u}^{(m)},
        \kappa_u^{(m)},
        \kappa_v^{(m)}
        \right)
    }
\right)
$$

It should be noted that these density calculations, $q(...)$ and $p(...)$, will be performed in log-space going forwards.
First, we need to implement $\log{q(\boldsymbol{\eta}~|~...)}$.

```{r}
source("data/dmvnorm.R")
eta_q_log_density <- function(eta, z, u, kappa_v, problem) {
  #' Return the *proportional* full conditional log density of eta
  #' from the Taylor expanded density function around z
  b <- problem$y + problem$E * exp(z) * (z - 1)
  c <- problem$E * exp(z)
  
  canonical_b <- kappa_v * u + b
  Q <- diag.spam(x = kappa_v, nrow = problem$n) + diag.spam(c)
  
  logprob <- dmvnorm.canonical(x = eta, b = canonical_b, Q = Q, log = TRUE)[[1]]
  return(logprob)
}
```

Here we have calculated the $\log q(\boldsymbol{\eta^*}~|~...)$ using `dmvnorm.canonical` as implemented in the provided file `dmvnorm.R`.

Now, we must implement $\log{p(\boldsymbol{\eta}~|~...)}$.
Notice that all the conditionals are equal in both $p$ function invocations, so we need not normalize the implementation of $\log{p(...)}$ as the normality constants cancel out. The implementation is therefore as follows

```{r}
eta_p_log_density <- function(eta, kappa_v, u, problem) {
  #' Return the proportional full conditional log density for eta
  return(
    -0.5 * t(eta) %*% diag.spam(x = kappa_v, nrow = problem$n) %*% eta
    + t(eta) %*% (kappa_v * u)
    + t(eta) %*% problem$y
    - t(exp(eta)) %*% problem$E
  )
}
```

We can now implement the calculation of $\boldsymbol{\alpha}^{(m)}$.

```{r}
acceptance_probability <- function(proposal_eta, previous_eta, kappa_v, u, problem) {
  #' Return the acceptance probability for going from `previous_eta` to `proposal_eta`
  log_p_forward <- eta_p_log_density(
          eta = proposal_eta,
          kappa_v = kappa_v,
          u = u,
          problem = problem
  )
  log_p_backward <- eta_p_log_density(
          eta = previous_eta,
          kappa_v = kappa_v,
          u = u,
          problem = problem
  )
  
  log_q_forward <- eta_q_log_density(
    eta = proposal_eta,
    z = previous_eta,
    u = u,
    kappa_v = kappa_v,
    problem = problem
  )
  log_q_backward <- eta_q_log_density(
    eta = previous_eta,
    z = proposal_eta,
    u = u,
    kappa_v = kappa_v,
    problem = problem
  )
  
  alpha <- exp(log_p_forward + log_q_backward - log_p_backward - log_q_forward)
  
  if (alpha > 1) {
    return(1)
  }
  return(alpha)
}
```

If $u < \boldsymbol{\alpha}^{(m)}$, where $u \sim \mathcal{U}(0, 1)$, we set $\boldsymbol{\eta}^{(m)} \leftarrow \boldsymbol{\eta}^{(m - 1)}$.

### Implementing the MCMC algorithm

Now we have all the required components required to implement the outer loop of the MCMC algorithm.
The algorithm requires initial values for $\boldsymbol{u}^{(1)}$ and $\boldsymbol{\eta}^{(1)}$.
We will set (rather arbitrarily) $\boldsymbol{u}^{(1)} = \vec{0}$, while setting the value of $\boldsymbol{\eta}^{(1)}$ by sampling our proposal density $q(...)$ with the following parameters

\begin{align*}
  \boldsymbol{z} \leftarrow& \vec{0} \\
  \boldsymbol{u} \leftarrow& \vec{0} \\
  \kappa_v \leftarrow& 180
\end{align*}

```{r, cache = TRUE}
MCMC <- function(steps, problem) {
  #' Run MCMC algorithm with number of steps = `steps`
  #' Returns a list of sampled values for: kappa_u, kappa_v, u, eta
  #' Additionally, the acceptance probabilities and time used is returned
  
  # Initial guess for parameters u and eta
  u <- c(rep_len(0.0, problem$n))
  eta <- draw_proposal_eta(z = u, u = u, kappa_v = 180, problem = problem)
  
  # Data structures for saving sample results
  kappa_us <- vector()
  kappa_vs <- vector()
  etas <- matrix(data = NA, nrow = steps, ncol = problem$n)
  us <- matrix(data = NA, nrow = steps, ncol = problem$n)
  alphas <- vector()
  
  # Save start time, used for tracking time usage of the MCMC algorithm
  start_time <- Sys.time()
  
  for (i in seq(1, steps)) {
    kappa_u <- draw_kappa_u(u = u, problem = problem)
    kappa_v <- draw_kappa_v(eta = eta, u = u, problem = problem)
    u <- draw_u(kappa_v = kappa_v, kappa_u = kappa_u, eta = eta, problem = problem)
    
    proposal_eta <- draw_proposal_eta(
      z = eta,
      u = u,
      kappa_v = kappa_v,
      problem = problem
    )
    alpha <- acceptance_probability(
      proposal_eta = proposal_eta,
      previous_eta = eta,
      kappa_v = kappa_v,
      u = u,
      problem = problem
    )
    if (runif(1)[1] < alpha) {
      eta = proposal_eta
    }
    
    # Appending results
    kappa_us <- c(kappa_us, kappa_u)
    kappa_vs <- c(kappa_vs, kappa_v)
    alphas <- c(alphas, alpha)
    us[i,] = u
    etas[i,] = eta
  }
  
  # Calculate the time used by the MCMC algorithm
  end_time <- Sys.time()
  time_used <- end_time - start_time
  time_used_per_step <- time_used / steps
  
  # Returning the final result in the form of a list
  result <- list(
    kappa_u = kappa_us,
    kappa_v = kappa_vs,
    u = us,
    eta = etas,
    alpha = alphas,
    time_used = time_used,
    time_used_per_step = time_used_per_step
  )
  return(result)
}
```


### Running the MCMC sampler

A posterior sample size of $M = 70~000$, after having a burn-in period of $1000$ steps, is sampled.
We will see shortly that this burn-in period is more than sufficient.

```{r, eval = FALSE}
set.seed(0)
M <- 70000
burnin <- 1000
result <- MCMC(steps = M + burnin, problem = problem)
```

```{r, include = FALSE, eval = FALSE}
# Run this line if it is the first time you have executed MCMC and want to save the result
save(result, list = c("result"), file = "data/MCMC_result.Rdata")
```

```{r, include = FALSE}
# Load the saved result for `result`
M <- 70000
burnin <- 1000
load(file = "data/MCMC_result.Rdata")
```

For reproducability, a specific run is cached and available for download at [http://folk.ntnu.no/jakobgm/MCMC_result.Rdata](http://folk.ntnu.no/jakobgm/MCMC_result.Rdata).
This is the data that will be analyzed in the upcoming sections.

#### Time usage

The time used by the algorithm is part of the returned result.

```{r}
result$time_used
```

$71~000$ iterations of the algorithm uses approximately 9 minutes and 20 seconds, which results in the following time usage per iteration

```{r}
milliseconds <- 1000 * 60 * as.numeric(result$time_used_per_step)
sprintf("~%1.1fms per iteration", milliseconds)
```

I.e., $\propto 7.9$ milliseconds per iteration, each iteration creating $2n + 3 = 1091$ double float data points.

#### Acceptance rates

The post-burn-in acceptance rates, $\boldsymbol{\alpha}$, for the Metropolis-Hastings step which samples $\boldsymbol{\eta}^{(m)}$ is also retrievable from the result.

```{r}
alpha <- result$alpha[-c(1:burnin)]
mean(alpha)
```

The acceptance probability is, on average, approximately $74\%$.
A more interesting overview of the distribution of $\alpha^{(m)}$ can be observed by plotting a histogram.

```{r, fig.cap = "\\label{fig:alphas} The distribution of alpha values for all the post-burn-in acceptance probabilities. Vertical red line indicates the mean acceptance probability."}
binwidth <- 0.04
enframe(alpha) %>%
  ggplot(aes(x = value)) +
  geom_histogram(
    aes(y = binwidth * ..density..),
    boundary = 0,
    breaks = seq(0.0, 1.0, by = binwidth)
  ) +
  geom_vline(aes(xintercept = mean(alpha), color="alpha mean")) +
  ylab("frequency") +
  xlab(expression(alpha)) +
  labs(title = "Histogram of calculated acceptance probabilities")
```

As can be seen the histogram in figure \ref{fig:alphas}, most of the acceptance probabilities are very close to $1$.
The remaining acceptance probabilities gradually decline in density towards $0$.
This is considered to be a relatively good result, since a sufficient amount of proposal samples are expected to be accepted.

# Exercise 3 - Convergence diagnostics

We will now make an attempt at diagnosing the convergence of the samples generated from the Monte Carlo Markov Chain algorithm.
First, since $\boldsymbol{\eta}$ is decomposed as $\boldsymbol{u} + \boldsymbol{u}$, we can retrieve samples of $\boldsymbol{v} = \boldsymbol{\eta} - \boldsymbol{u}$.

```{r}
vs <- result$eta - result$u
```

We will investigate the precision parameters $\kappa_u$ and $\kappa_v$ in addition to three randomly chosen components of $\boldsymbol{u}$ and $\boldsymbol{v}$. The three components $\{80, 360, 500\}$ will be used for both $\boldsymbol{u}$ and $\boldsymbol{v}$. This data subset is inserted into a data frame.

```{r}
samples <- tibble(
  step = seq(1, burnin + M),
  kappa_u = result$kappa_u, kappa_v = result$kappa_v,
  u_80 = result$u[, 80], u_360 = result$u[, 360], u_500 = result$u[, 500],
  v_80 = vs[, 80], v_360 = vs[, 360], v_500 = vs[, 500]
)
params <- colnames(samples)[-1]
```

### 3a) Trace plots

#### After burn-in period

First, we will plot the trace plots for all the chosen sample parameters, with the burn-in period removed.

```{r, fig.width = 8.3, fig.height = 11.7, fig.fullwidth=TRUE, fig.cap = "\\label{fig:trace_plots} Sample line plot for each step of the MCMC algorithm (post-burn-in)"}
burnins <- seq(1, burnin)
samples %>%
  slice(-burnins) %>%
  gather(key, value, params) %>%
  ggplot(aes(x = step)) +
  geom_line(aes(y = value)) +
  facet_wrap(vars(key), scales = "free_y", ncol = 2) +
  labs(title = "Parameter trace plots")
```

By observing figure \ref{fig:trace_plots}, we can observe that all 8 parameters portray a "band-like" shape, which is a good indication for convergence.
Additionally, there is no indication of the parameters becoming "stuck" at any point during the iterations, and the chains mixes properly.
The samples of the three $\boldsymbol{v}$-components seem to be symmetrically distributed around $0$, which is what we would expect to see from these unstructured white-noise parameters.

#### Posterior means

The post-burn-in posterior means can be calculated

```{r}
library(kableExtra)
samples[,-1] %>%
  slice(-burnins) %>%
  colMeans() %>%
  kable(
    col.names = c("Posterior mean"),
    caption = "\\label{tab:posterior_means}Posterior means for the eight parameters of interest."
  )
```

These values can be compared to the burn-in trace plots in the upcoming section.

#### burn-in period

Let's try to justify our earlier assumption of a burn-in period of $1~000$ steps being more than sufficient.
We will plot trace plots for the first $2~000$ steps.

```{r, fig.width = 8.3, fig.height = 8, fig.fullwidth=TRUE, fig.cap = "\\label{fig:burnin} Sample line plot for beginning steps of the MCMC algorithm (burn-in). The red line indicates the chosen burn-in boundary."}
beginning <- seq(1, 2 * burnin)
samples %>%
  slice(beginning) %>%
  gather(key, value, params) %>%
  ggplot(aes(x = step)) +
  geom_line(aes(y = value)) +
  geom_vline(xintercept = burnin, color = "red") +
  facet_wrap(vars(key), scales = "free_y", ncol = 2) +
  labs(title = "Parameter burn-in trace plots")
```

\newpage
From figure \ref{fig:burnin} it becomes clear that all the 8 chains have converged long before a thousand steps (red vertical line) have been calculated.
$\kappa_u$ is the most extreme example beginning at $\sim 25~000$, and as can be seen in table \ref{tab:posterior_means}, $\kappa_u$ has a posterior mean of $\sim 17$, but it quickly converges to the likely correct domain.

This "zoomed in" plot also allows us to inspect the degree of movement of each chain to a better degree.
For instance, the $\kappa_v$ sample plot is the most "sticky" of them all, and will probably show a greater degree of auto-correlation compared to the other parameters.
The three $\boldsymbol{v}$ components show the greatest degree of variability in movement, and we would expect good results for these components in the upcoming ACF plots.

### 3b) Autocorrelation plots

Now we plot the autocorrelation of each of the 8 parameters of interest.

```{r acf, fig.width = 8.3, fig.height = 10, fig.fullwidth=TRUE, fig.cap = "\\label{fig:acf} Autocorrelation plots for all 8 parameters of interest."}
par(mfrow=c(4,2))
usable_samples <- samples[-burnins, -1]
for (param in params) {
  acf(usable_samples[param])
}
```

\newpage
As we can see in figure \ref{fig:acf}, $\kappa_v$ and $\kappa_u$ show the greatest degree of autocorrelation, as predicted from the trace plots in figure \ref{fig:burnin} earlier. We can investigate this in more detail by plotting the ACF for $\kappa_v$ with a larger displayed lag.

```{r acf_kappa_u, fig.cap = "\\label{fig:acf_kappa_u} Autocorrelation function plot for kappa_u with max lag set to 200."}
acf(usable_samples["kappa_u"], lag.max = 150)
```

As can be seen in figure \ref{fig:acf_kappa_u}, a lag of $\sim 150$ is required before two samples can be considered independent of each other. This autocorrelation must be remedied by generating a sufficient amount of samples, as it reduces our effective number of samples.

The three $\boldsymbol{v}$ components in figure \ref{fig:acf}, i.e. unstructured random noise, have the lowest degree of autocorrelation, also as predicted from the previous plots.

 
### 3c) Convergence check with `geweke.diag()`

We will test the convergence of the Markov chains by using the function \texttt{geweke.diag()} from the \texttt{coda} R-package.
Each chain is tested independently in the following way

\begin{itemize}
  \item Remove the burn-in samples.
  \item Extract the first $10\%$ samples and $50\%$ last samples from the chain.
  \item Calculate the difference of the two sample means, and divide by its estimated standard error.
  \item Under the assumption that \textit{all} the samples are drawn from the stationary distribution of the chain, and the two chains being assymptotically independent, the result should be standard normal.
\end{itemize}

It are these $Z$-statistics which can be calculated for our parameters of interest with \texttt{geweke.diag()}. 

```{r}
library(coda)
z_statistics <- geweke.diag(
  usable_samples,
  frac1=0.1, frac2=0.5
)$z
```

We can now perform the following hypothesis test

\begin{gather*}
  \mathrm{H_0}: \text{The Markov chain has converged} \\
  \text{ vs. } \\
  \mathrm{H_1}: \text{The Markov chain has } \textit{not} \text{ converged}.
\end{gather*}

This is a two-tailed hypothesis test, for which we can calculate a corresponding $p$-value for

$$
p\text{-value} = 2 \cdot \text{P}(Z \geq |z|)
$$

This is what we will now calculate. Now, remember that a discarded null hypothesis in this case implies *non-convergence*, not the other way around.

```{r}
p_values <- 2 * pnorm(abs(z_statistics), lower.tail = FALSE)
geweke_results <- tibble(
  parameter = names(z_statistics),
  z_statistic = z_statistics,
  p_value = p_values
)
geweke_results %>%
  kable(caption = "\\label{tab:geweke} The Z-statistic and corresponding p-values for the Geweke test on the eight parameters of interest.")
```

The results in table \ref{tab:geweke} are really promisising, as not a single null hypothesis needs to be discarded even at a $10\%$ confidence level.
In the case of convergence we would expect the $p$-values to be uniformly distributed between zero and one, i.e. $p \sim \mathcal{U}(0, 1)$. We can check if this is really the case for all $\boldsymbol{u}$ and $\boldsymbol{v}$ components by plotting a $p$-value histogram for both decompositions.

```{r p_values_plot, fig.fullwidth = TRUE, fig.cap = "\\label{fig:p_values_plot} Histograms for the p-values of the Geweke tests for all components of u and v."}
par(mfcol = c(1, 2))
u_zstats <- geweke.diag(result$u[-burnins,])$z
u_p_values <- 2 * pnorm(abs(u_zstats), lower.tail = FALSE)
u_histogram <- hist(u_p_values, xlab = "p-value", main = "Components of u")

v_zstats <- geweke.diag(vs[-burnins,])$z
v_p_values <- 2 * pnorm(abs(v_zstats), lower.tail = FALSE)
v_histogram <- hist(v_p_values, xlab = "p-value", main = "Components of v")
```

As can be seen in figure \ref{fig:p_values_plot}, nothing points to the contrary of the $p$-values being realizations of a uniform distribution. There is therefore no reason to conclude that we do not have convergence.

#### Conclusion

None of the three convergence diagnostic methods definitely refute the notion of chain convergence, which is good.
It is never possible to conclude with \textit{certainty} that an MCMC algorithm has converged, but the presented diagnostics strengthens our belief in that the chains have converged and that the post-burn-in samples are distributed according to the target distributions.


# Exercise 4 - Effective sample size

## ESS

### Theory

We now want to investigate the effective sample size (ESS) for our $\kappa_u$ and $\kappa_v$ parameter samples.

ESS is an estimation for how many perfectly independent samples that would be required in order to obtain a parameter estimate that has the \textit{same} precision as an estimate generated from our MCMC sampler with $M$ samples. Therefore, $\text{ESS} \leq M$ in all cases, and the closer they are, the better.

In order to calculate this, first define the \textit{autocorrelation time}, $\tau$, as

$$
\tau := 1 + 2 \cdot \sum_{k = 1}^{\infty} \rho(k),
$$
where $\rho(k)$ is the autocorrelation at lag $k$, earlier shown in the ACF plots in exercise 3.
Now, ESS can be calculated as

$$
\text{ESS} = \frac{N}{\tau}
$$

### Calculation and interpretation

The \texttt{effectiveSize()} function from the \texttt{coda} R-library can be used to calculate the ESS for our MCMC samples.

```{r}
ESS <- effectiveSize(usable_samples)
ESS %>%
  kable(
    caption = "\\label{tab:ESS}Effective number of samples for our 8 paramaters of interest."
  )
```

What does table \ref{tab:ESS} imply?
It is estimated that independent samplers for $\kappa_u$ and $\kappa_v$ would only require $1156$ and $626$ samples, respectively, in order to obtain parameter estimates with the *same* precision as if we used our MCMC samples of size $70~000$. This is less than $2\%$ of the MCMC sample size, $M$.

With other words, an independent sampler that runs 50 times as slow as our MCMC implementation would therefore be of approximately equal benefit as our MCMC implementation, not considering memory usage and burn-in periods.

### Possible improvements for the ESS values

Large values for ESS is to be preferred, and there are several strategies that can be devised in order to improve the value for ESS, for instance

\begin{itemize}
  \item Combine the results of \textit{multiple} independent chains. Compare the results in order to check convergence and if they mix properly.
  \item Perform more steps of the MCMC algorithm. This would of course increase the absolute value of the ESS, but might not increase its \textit{relative} value.
  \item Try to integrate out certain variables, for instance $\kappa_v$ and $\kappa_u$, but this might not be analytically feasible. 
  \item Finding a better proposal density for $\boldsymbol{\eta}$. This will increase the acceptance probability, and therefore decrease the autocorrelation of $\boldsymbol{\eta}$ and all its dependents.
  \item Reparametrizing the model.
\end{itemize}

## Relative ESS

### Calculation

We can also calculate the \textit{relative} ESS by dividing the mean ESS by the time used by the MCMC algorithm.

```{r}
seconds_used <- 60 * as.numeric(result$time_used)
relative_ESS <- mean(ESS) / seconds_used
sprintf("%1.3f effective samples per second", relative_ESS)
```

Our implementation generates $\sim 27$ effective samples per second.

### Interpretation

In isolation, this might not be very interesting, but it is very useful for comparison purposes.
Making a change to an MCMC implementation in order to improve its resulting ESS might not be worth it if it substantially increases the time used by the algorithm. Solely comparing values for ESS not give the entire picture of the situation. For instance, "blocking" correlated variables updates together might be too slow compared to a more naive joint update, and the change may not be worth it in terms of computational time cost. Comparison of relative ESS can in such cases be used as a deciding factor.


# Exercise 5 - Interpretation of results

Now we interpret our results, specifically the spatially structured components of $\boldsymbol{\eta}$, i.e. $\boldsymbol{u}$.
We will use the median of the post-burn-in posterior samples for $\boldsymbol{u}$, $\hat{u}_i = \mathrm{median}(u_i)$, as our estimator.

In order to interpret the following results, observe that

$$
\lambda_i = E_i \exp(\eta_i) = E_i \exp(u_i + v_i) \propto \exp(u_i),
$$

with other words, $u_i$ has a multiplicative effect on the expected numbers of oral cavity cancer occurrences in each German district during the 5-year period. Great values for $\hat{u}_i$ indicate some underlying regional effect in the spatial area which may contribute increased cancer rates. If this is of environmental-, socio-economic, or some other nature is unknown to us, as there are too many correlational effects between place of residence and other factors. Such correlations could be quality of life, health service quality, income, age structure of the population, and so on.

The standardised mortality rates (SMR) $y_i / E_i$ can be visualized on top of the map of Germany's districts and compared to the spatially structured effects, $\hat{u}_i$, side-by-side.

```{r spatial_effect_plot, fig.fullwidth = TRUE, fig.cap = "\\label{fig:germany} On the left hand side, the standardized mortality rates are plotted. While on the right hand side, a plot of the spatially structured effects on oral cancer rates in Germany."}
# Calculate the posterior median of exp(u)
exp_u_median <- result$u %>%
  as_tibble() %>%
  slice(-burnins) %>%
  exp() %>%
  summarize_all(median) %>%
  as_vector()

# Custom color scheme for the Germany plot
col <- diverge_hcl(8)

# Put the next to figures side-by-side
par(mfcol=c(1, 2))

germany.plot(
  Oral$Y / Oral$E,
  col=col,
  legend=TRUE,
  main = bquote("Standardized mortality rates: "~y_i / E[i]),
  cex.mai = 0.9
)
germany.plot(
  exp_u_median,
  col = col,
  legend = TRUE,
  main = bquote("Structured spatial effects: "~e^u[i]),
  cex.main = 0.9
)
```

From figure \ref{fig:germany}, spatial patterns immediately become more obvious in the spatial effect plot on the right hand side, compared to the standardized mortality rates plot on the left hand side.
The German districts close to the German-French border (Baden-Württemberg, The Black Forest, and Rhineland-Palatinate/Saarland) seem to be especially adversely affected.

On the other hand, the German state of Saxony, along the border to the Czech Republic is \textit{not} as adversely affected by the spatial effect. The same can be said for almost the entirety of the old German Democratic Republic, with exception of the area east of Rostock towards the Baltic Sea.

It is important to notice that this oral cavity cancer data set is from the 5-year period of 1986-1990, coincidentally the years leading up to, and including, the reunification of East- and West-Germany. There were, and still are, huge socio-economic, cultural, and demographic differences between these two regions, which can go a long way towards explaining why we see this pattern in the underlying spatial structure.

Why the French-German border region and the Baltic coastal area stands out as \textit{especially} adversely affected by oral cancer, we do not know.

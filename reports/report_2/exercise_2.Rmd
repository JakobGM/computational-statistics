---
output:
  pdf_document: default
  html_document: default
---
# Exercise 2 - Implementation of the MCMC sampler

## Importing the data set

We will now implement an MCMC sampler for the parameters discussed in exercise 1.
First, let's import the libraries which will be used in the implementation.

```{r}
# Lots of helper functions and new data types
library(tidyverse)

# For sparse matrix support
library(spam)

# Spatial data library
library(fields, warn.conflict=FALSE)

# Custom color scheme
library(colorspace)
col <- diverge_hcl(8)
```

The data of interest is contained in the `Oral` dataset.
The standardised mortality rates (SMR) $y_i / E_i$ can be visualized on top of the map of germany's districts.

```{r}
attach(Oral)
germany.plot(Oral$Y / Oral$E, col=col, legend=TRUE)
```

The neighbourhood structure matrix, $\Rmat$, is provided in the data file \texttt{tma4300_ex2_Rmatrix.Rdata}.

```{r}
load("data/tma4300_ex2_Rmatrix.Rdata")
```

We will now define a list named \texttt{problem}, which will contain the global state for our problem.
It will contain constants for our problem, such as $\yvec$, $\Evec$, $n$, $\Rmat$, $\alpha_u$, $\alpha_v$, $\beta_u$, and $\beta_v$.
These variables will remain constant through all the iterations of the MCMC algorithm, and will be passed as the `problem` parameter to all functions.

```{r}
problem = list(
  y = Oral$Y,
  E = Oral$E,
  n = length(Oral$Y),
  R = R,
  alpha_u = 1,
  alpha_v = 1,
  beta_u = 0.01,
  beta_v = 0.01
)
```

## Implementing full conditional sampling functions

We start by implementing full conditional samplers for $\boldsymbol{\kappa}^{(m)}_u$, $\boldsymbol{\kappa}^{(m)}_v$, and $\boldsymbol{u}^{(m)}$.

### Sampling $\boldsymbol{\kappa}^{(m)}_u$ and $\boldsymbol{\kappa}^{(m)}_v$

The full conditional for $\kappa_u$ is

$$
\kappa_u~|~\boldsymbol{y}, \kappa_v^{(m-1)}, \boldsymbol{\eta}^{(m - 1)}, \boldsymbol{u}^{(m -  1)}
\sim \text{Gamma}\left(\frac{n - 1}{2} + \alpha_u, \frac{1}{2} {\boldsymbol{u}^{(m - 1)}}^{T} R \boldsymbol{u}^{(m - 1)} + \beta_u \right)
$$

Which can be implemented with the \texttt{rgamma} R-function.

```{r}
draw_kappa_u <- function(u, problem) {
  shape <- (problem$n - 1) / 2 + problem$alpha_u
  rate <- 0.5 * t(u) %*% problem$R %*% u + problem$beta_u
  sample <- rgamma(shape = shape, rate = rate, n = 1)[[1]]
  return(sample)
}
```

Likewise, for $\kappa_v$ we have the full conditional

$$
\kappa_v~|~\boldsymbol{y}, \kappa_u^{(m)}, \boldsymbol{\eta}^{(m - 1)}, \boldsymbol{u}^{(m -  1)}
\sim \text{Gamma}\left(\frac{n}{2} + \alpha_v, \frac{1}{2} \left(\boldsymbol{\eta}^{(m - 1)} - \boldsymbol{u}^{(m - 1)} \right)^{T} \left(\boldsymbol{\eta}^{(m - 1)} - \boldsymbol{u}^{(m - 1)} \right) + \beta_v \right),
$$

Which will implemented in the same way

```{r}
draw_kappa_v <- function(eta, u, problem) {
  shape <- problem$n / 2 + problem$alpha_v
  rate <- 0.5 * t(eta - u) %*% (eta - u) + problem$beta_v
  sample <- rgamma(shape = shape, rate = rate, n = 1)[[1]]
  return(sample)
}
```

### Full conditional sampler for $\uvec$

The full conditional for $\boldsymbol{u}$ is

$$
\boldsymbol{u}~|~\boldsymbol{\eta}^{(n - 1)}, \kappa_u^{(m)}, \kappa_v^{(m)}
\sim
\mathcal{N}\left(
  \left(\kappa_v^{(m)} I + \kappa_u^{(m)} R\right)^{-1} \kappa_v^{(m)} \boldsymbol{\eta}^{(m - 1)},~
  \left(\kappa_v^{(m)} I + \kappa_u^{(m)} R\right)^{-1}
\right)
$$
Now, notice that the adjacency matrix $R$ is a sparse matrix. Thus, $\left(\kappa_v^{(n - 1)} I + \kappa_u^{(n - 1)} R\right)$ is also a sparse matrix.
It is therefore preferable to store this matrix as a *sparse* matrix, using the `spam` R-library.
Additionally, we use the `rmvnorm.canonical` function in order to formulate the normal distribution in form of the sparse *precision* matrix instead of the covariance matrix. With other words

\begin{align*}
Q \leftarrow &\kappa_v^{(m)} I + \kappa_u^{(m)} R \\
b \leftarrow &\kappa_v^{(m)} \boldsymbol{\eta}^{(m - 1)}
\end{align*}

The implementation of the full conditional sampler for $\boldsymbol{u}$ therefore becomes

```{r}
draw_u <- function(kappa_v, kappa_u, eta, problem) {
  Q <- diag.spam(x = kappa_v, nrow = problem$n) + kappa_u * problem$R
  b <- kappa_v * eta
  sample <- c(rmvnorm.canonical(n = 1, b = b, Q = Q))
  return(sample)
}
```

### Metropolis-Hastings step for $\etavec$

The sampling for $\boldsymbol{\eta}$ requires a Metropolis-Hastings step, since the distribution is not in a well-known form.

We will draw a proposal, $\boldsymbol{\eta}^*$, from the taylor expansion $q(\boldsymbol{\eta}^* ~|~ ...)$ of $p(\boldsymbol{\eta}^* ~|~ ...)$ around $z = \boldsymbol{\eta}^{(m - 1)}$.

$$
q \left(
  \boldsymbol{\eta^*}
  ~|~
    \boldsymbol{z} = \boldsymbol{\eta}^{(m - 1)},
    \boldsymbol{y},
    \boldsymbol{u}^{(m)},
    \kappa_u^{(m)},
    \kappa_v^{(m)}
\right)
\propto
\exp \left\{
  -\frac{1}{2} {\boldsymbol{\eta}^*}^T
    \left(
      \kappa_v^{(m)} \textbf{I} + \text{diag}(c(\boldsymbol{\eta}^{(m - 1)})) 
    \right)
    \boldsymbol{\eta}^*
  + {\boldsymbol{\eta}^*}^T \left( \kappa_v^{(m)} \boldsymbol{u}^{(m)} + b(\boldsymbol{\eta}^{(m - 1)}) \right)
\right\},
$$

with $b(\boldsymbol{z})$ and $c(\boldsymbol{z})$ defined as in exercise 1.
Again, we can use the sparse, canonical representation in the implementation

\begin{align*}
  Q \leftarrow &\kappa_v^{(m)} \textbf{I} + \text{diag}(c(\boldsymbol{\eta}^{(m - 1)})) \\
  b \leftarrow &\kappa_v^{(m)} \boldsymbol{u}^{(m)} + b(\boldsymbol{\eta}^{(m - 1)})
\end{align*}

So the proposal sample drawer is therefore implemented as follows

```{r}
source("data/dmvnorm.R")
draw_proposal_eta <- function(z, u, kappa_v, problem) {
  b <- problem$y + problem$E * exp(z) * (z - 1)
  c <- problem$E * exp(z)
  
  canonical_b <- kappa_v * u + b
  Q <- diag.spam(x = kappa_v, nrow = problem$n) + diag.spam(c)
  
  sample <- c(rmvnorm.canonical(n = 1, b = canonical_b, Q = Q))
  logprob <- dmvnorm.canonical(x = sample, b = canonical_b, Q = Q, log = TRUE)[[1]]
  return(list(sample = sample, logprob = logprob))
}
```

Here we have calculated the $\log q(\boldsymbol{\eta^*}~|~z = \boldsymbol{\eta}^{(m - 1)})$ using `dmvnorm.canonical` as implemented in the provided file \texttt{dmvnorm.R}.
This is because it will be needed in the in the calculation for the acceptance probability $\boldsymbol{\alpha}^{(m)}$ for setting $\boldsymbol{\eta}^{(m)} \leftarrow \boldsymbol{\eta}^*$.

$$
\boldsymbol{\alpha}^{(m)}
=
\min \left(
  1,
  \frac{
    p \left(
      \boldsymbol{\eta^*}
      ~|~
      \boldsymbol{y},
      \boldsymbol{u}^{(m)},
      \kappa_u^{(m)},
      \kappa_v^{(m)}
      \right)
  }{
    p \left(
      \boldsymbol{\eta^{(m-1)}}
      ~|~
      \boldsymbol{y},
      \boldsymbol{u}^{(m)},
      \kappa_u^{(m)},
      \kappa_v^{(m)}
      \right)
  }
  \cdot
    \frac{
      q \left(
        \boldsymbol{\eta^{(m-1)}}
        ~|~
        \boldsymbol{z} = \boldsymbol{\eta}^*,
        \boldsymbol{y},
        \boldsymbol{u}^{(m)},
        \kappa_u^{(m)},
        \kappa_v^{(m)}
        \right)
    }{
      q \left(
        \boldsymbol{\eta^*}
        ~|~
        \boldsymbol{z} = \boldsymbol{\eta}^{(m - 1)},
        \boldsymbol{y},
        \boldsymbol{u}^{(m)},
        \kappa_u^{(m)},
        \kappa_v^{(m)}
        \right)
    }
\right)
$$

It should be noted that these density calculations will be performed in log-space going forwards.

The calculation of $\log{q(...)}$ is already handled by \texttt{draw_proposal_eta()}, but we must implement $\log{p(...)}$.
First notice that all the conditionals are equal in both $p$ function invocations, so we need not normalize the implementation of $\log{p(...)}$ as the normality constants cancel out. The implementation is therefore as follows

```{r}
eta_log_density <- function(eta, kappa_v, u, problem) {
  #' Proportional full conditional density for eta
  return(
    -0.5 * t(eta) %*% diag.spam(x = kappa_v, nrow = problem$n) %*% eta
    + t(eta) %*% (kappa_v * u)
    + t(eta) %*% problem$y
    - t(exp(eta)) %*% problem$E
  )
}
```

We can now implement the calculation of $\boldsymbol{\alpha}^{(m)}$.

```{r}
acceptance_probability <- function(proposal_eta, previous_eta, kappa_v, u, problem) {
  log_p_forward <- eta_log_density(
          eta = proposal_eta$sample,
          kappa_v = kappa_v,
          u = u,
          problem = problem
  )
  log_p_backward <- eta_log_density(
          eta = previous_eta$sample,
          kappa_v = kappa_v,
          u = u,
          problem = problem
  )
  
  log_q_forward <- proposal_eta$logprob
  log_q_backward <- draw_proposal_eta(z = proposal_eta$sample, u = u, kappa_v = kappa_v, problem = problem)$logprob
  
  alpha <- exp(log_p_forward + log_q_backward - log_p_backward - log_q_forward)
  
  if (alpha > 1) {
    return(1)
  }
  return(alpha)
}
```

### Implementing the MCMC algorithm

Implement MCMC

```{r, cache = TRUE}
steps <- 5000

# Initial guess for parameters u and eta
u <- c(rep_len(0.0, problem$n))
eta <- draw_proposal_eta(z = u, u = u, kappa_v = 0.0001, problem = problem)

# Data structures for saving sample results
kappa_us <- vector()
kappa_vs <- vector()
etas <- matrix(data = NA, nrow = steps, ncol = problem$n)
us <- matrix(data = NA, nrow = steps, ncol = problem$n)

for (i in seq(1, steps)) {
  kappa_u <- draw_kappa_u(u = u, problem = problem)
  kappa_v <- draw_kappa_v(eta = eta$sample, u = u, problem = problem)
  u <- draw_u(kappa_v = kappa_v, kappa_u = kappa_u, eta = eta$sample, problem = problem)
  
  proposal_eta <- draw_proposal_eta(
          z = eta$sample,
          u = u,
          kappa_v = kappa_v,
          problem = problem
  )
  alpha <- acceptance_probability(
    proposal_eta = proposal_eta,
    previous_eta = eta,
    kappa_v = kappa_v,
    u = u,
    problem = problem
  )
  if (runif(1)[1] < alpha) {
    eta = proposal_eta
  }
  
  # Appending results
  kappa_us <- c(kappa_us, kappa_u)
  kappa_vs <- c(kappa_vs, kappa_v)
  us[i,] = u
  etas[i,] = eta$sample
}
```

## Convergence diagnostics

```{r}
vs <- etas - us

samples <- tibble(
  step = seq(1, steps),
  kappa_u = kappa_us, kappa_v = kappa_vs,
  u_1 = us[, 80], u_2 = us[, 360], u_3 = us[, 500],
  v_1 = vs[, 80], v_2 = vs[, 360], v_3 = vs[, 500]
)
params <- colnames(samples)[-1]
```

### Trace plots

```{r, fig.width = 8.3, fig.height = 11.7, fig.fullwidth=TRUE}
burnin <- seq(1, 500)
samples %>%
  slice(-burnin) %>%
  gather(key, value, params) %>%
  ggplot(aes(x = step)) +
  geom_line(
    aes(y = value)
  ) +
  facet_wrap(vars(key), scales = "free_y", ncol = 2)
```

### Autocorrelation plots

```{r, fig.width = 8.3, fig.height = 11.7, fig.fullwidth=TRUE}
par(mfrow=c(4,2))
for (param in params) {
  acf(samples[param])
}
```
 
### Convergence check with `geweke.diag()`
 
```{r}
library(coda)
library(kableExtra)

burnins <- seq(1, steps/10)
effective_samples <- samples[-burnins, -1]
z_statistics <- geweke.diag(
  effective_samples,
  frac1=0.1, frac2=0.5
)$z
p_values <- 2 * pnorm(abs(z_statistics), lower.tail = FALSE)
geweke_results <- tibble(
  parameter = names(z_statistics),
  z_statistic = z_statistics,
  p_value = p_values
)
geweke_results %>% kable()

zstats <- geweke.diag(us)
pvals <- 2 * pnorm(abs(zstats$z), lower.tail = FALSE)
hist <- hist(pvals)

zstats2 <- geweke.diag(vs)
pvals2 <- 2 * pnorm(abs(zstats$z), lower.tail = FALSE)
hist2 <- hist(pvals2)
```

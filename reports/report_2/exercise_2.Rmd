---
output:
  pdf_document:
    fig_caption: yes
  html_document:
    fig_caption: yes
---
# Exercise 2 - Implementation of the MCMC sampler

\providecommand{\kappau}{\kappa_u}
\providecommand{\kappav}{\kappa_v}
\providecommand{\uvec}{\boldsymbol{u}}
\providecommand{\Rmat}{\boldsymbol{R}}
\providecommand{\yvec}{\boldsymbol{y}}
\providecommand{\Evec}{\boldsymbol{E}}
\providecommand{\etavec}{\boldsymbol{\eta}}

## Importing the data set

We will now implement an MCMC sampler for the parameters discussed in exercise 1.
First, let's import the libraries which will be used in the implementation.

```{r}
# Lots of helper functions and new data types
library(tidyverse)

# For sparse matrix support
library(spam)

# Spatial data library
library(fields, warn.conflict=FALSE)

# Custom color scheme
library(colorspace)
col <- diverge_hcl(8)
```

The data of interest is contained in the `Oral` dataset.
The standardised mortality rates (SMR) $y_i / E_i$ can be visualized on top of the map of germany's districts.

```{r}
attach(Oral)
germany.plot(Oral$Y / Oral$E, col=col, legend=TRUE)
```

The neighbourhood structure matrix, $\boldsymbol{R}$, is provided in the data file \texttt{tma4300_ex2_Rmatrix.Rdata}.

```{r}
load("data/tma4300_ex2_Rmatrix.Rdata")
```

We will now define a list named \texttt{problem}, which will contain the global state for our problem.
It will contain constants for our problem, such as $\yvec$, $\boldsymbol{E}$, $n$, $\boldsymbol{r}$, $\alpha_u$, $\alpha_v$, $\beta_u$, and $\beta_v$.
These variables will remain constant through all the iterations of the MCMC algorithm, and will be passed as the `problem` parameter to all functions.

```{r}
problem = list(
  y = Oral$Y,
  E = Oral$E,
  n = length(Oral$Y),
  R = R,
  alpha_u = 1,
  alpha_v = 1,
  beta_u = 0.01,
  beta_v = 0.01
)
```

## Implementing full conditional sampling functions

We start by implementing full conditional samplers for $\boldsymbol{\kappa}^{(m)}_u$, $\boldsymbol{\kappa}^{(m)}_v$, and $\boldsymbol{u}^{(m)}$.

### Sampling $\boldsymbol{\kappa}^{(m)}_u$ and $\boldsymbol{\kappa}^{(m)}_v$

The full conditional for $\kappa_u$ is

$$
\kappa_u~|~\boldsymbol{y}, \kappa_v^{(m-1)}, \boldsymbol{\eta}^{(m - 1)}, \boldsymbol{u}^{(m -  1)}
\sim \text{Gamma}\left(\frac{n - 1}{2} + \alpha_u, \frac{1}{2} {\boldsymbol{u}^{(m - 1)}}^{T} R \boldsymbol{u}^{(m - 1)} + \beta_u \right)
$$

Which can be implemented with the \texttt{rgamma} R-function.

```{r}
draw_kappa_u <- function(u, problem) {
  shape <- (problem$n - 1) / 2 + problem$alpha_u
  rate <- 0.5 * t(u) %*% problem$R %*% u + problem$beta_u
  sample <- rgamma(shape = shape, rate = rate, n = 1)[[1]]
  return(sample)
}
```

Likewise, for $\kappa_v$ we have the full conditional

$$
\kappa_v~|~\boldsymbol{y}, \kappa_u^{(m)}, \boldsymbol{\eta}^{(m - 1)}, \boldsymbol{u}^{(m -  1)}
\sim \text{Gamma}\left(\frac{n}{2} + \alpha_v, \frac{1}{2} \left(\boldsymbol{\eta}^{(m - 1)} - \boldsymbol{u}^{(m - 1)} \right)^{T} \left(\boldsymbol{\eta}^{(m - 1)} - \boldsymbol{u}^{(m - 1)} \right) + \beta_v \right),
$$

Which will implemented in the same way

```{r}
draw_kappa_v <- function(eta, u, problem) {
  shape <- problem$n / 2 + problem$alpha_v
  rate <- 0.5 * t(eta - u) %*% (eta - u) + problem$beta_v
  sample <- rgamma(shape = shape, rate = rate, n = 1)[[1]]
  return(sample)
}
```

### Full conditional sampler for $\uvec$

The full conditional for $\boldsymbol{u}$ is

$$
\boldsymbol{u}~|~\boldsymbol{\eta}^{(n - 1)}, \kappa_u^{(m)}, \kappa_v^{(m)}
\sim
\mathcal{N}\left(
  \left(\kappa_v^{(m)} I + \kappa_u^{(m)} R\right)^{-1} \kappa_v^{(m)} \boldsymbol{\eta}^{(m - 1)},~
  \left(\kappa_v^{(m)} I + \kappa_u^{(m)} R\right)^{-1}
\right)
$$
Now, notice that the adjacency matrix $R$ is a sparse matrix. Thus, $\left(\kappa_v^{(n - 1)} I + \kappa_u^{(n - 1)} R\right)$ is also a sparse matrix.
It is therefore preferable to store this matrix as a *sparse* matrix, using the `spam` R-library.
Additionally, we use the `rmvnorm.canonical` function in order to formulate the normal distribution in form of the sparse *precision* matrix instead of the covariance matrix. With other words

\begin{align*}
Q \leftarrow &\kappa_v^{(m)} I + \kappa_u^{(m)} R \\
b \leftarrow &\kappa_v^{(m)} \boldsymbol{\eta}^{(m - 1)}
\end{align*}

The implementation of the full conditional sampler for $\boldsymbol{u}$ therefore becomes

```{r}
draw_u <- function(kappa_v, kappa_u, eta, problem) {
  Q <- diag.spam(x = kappa_v, nrow = problem$n) + kappa_u * problem$R
  b <- kappa_v * eta
  sample <- c(rmvnorm.canonical(n = 1, b = b, Q = Q))
  return(sample)
}
```

### Metropolis-Hastings step for $\etavec$

The sampling for $\boldsymbol{\eta}$ requires a Metropolis-Hastings step, since the distribution is not in a well-known form.

We will draw a proposal, $\boldsymbol{\eta}^*$, from the taylor expansion $q(\boldsymbol{\eta}^* ~|~ ...)$ of $p(\boldsymbol{\eta}^* ~|~ ...)$ around $z = \boldsymbol{\eta}^{(m - 1)}$.

$$
q \left(
  \boldsymbol{\eta^*}
  ~|~
    \boldsymbol{z} = \boldsymbol{\eta}^{(m - 1)},
    \boldsymbol{y},
    \boldsymbol{u}^{(m)},
    \kappa_u^{(m)},
    \kappa_v^{(m)}
\right)
\propto
\exp \left\{
  -\frac{1}{2} {\boldsymbol{\eta}^*}^T
    \left(
      \kappa_v^{(m)} \textbf{I} + \text{diag}(c(\boldsymbol{\eta}^{(m - 1)})) 
    \right)
    \boldsymbol{\eta}^*
  + {\boldsymbol{\eta}^*}^T \left( \kappa_v^{(m)} \boldsymbol{u}^{(m)} + b(\boldsymbol{\eta}^{(m - 1)}) \right)
\right\},
$$

with $b(\boldsymbol{z})$ and $c(\boldsymbol{z})$ defined as in exercise 1.
Again, we can use the sparse, canonical representation in the implementation

\begin{align*}
  Q \leftarrow &\kappa_v^{(m)} \textbf{I} + \text{diag}(c(\boldsymbol{\eta}^{(m - 1)})) \\
  b \leftarrow &\kappa_v^{(m)} \boldsymbol{u}^{(m)} + b(\boldsymbol{\eta}^{(m - 1)})
\end{align*}

So the proposal sample drawer is therefore implemented as follows

```{r}
source("data/dmvnorm.R")
draw_proposal_eta <- function(z, u, kappa_v, problem) {
  b <- problem$y + problem$E * exp(z) * (z - 1)
  c <- problem$E * exp(z)
  
  canonical_b <- kappa_v * u + b
  Q <- diag.spam(x = kappa_v, nrow = problem$n) + diag.spam(c)
  
  sample <- c(rmvnorm.canonical(n = 1, b = canonical_b, Q = Q))
  logprob <- dmvnorm.canonical(x = sample, b = canonical_b, Q = Q, log = TRUE)[[1]]
  return(list(sample = sample, logprob = logprob))
}
```

Here we have calculated the $\log q(\boldsymbol{\eta^*}~|~z = \boldsymbol{\eta}^{(m - 1)})$ using `dmvnorm.canonical` as implemented in the provided file \texttt{dmvnorm.R}.
This is because it will be needed in the in the calculation for the acceptance probability $\boldsymbol{\alpha}^{(m)}$ for setting $\boldsymbol{\eta}^{(m)} \leftarrow \boldsymbol{\eta}^*$.

$$
\boldsymbol{\alpha}^{(m)}
=
\min \left(
  1,
  \frac{
    p \left(
      \boldsymbol{\eta^*}
      ~|~
      \boldsymbol{y},
      \boldsymbol{u}^{(m)},
      \kappa_u^{(m)},
      \kappa_v^{(m)}
      \right)
  }{
    p \left(
      \boldsymbol{\eta^{(m-1)}}
      ~|~
      \boldsymbol{y},
      \boldsymbol{u}^{(m)},
      \kappa_u^{(m)},
      \kappa_v^{(m)}
      \right)
  }
  \cdot
    \frac{
      q \left(
        \boldsymbol{\eta^{(m-1)}}
        ~|~
        \boldsymbol{z} = \boldsymbol{\eta}^*,
        \boldsymbol{y},
        \boldsymbol{u}^{(m)},
        \kappa_u^{(m)},
        \kappa_v^{(m)}
        \right)
    }{
      q \left(
        \boldsymbol{\eta^*}
        ~|~
        \boldsymbol{z} = \boldsymbol{\eta}^{(m - 1)},
        \boldsymbol{y},
        \boldsymbol{u}^{(m)},
        \kappa_u^{(m)},
        \kappa_v^{(m)}
        \right)
    }
\right)
$$

It should be noted that these density calculations will be performed in log-space going forwards.

The calculation of $\log{q(...)}$ is already handled by \texttt{draw_proposal_eta()}, but we must implement $\log{p(...)}$.
First notice that all the conditionals are equal in both $p$ function invocations, so we need not normalize the implementation of $\log{p(...)}$ as the normality constants cancel out. The implementation is therefore as follows

```{r}
eta_log_density <- function(eta, kappa_v, u, problem) {
  #' Proportional full conditional density for eta
  return(
    -0.5 * t(eta) %*% diag.spam(x = kappa_v, nrow = problem$n) %*% eta
    + t(eta) %*% (kappa_v * u)
    + t(eta) %*% problem$y
    - t(exp(eta)) %*% problem$E
  )
}
```

We can now implement the calculation of $\boldsymbol{\alpha}^{(m)}$.

```{r}
acceptance_probability <- function(proposal_eta, previous_eta, kappa_v, u, problem) {
  log_p_forward <- eta_log_density(
          eta = proposal_eta$sample,
          kappa_v = kappa_v,
          u = u,
          problem = problem
  )
  log_p_backward <- eta_log_density(
          eta = previous_eta$sample,
          kappa_v = kappa_v,
          u = u,
          problem = problem
  )
  
  log_q_forward <- proposal_eta$logprob
  log_q_backward <- draw_proposal_eta(z = proposal_eta$sample, u = u, kappa_v = kappa_v, problem = problem)$logprob
  
  alpha <- exp(log_p_forward + log_q_backward - log_p_backward - log_q_forward)
  
  if (alpha > 1) {
    return(1)
  }
  return(alpha)
}
```

### Implementing the MCMC algorithm

We now have all the required components required to implement the outer loop MCMC algorithm.
The algorithm requires initial values for $\boldsymbol{u}^{(1)}$ and $\boldsymbol{\eta}^{(1)}$.
We will set (rather arbitrarily) $\boldsymbol{u}^{(1)} = \vec{0}$, while setting the value of $\boldsymbol{\eta}^{(0)}$ by sampling our proposal density $q(...)$ with the following parameters

\begin{align*}
  \boldsymbol{z} \leftarrow& \vec{0} \\
  \boldsymbol{u} \leftarrow& \vec{0} \\
  \kappa_v \leftarrow& 500
\end{align*}

```{r, cache = TRUE}
MCMC <- function(steps, problem) {
  # Initial guess for parameters u and eta
  u <- c(rep_len(0.0, problem$n))
  eta <- draw_proposal_eta(z = u, u = u, kappa_v = 500, problem = problem)
  
  # Data structures for saving sample results
  kappa_us <- vector()
  kappa_vs <- vector()
  etas <- matrix(data = NA, nrow = steps, ncol = problem$n)
  us <- matrix(data = NA, nrow = steps, ncol = problem$n)
  alphas <- vector()
  
  # Save start time, used for tracking time usage of the MCMC algorithm
  start_time <- Sys.time()
  
  for (i in seq(1, steps)) {
    kappa_u <- draw_kappa_u(u = u, problem = problem)
    kappa_v <- draw_kappa_v(eta = eta$sample, u = u, problem = problem)
    u <- draw_u(kappa_v = kappa_v, kappa_u = kappa_u, eta = eta$sample, problem = problem)
    
    proposal_eta <- draw_proposal_eta(
            z = eta$sample,
            u = u,
            kappa_v = kappa_v,
            problem = problem
    )
    alpha <- acceptance_probability(
      proposal_eta = proposal_eta,
      previous_eta = eta,
      kappa_v = kappa_v,
      u = u,
      problem = problem
    )
    if (runif(1)[1] < alpha) {
      eta = proposal_eta
    }
    
    # Appending results
    kappa_us <- c(kappa_us, kappa_u)
    kappa_vs <- c(kappa_vs, kappa_v)
    alphas <- c(alphas, alpha)
    us[i,] = u
    etas[i,] = eta$sample
  }
  
  # Calculate the time used by the MCMC algorithm
  end_time <- Sys.time()
  time_used <- end_time - start_time
  time_used_per_step <- time_used / steps
  
  # Returning the final result in the form of a list
  result <- list(
    kappa_u = kappa_us,
    kappa_v = kappa_vs,
    u = us,
    eta = etas,
    alpha = alphas,
    time_used = time_used,
    time_used_per_step = time_used_per_step
  )
  return(result)
}
```

A posterior sample size of $M = 70~000$, after having a burn-in period of $1000$ steps, is sampled.
We will see shortly that this burn-in period is more than sufficient.

```{r, eval = FALSE}
set.seed(0)
M <- 70000
burnin <- 1000
result <- MCMC(steps = M + burnin, problem = problem)
```

```{r, show = FALSE, eval = FALSE}
# Run this line if it is the first time you have executed MCMC and want to save the result
save(result, list = c("result"), file = "data/MCMC_result.Rdata")
```

```{r, show = FALSE}
# Load the saved result for `result`
M <- 70000
burnin <- 1000
load(file = "data/MCMC_result.Rdata")
```

#### Time usage

The time used by the algorithm is part of the returned result.

```{r}
result$time_used
```

$71~000$ iterations of the algorithm uses approximately 10 minutes, which implies the following time usage per iteration

```{r}
milliseconds <- 1000 * 60 * as.numeric(result$time_used_per_step)
sprintf("~%1.1fms per iteration", milliseconds)
```

I.e., $\propto 8.6$ milliseconds per iteration, each iteration creating $2n + 3 = 1091$ double data points.

#### Acceptance rates

The post-burn-in acceptance rates, $\boldsymbol{\alpha}$, for the Metropolis-Hastings step which samples $\boldsymbol{\eta}^{(m)}$ is also retrievable from the result.

```{r}
alpha <- result$alpha[-c(1:burnin)]
mean(alpha)
```

The acceptance probability is, on average, approximately $50\%$.
A more interesting overview of the distribution of $\alpha^{(m)}$ can be observed by plotting a histogram.

```{r}
binwidth <- 0.04
enframe(alpha) %>%
  ggplot() +
  aes(x = value) +
  geom_histogram(
    aes(y = binwidth * ..density..),
    boundary = 0,
    breaks = seq(0.0, 1.0, by = binwidth)
  ) +
  ylab("frequency") +
  xlab(expression(alpha)) +
  labs(
    title = "Histogram of calculated acceptance probabilities",
    caption = "The distribution of alpha values for all the post-burn-in acceptance probabilities"
  )
```

As you can see from the histogram for $\boldsymbol{\alpha}$, most values for $\alpha$ are either close to or equal to $0$ or $1$.
This is the reason for the $\alpha$ mean being $\approx 0.5$. Half of the proposals are almost guaranteed to be accepted, while the other half is almost guaranteed not to. We can calculate the fraction of the acceptance probabilities that can be considered extreme, let's say $\alpha \notin [0.01, 0.99]$.

```{r}
extreme_values_count <- length(alpha[alpha < 0.01]) + length(alpha[alpha > 0.99])
extreme_values_count / length(alpha)
```

$\approx 92\%$ of the proposals are of that nature!

# Exercise 3 - Convergence diagnostics

We will now make an attempt at diagnosing the convergence of the samples generated from the Monte Carlo Markov Chain algorithm.
First, since $\boldsymbol{\eta}$ is decomposed as $\boldsymbol{u} + \boldsymbol{u}$, we can retrieve samples of $\boldsymbol{v} = \boldsymbol{\eta} - \boldsymbol{u}$.

```{r}
vs <- result$eta - result$u
```

We will investigate the precision parameters $\kappa_u$ and $\kappa_v$ in addition to three randomly chosen components of $\boldsymbol{u}$ and $\boldsymbol{v}$. The three components $\{80, 360, 500\}$ will be used for both $\boldsymbol{u}$ and $\boldsymbol{v}$. This data subset is inserted into a data frame.

```{r}
samples <- tibble(
  step = seq(1, burnin + M),
  kappa_u = result$kappa_u, kappa_v = result$kappa_v,
  u_1 = result$u[, 80], u_2 = result$u[, 360], u_3 = result$u[, 500],
  v_1 = vs[, 80], v_2 = vs[, 360], v_3 = vs[, 500]
)
params <- colnames(samples)[-1]
```

### 3a) Trace plots

#### After burn-in period

First, we will plot the trace plots for all the chosen sample parameters, with the burn-in period removed.

```{r, fig.width = 8.3, fig.height = 11.7, fig.fullwidth=TRUE}
burnins <- seq(1, burnin)
samples %>%
  slice(-burnins) %>%
  gather(key, value, params) %>%
  ggplot(aes(x = step)) +
  geom_line(aes(y = value)) +
  facet_wrap(vars(key), scales = "free_y", ncol = 2) +
  labs(
    title = "Parameter trace plots",
    caption = "Figure 3-1: Sample line plot for each step of the MCMC algorithm (post-burn-in)"
  )
```

By observing figure `3-1`, we can observe that all 8 parameters portray a "band-like" shape, which is a good indication for convergence.
Additionally, there is no indication of the parameters becoming "stuck" at any point during the iterations.
The samples of the three $\boldsymbol{v}$-components seem to be symmetrically distributed around $0$, which is what we would expect to see from these unstructured white-noise parameters. The post-burn-in posterior means can be calculated in order to confirm this

```{r}
library(kableExtra)
samples[,-1] %>% slice(-burnins) %>% colMeans() %>% kable()
```

Nothing out of the ordinary here to indicate something wrong with the implementation.

#### burn-in period

Lastly, let's justify our earlier assumption of a burn-in period of $1~000$ steps being more than sufficient.
We will plot trace plots for the first $2~000$ steps.

```{r, fig.width = 8.3, fig.height = 8, fig.fullwidth=TRUE, fig.cap = "\\label{fig:burnin} Sample line plot for beginning steps of the MCMC algorithm (burn-in). The red line indicates the chosen burn-in boundary."}
beginning <- seq(1, 2 * burnin)
samples %>%
  slice(beginning) %>%
  gather(key, value, params) %>%
  ggplot(aes(x = step)) +
  geom_line(aes(y = value)) +
  geom_vline(
    xintercept = burnin,
    color = "red"
  ) +
  facet_wrap(vars(key), scales = "free_y", ncol = 2) +
  labs(title = "Parameter burn-in trace plots")
```

\newpage
From figure \ref{fig:burnin} it becomes clear that all the 8 chains have converged long before a thousand steps (red vertical line) have been calculated.
$\kappa_u$ is the most extreme example beginning at $\approx 25~000$, and we have already seen that it has a calculated posterior mean of $\approx 16$, but it quickly converges to the (probably) correct domain.

This "zoomed in" plot also allows us to inspect the degree of movement of each chain to a better degree.
For instance, the $\kappa_v$ sample plot is the most "sticky" of them all, and will probably show a greater degree of auto-correlation.
The three $\boldsymbol{v}$ components show the greatest degree of variability in movement, and we would expect good results for these components in the upcoming ACF plots.

### 3b) Autocorrelation plots

Now we plot the autocorrelation of each of the 8 parameters of interest.

```{r acf, fig.width = 8.3, fig.height = 10, fig.fullwidth=TRUE, fig.cap = "\\label{fig:acf} Autocorrelation plots for all 8 parameters of interest."}
par(mfrow=c(4,2))
usable_samples <- samples[-burnins, -1]
for (param in params) {
  acf(usable_samples[param])
}
```

\newpage
As we can see in figure \ref{fig:acf}, $\kappa_v$ and $\kappa_u$ show the greatest degree of autocorrelation, as predicted from the trace plots in figure \ref{fig:burnin} earlier. We can investigate this in more detail by plotting the ACF for $\kappa_v$ with a larger displayed lag.

```{r acf_kappa_u, fig.cap = "\\label{fig:acf_kappa_u} Autocorrelation function plot for kappa_u with max lag set to 200."}
acf(usable_samples["kappa_u"], lag.max = 200)
```

As can be seen in figure \ref{fig:acf_kappa_u}, a lag of $\approx 200$ is required before two samples can be considered independent of each other. This autocorrelation must be remedied by generating a sufficient amount of samples, as it reduces our effective number of samples.

The three $\boldsymbol{v}$ components in figure \ref{fig:acf}, i.e. unstructured random noise, have the lowest degree of autocorrelation, also as predicted from the previous plots.

 
### 3c) Convergence check with `geweke.diag()`

We will test the convergence of the Markov chains by using the function \texttt{geweke.diag()} from the \texttt{coda} R-package.
Each chain is tested independently in the following way

\begin{itemize}
  \item Remove the burn-in samples.
  \item Extract the first $10\%$ samples and $50\%$ last samples from the chain.
  \item Calculate the difference of the two sample means, and divide by its estimated standard error.
  \item Under the assumption that \textit{all} the samples are drawn from the stationary distribution of the chain, and the two chains being assymptotically independent, the result should be standard normal.
\end{itemize}

It are these $Z$-statistics which can be calculated for our parameters of interest with \texttt{geweke.diag()}. 

```{r}
library(coda)
z_statistics <- geweke.diag(
  usable_samples,
  frac1=0.1, frac2=0.5
)$z
```

We can now perform the following hypothesis test

\begin{gather*}
  \mathrm{H_0}: \text{The Markov chain has converged} \\
  \text{ vs. } \\
  \mathrm{H_1}: \text{The Markov chain has } \textit{not} \text{ converged}.
\end{gather*}

This is a two-tailed hypothesis test, for which we can calculate a corresponding $p$-value for

$$
p\text{-value} = 2 \cdot \text{P}(Z \geq |z|)
$$

This is what we will now calculate. Now, remember that a discarded null hypothesis in this case implies *non-convergence*, not the other way around.

```{r}
p_values <- 2 * pnorm(abs(z_statistics), lower.tail = FALSE)
geweke_results <- tibble(
  parameter = names(z_statistics),
  z_statistic = z_statistics,
  p_value = p_values
)
geweke_results %>% kable()
```

These are really promising results as not a single null hypothesis needs to be discarded even at a $10\%$ confidence level.
In the case of convergence we would expect the $p$-values to be uniformly distributed between zero and one, i.e. $p \sim \mathcal{U}(0, 1)$. We can check if this is really the case for all $\boldsymbol{u}$ and $\boldsymbol{v}$ components by plotting a $p$-value histogram for both decompositions.

```{r p_values_plot, fig.fullwidth = TRUE, fig.cap = "\\label{fig:p_values_plot} Histograms for the p-values of the Geweke tests for all components of u and v."}
par(mfcol = c(1, 2))
u_zstats <- geweke.diag(result$u[-burnins,])$z
u_p_values <- 2 * pnorm(abs(u_zstats), lower.tail = FALSE)
u_histogram <- hist(u_p_values, xlab = "p-value", main = "Components of u")

v_zstats <- geweke.diag(vs[-burnins,])$z
v_p_values <- 2 * pnorm(abs(v_zstats), lower.tail = FALSE)
v_histogram <- hist(v_p_values, xlab = "p-value", main = "Components of v")
```

As can be seen in figure \ref{fig:p_values_plot}, nothing points to the contrary of the $p$-values being realizations of a uniform distribution. There is therefore no reason to conclude that we do not have convergence.

#### Conclusion

None of the three convergence diagnostic methods definitely refute the notion of chain convergence, which is good.
It is never possible to conclude with \textit{certainty} that an MCMC algorithm has converged, but the presented diagnostics strengthens our belief in that the chains have converged and that the post-burn-in samples are distributed according to the target distributions.


# Exercise 4 - Effective sample size

## ESS

### Theory

We now want to investigate the effective sample size (ESS) for our $\kappa_u$ and $\kappa_v$ parameter samples.

ESS is an estimation for how many perfectly independent samples that would be required in order to obtain a parameter estimate that has the \textit{same} precision as an estimate generated from our MCMC sampler with $M$ samples. Therefore, $\text{ESS} \leq M$ in all cases, and the closer they are, the better.

In order to calculate this, first define the \textit{autocorrelation time}, $\tau$, as

$$
\tau := 1 + 2 \cdot \sum_{k = 1}^{\infty} \rho(k),
$$
where $\rho(k)$ is the autocorrelation at lag $k$, earlier shown in the ACF plots in exercise 3.
Now, ESS can be calculated as

$$
\text{ESS} = \frac{N}{\tau}
$$

### Calculation and interpretation

The \texttt{effectiveSize()} function from the \texttt{coda} R-library can be used to calculate the ESS for our MCMC samples.

```{r}
ESS <- effectiveSize(usable_samples)
ESS[c("kappa_u", "kappa_v")]
```

What does this imply?
It is estimated that independent samplers for $\kappa_u$ and $\kappa_v$ would only require $766$ and $663$ samples, respectively, in order to obtain parameter estimates with the same precision as if we used our MCMC samples of size $70~000$. This is approximately $1\%$.

With other words, an independent sampler that runs 100 times as slow as our MCMC implementation would therefore be of approximately equal benefit as our MCMC implementation, not considering memory usage and burn-in periods.

### Possible improvements for the ESS values

Large values for ESS is to be preferred, and there are several strategies that can be devised in order to improve the value for ESS, for instance

\begin{itemize}
  \item Combine the results of \textit{multiple} independent chains. Compare the results in order to check convergence and if they mix properly.
  \item Perform more steps of the MCMC algorithm. This would of course increase the absolute value of the ESS, but might not increase its \textit{relative} value.
  \item Try to integrate out certain variables, for instance $\kappa_v$ and $\kappa_u$, but this might not be analytically feasible. 
  \item Finding a better proposal density for $\boldsymbol{\eta}$. This will increase the acceptance probability, and therefore decrease the autocorrelation of $\boldsymbol{\eta}$ and all its dependents.
  \item Reparametrizing the model.
\end{itemize}

## Relative ESS

### Calculation

We can also calculate the \textit{relative} ESS by dividing the mean ESS by the time used by the MCMC algorithm.

```{r}
seconds_used <- 60 * as.numeric(result$time_used)
relative_ESS <- mean(ESS[c("kappa_u", "kappa_v")]) / seconds_used
sprintf("%1.3f effective samples per second", relative_ESS)
```

Our implementation generates $\approx 1.16$ effective samples per second.

### Interpretation

In isolation, this might not be very interesting, but it is very useful for comparison purposes.
Making a change to an MCMC implementation in order to improve its resulting ESS might not be worth it if it substantially increases the time used by the algorithm. Solely comparing values for ESS not give the entire picture of the situation. For instance, "blocking" correlated variables updates together might be too slow compared to a more naive joint update, and the change may not be worth it in terms of computational time cost. Comparison of relative ESS can in such cases be used as a deciding factor.


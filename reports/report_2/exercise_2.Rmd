---
output:
  pdf_document: default
  html_document: default
---
# Exercise 2 - Implementation of the MCMC sampler

\providecommand{\kappau}{\kappa_u}
\providecommand{\kappav}{\kappa_v}
\providecommand{\uvec}{\boldsymbol{u}}
\providecommand{\Rmat}{\boldsymbol{R}}
\providecommand{\yvec}{\boldsymbol{y}}
\providecommand{\Evec}{\boldsymbol{E}}
\providecommand{\etavec}{\boldsymbol{\eta}}

## Importing the data set

We will now implement an MCMC sampler for the parameters discussed in exercise 1.
First, let's import the libraries which will be used in the implementation.

```{r}
# Lots of helper functions and new data types
library(tidyverse)

# For sparse matrix support
library(spam)

# Spatial data library
library(fields, warn.conflict=FALSE)

# Custom color scheme
library(colorspace)
col <- diverge_hcl(8)
```

The data of interest is contained in the `Oral` dataset.
The standardised mortality rates (SMR) $y_i / E_i$ can be visualized on top of the map of germany's districts.

```{r}
attach(Oral)
germany.plot(Oral$Y / Oral$E, col=col, legend=TRUE)
```

The neighbourhood structure matrix, $\boldsymbol{R}$, is provided in the data file \texttt{tma4300_ex2_Rmatrix.Rdata}.

```{r}
load("data/tma4300_ex2_Rmatrix.Rdata")
```

We will now define a list named \texttt{problem}, which will contain the global state for our problem.
It will contain constants for our problem, such as $\yvec$, $\boldsymbol{E}$, $n$, $\boldsymbol{r}$, $\alpha_u$, $\alpha_v$, $\beta_u$, and $\beta_v$.
These variables will remain constant through all the iterations of the MCMC algorithm, and will be passed as the `problem` parameter to all functions.

```{r}
problem = list(
  y = Oral$Y,
  E = Oral$E,
  n = length(Oral$Y),
  R = R,
  alpha_u = 1,
  alpha_v = 1,
  beta_u = 0.01,
  beta_v = 0.01
)
```

## Implementing full conditional sampling functions

We start by implementing full conditional samplers for $\boldsymbol{\kappa}^{(m)}_u$, $\boldsymbol{\kappa}^{(m)}_v$, and $\boldsymbol{u}^{(m)}$.

### Sampling $\boldsymbol{\kappa}^{(m)}_u$ and $\boldsymbol{\kappa}^{(m)}_v$

The full conditional for $\kappa_u$ is

$$
\kappa_u~|~\boldsymbol{y}, \kappa_v^{(m-1)}, \boldsymbol{\eta}^{(m - 1)}, \boldsymbol{u}^{(m -  1)}
\sim \text{Gamma}\left(\frac{n - 1}{2} + \alpha_u, \frac{1}{2} {\boldsymbol{u}^{(m - 1)}}^{T} R \boldsymbol{u}^{(m - 1)} + \beta_u \right)
$$

Which can be implemented with the \texttt{rgamma} R-function.

```{r}
draw_kappa_u <- function(u, problem) {
  shape <- (problem$n - 1) / 2 + problem$alpha_u
  rate <- 0.5 * t(u) %*% problem$R %*% u + problem$beta_u
  sample <- rgamma(shape = shape, rate = rate, n = 1)[[1]]
  return(sample)
}
```

Likewise, for $\kappa_v$ we have the full conditional

$$
\kappa_v~|~\boldsymbol{y}, \kappa_u^{(m)}, \boldsymbol{\eta}^{(m - 1)}, \boldsymbol{u}^{(m -  1)}
\sim \text{Gamma}\left(\frac{n}{2} + \alpha_v, \frac{1}{2} \left(\boldsymbol{\eta}^{(m - 1)} - \boldsymbol{u}^{(m - 1)} \right)^{T} \left(\boldsymbol{\eta}^{(m - 1)} - \boldsymbol{u}^{(m - 1)} \right) + \beta_v \right),
$$

Which will implemented in the same way

```{r}
draw_kappa_v <- function(eta, u, problem) {
  shape <- problem$n / 2 + problem$alpha_v
  rate <- 0.5 * t(eta - u) %*% (eta - u) + problem$beta_v
  sample <- rgamma(shape = shape, rate = rate, n = 1)[[1]]
  return(sample)
}
```

### Full conditional sampler for $\uvec$

The full conditional for $\boldsymbol{u}$ is

$$
\boldsymbol{u}~|~\boldsymbol{\eta}^{(n - 1)}, \kappa_u^{(m)}, \kappa_v^{(m)}
\sim
\mathcal{N}\left(
  \left(\kappa_v^{(m)} I + \kappa_u^{(m)} R\right)^{-1} \kappa_v^{(m)} \boldsymbol{\eta}^{(m - 1)},~
  \left(\kappa_v^{(m)} I + \kappa_u^{(m)} R\right)^{-1}
\right)
$$
Now, notice that the adjacency matrix $R$ is a sparse matrix. Thus, $\left(\kappa_v^{(n - 1)} I + \kappa_u^{(n - 1)} R\right)$ is also a sparse matrix.
It is therefore preferable to store this matrix as a *sparse* matrix, using the `spam` R-library.
Additionally, we use the `rmvnorm.canonical` function in order to formulate the normal distribution in form of the sparse *precision* matrix instead of the covariance matrix. With other words

\begin{align*}
Q \leftarrow &\kappa_v^{(m)} I + \kappa_u^{(m)} R \\
b \leftarrow &\kappa_v^{(m)} \boldsymbol{\eta}^{(m - 1)}
\end{align*}

The implementation of the full conditional sampler for $\boldsymbol{u}$ therefore becomes

```{r}
draw_u <- function(kappa_v, kappa_u, eta, problem) {
  Q <- diag.spam(x = kappa_v, nrow = problem$n) + kappa_u * problem$R
  b <- kappa_v * eta
  sample <- c(rmvnorm.canonical(n = 1, b = b, Q = Q))
  return(sample)
}
```

### Metropolis-Hastings step for $\etavec$

The sampling for $\boldsymbol{\eta}$ requires a Metropolis-Hastings step, since the distribution is not in a well-known form.

We will draw a proposal, $\boldsymbol{\eta}^*$, from the taylor expansion $q(\boldsymbol{\eta}^* ~|~ ...)$ of $p(\boldsymbol{\eta}^* ~|~ ...)$ around $z = \boldsymbol{\eta}^{(m - 1)}$.

$$
q \left(
  \boldsymbol{\eta^*}
  ~|~
    \boldsymbol{z} = \boldsymbol{\eta}^{(m - 1)},
    \boldsymbol{y},
    \boldsymbol{u}^{(m)},
    \kappa_u^{(m)},
    \kappa_v^{(m)}
\right)
\propto
\exp \left\{
  -\frac{1}{2} {\boldsymbol{\eta}^*}^T
    \left(
      \kappa_v^{(m)} \textbf{I} + \text{diag}(c(\boldsymbol{\eta}^{(m - 1)})) 
    \right)
    \boldsymbol{\eta}^*
  + {\boldsymbol{\eta}^*}^T \left( \kappa_v^{(m)} \boldsymbol{u}^{(m)} + b(\boldsymbol{\eta}^{(m - 1)}) \right)
\right\},
$$

with $b(\boldsymbol{z})$ and $c(\boldsymbol{z})$ defined as in exercise 1.
Again, we can use the sparse, canonical representation in the implementation

\begin{align*}
  Q \leftarrow &\kappa_v^{(m)} \textbf{I} + \text{diag}(c(\boldsymbol{\eta}^{(m - 1)})) \\
  b \leftarrow &\kappa_v^{(m)} \boldsymbol{u}^{(m)} + b(\boldsymbol{\eta}^{(m - 1)})
\end{align*}

So the proposal sample drawer is therefore implemented as follows

```{r}
source("data/dmvnorm.R")
draw_proposal_eta <- function(z, u, kappa_v, problem) {
  b <- problem$y + problem$E * exp(z) * (z - 1)
  c <- problem$E * exp(z)
  
  canonical_b <- kappa_v * u + b
  Q <- diag.spam(x = kappa_v, nrow = problem$n) + diag.spam(c)
  
  sample <- c(rmvnorm.canonical(n = 1, b = canonical_b, Q = Q))
  logprob <- dmvnorm.canonical(x = sample, b = canonical_b, Q = Q, log = TRUE)[[1]]
  return(list(sample = sample, logprob = logprob))
}
```

Here we have calculated the $\log q(\boldsymbol{\eta^*}~|~z = \boldsymbol{\eta}^{(m - 1)})$ using `dmvnorm.canonical` as implemented in the provided file \texttt{dmvnorm.R}.
This is because it will be needed in the in the calculation for the acceptance probability $\boldsymbol{\alpha}^{(m)}$ for setting $\boldsymbol{\eta}^{(m)} \leftarrow \boldsymbol{\eta}^*$.

$$
\boldsymbol{\alpha}^{(m)}
=
\min \left(
  1,
  \frac{
    p \left(
      \boldsymbol{\eta^*}
      ~|~
      \boldsymbol{y},
      \boldsymbol{u}^{(m)},
      \kappa_u^{(m)},
      \kappa_v^{(m)}
      \right)
  }{
    p \left(
      \boldsymbol{\eta^{(m-1)}}
      ~|~
      \boldsymbol{y},
      \boldsymbol{u}^{(m)},
      \kappa_u^{(m)},
      \kappa_v^{(m)}
      \right)
  }
  \cdot
    \frac{
      q \left(
        \boldsymbol{\eta^{(m-1)}}
        ~|~
        \boldsymbol{z} = \boldsymbol{\eta}^*,
        \boldsymbol{y},
        \boldsymbol{u}^{(m)},
        \kappa_u^{(m)},
        \kappa_v^{(m)}
        \right)
    }{
      q \left(
        \boldsymbol{\eta^*}
        ~|~
        \boldsymbol{z} = \boldsymbol{\eta}^{(m - 1)},
        \boldsymbol{y},
        \boldsymbol{u}^{(m)},
        \kappa_u^{(m)},
        \kappa_v^{(m)}
        \right)
    }
\right)
$$

It should be noted that these density calculations will be performed in log-space going forwards.

The calculation of $\log{q(...)}$ is already handled by \texttt{draw_proposal_eta()}, but we must implement $\log{p(...)}$.
First notice that all the conditionals are equal in both $p$ function invocations, so we need not normalize the implementation of $\log{p(...)}$ as the normality constants cancel out. The implementation is therefore as follows

```{r}
eta_log_density <- function(eta, kappa_v, u, problem) {
  #' Proportional full conditional density for eta
  return(
    -0.5 * t(eta) %*% diag.spam(x = kappa_v, nrow = problem$n) %*% eta
    + t(eta) %*% (kappa_v * u)
    + t(eta) %*% problem$y
    - t(exp(eta)) %*% problem$E
  )
}
```

We can now implement the calculation of $\boldsymbol{\alpha}^{(m)}$.

```{r}
acceptance_probability <- function(proposal_eta, previous_eta, kappa_v, u, problem) {
  log_p_forward <- eta_log_density(
          eta = proposal_eta$sample,
          kappa_v = kappa_v,
          u = u,
          problem = problem
  )
  log_p_backward <- eta_log_density(
          eta = previous_eta$sample,
          kappa_v = kappa_v,
          u = u,
          problem = problem
  )
  
  log_q_forward <- proposal_eta$logprob
  log_q_backward <- draw_proposal_eta(z = proposal_eta$sample, u = u, kappa_v = kappa_v, problem = problem)$logprob
  
  alpha <- exp(log_p_forward + log_q_backward - log_p_backward - log_q_forward)
  
  if (alpha > 1) {
    return(1)
  }
  return(alpha)
}
```

### Implementing the MCMC algorithm

We now have all the required components required to implement the outer loop MCMC algorithm.
The algorithm requires initial values for $\boldsymbol{u}^{(1)}$ and $\boldsymbol{\eta}^{(1)}$.
We will set (rather arbitrarily) $\boldsymbol{u}^{(1)} = \vec{0}$, while setting the value of $\boldsymbol{\eta}^{(0)}$ by sampling our proposal density $q(...)$ with the following parameters

\begin{align*}
  \boldsymbol{z} \leftarrow& \vec{0} \\
  \boldsymbol{u} \leftarrow& \vec{0} \\
  \kappa_v \leftarrow& 500
\end{align*}

```{r, cache = TRUE}
MCMC <- function(steps, problem) {
  # Initial guess for parameters u and eta
  u <- c(rep_len(0.0, problem$n))
  eta <- draw_proposal_eta(z = u, u = u, kappa_v = 500, problem = problem)
  
  # Data structures for saving sample results
  kappa_us <- vector()
  kappa_vs <- vector()
  etas <- matrix(data = NA, nrow = steps, ncol = problem$n)
  us <- matrix(data = NA, nrow = steps, ncol = problem$n)
  alphas <- vector()
  
  # Save start time, used for tracking time usage of the MCMC algorithm
  start_time <- Sys.time()
  
  for (i in seq(1, steps)) {
    kappa_u <- draw_kappa_u(u = u, problem = problem)
    kappa_v <- draw_kappa_v(eta = eta$sample, u = u, problem = problem)
    u <- draw_u(kappa_v = kappa_v, kappa_u = kappa_u, eta = eta$sample, problem = problem)
    
    proposal_eta <- draw_proposal_eta(
            z = eta$sample,
            u = u,
            kappa_v = kappa_v,
            problem = problem
    )
    alpha <- acceptance_probability(
      proposal_eta = proposal_eta,
      previous_eta = eta,
      kappa_v = kappa_v,
      u = u,
      problem = problem
    )
    if (runif(1)[1] < alpha) {
      eta = proposal_eta
    }
    
    # Appending results
    kappa_us <- c(kappa_us, kappa_u)
    kappa_vs <- c(kappa_vs, kappa_v)
    alphas <- c(alphas, alpha)
    us[i,] = u
    etas[i,] = eta$sample
  }
  
  # Calculate the time used by the MCMC algorithm
  end_time <- Sys.time()
  time_used <- end_time - start_time
  time_used_per_step <- time_used / steps
  
  # Returning the final result in the form of a list
  result <- list(
    kappa_u = kappa_us,
    kappa_v = kappa_vs,
    u = us,
    eta = etas,
    alpha = alphas,
    time_used = time_used,
    time_used_per_step = time_used_per_step
  )
  return(result)
}
```

A posterior sample size of $M = 70~000$, after having a burn-in period of $1000$ steps, is sampled.
We will see shortly that this burn-in period is more than sufficient.

```{r, eval = FALSE}
set.seed(0)
M <- 70000
burnin <- 1000
result <- MCMC(steps = M + burnin, problem = problem)
```

```{r, show = FALSE, eval = FALSE}
# Run this line if it is the first time you have executed MCMC and want to save the result
save(result, list = c("result"), file = "data/MCMC_result.Rdata")
```

```{r, show = FALSE}
# Load the saved result for `result`
M <- 70000
burnin <- 1000
load(file = "data/MCMC_result.Rdata")
```

#### Time usage

The time used by the algorithm is part of the returned result.

```{r}
result$time_used
```

$71~000$ iterations of the algorithm uses approximately 10 minutes, which implies the following time usage per iteration

```{r}
milliseconds <- 1000 * 60 * as.numeric(result$time_used_per_step)
sprintf("~%1.1fms per iteration", milliseconds)
```

I.e., $\propto 8.6$ milliseconds per iteration, each iteration creating $2n + 3 = 1091$ double data points.

#### Acceptance rates

The post-burnin acceptance rates, $\boldsymbol{\alpha}$, for the Metropolis-Hastings step which samples $\boldsymbol{\eta}^{(m)}$ is also retrievable from the result.

```{r}
alpha <- result$alpha[-c(1:burnin)]
mean(alpha)
```

The acceptance probability is, on average, approximately $50\%$.
A more interesting overview of the distribution of $\alpha^{(m)}$ can be observed by plotting a histogram.

```{r}
binwidth <- 0.04
enframe(alpha) %>%
  ggplot() +
  aes(x = value) +
  geom_histogram(
    aes(y = binwidth * ..density..),
    boundary = 0,
    breaks = seq(0.0, 1.0, by = binwidth)
  ) +
  ylab("frequency") +
  xlab(expression(alpha)) +
  labs(
    title = "Histogram of calculated acceptance probabilities",
    caption = "The distribution of alpha values for all the post-burnin acceptance probabilities"
  )
```

As you can see from the histogram for $\boldsymbol{\alpha}$, most values for $\alpha$ are either close to or equal to $0$ or $1$.
This is the reason for the $\alpha$ mean being $\approx 0.5$. Half of the proposals are almost guaranteed to be accepted, while the other half is almost guaranteed not to. We can calculate the fraction of the acceptance probabilities that can be considered extreme, let's say $\alpha \notin [0.01, 0.99]$.

```{r}
extreme_values_count <- length(alpha[alpha < 0.01]) + length(alpha[alpha > 0.99])
extreme_values_count / length(alpha)
```

$\approx 92\%$ of the proposals are of that nature!

# Exercise 3 - Convergence diagnostics

We will now make an attempt at diagnosing the convergence of the samples generated from the Monte Carlo Markov Chain algorithm.
First, since $\boldsymbol{\eta}$ is decomposed as $\boldsymbol{u} + \boldsymbol{u}$, we can retrieve samples of $\boldsymbol{v} = \boldsymbol{\eta} - \boldsymbol{u}$.

```{r}
vs <- result$eta - result$u
```

We will investigate the precision parameters $\kappa_u$ and $\kappa_v$ in addition to three randomly chosen components of $\boldsymbol{u}$ and $\boldsymbol{v}$. The three components $\{80, 360, 500\}$ will be used for both $\boldsymbol{u}$ and $\boldsymbol{v}$. This data subset is inserted into a data frame.

```{r}
samples <- tibble(
  step = seq(1, burnin + M),
  kappa_u = result$kappa_u, kappa_v = result$kappa_v,
  u_1 = result$u[, 80], u_2 = result$u[, 360], u_3 = result$u[, 500],
  v_1 = vs[, 80], v_2 = vs[, 360], v_3 = vs[, 500]
)
params <- colnames(samples)[-1]
```

### 3a) Trace plots

First, we will plot the trace plot for all the chosen sample parameters.

```{r, fig.width = 8.3, fig.height = 11.7, fig.fullwidth=TRUE}
burnins <- seq(1, burnin)
samples %>%
  slice(-burnins) %>%
  gather(key, value, params) %>%
  ggplot(aes(x = step)) +
  geom_line(aes(y = value)) +
  facet_wrap(vars(key), scales = "free_y", ncol = 2)
```

### 3b) Autocorrelation plots

```{r, fig.width = 8.3, fig.height = 11.7, fig.fullwidth=TRUE}
par(mfrow=c(4,2))
for (param in params) {
  acf(samples[param])
}
```
 
### 3c) Convergence check with `geweke.diag()`
 
```{r}
library(coda)
library(kableExtra)

burnins <- seq(1, burnin)
effective_samples <- samples[-burnins, -1]
z_statistics <- geweke.diag(
  effective_samples,
  frac1=0.1, frac2=0.5
)$z
p_values <- 2 * pnorm(abs(z_statistics), lower.tail = FALSE)
geweke_results <- tibble(
  parameter = names(z_statistics),
  z_statistic = z_statistics,
  p_value = p_values
)
geweke_results %>% kable()

zstats <- geweke.diag(result$u[-burnins,])
pvals <- 2 * pnorm(abs(zstats$z), lower.tail = FALSE)
hist <- hist(pvals)

zstats2 <- geweke.diag(vs[-burnins,])
pvals2 <- 2 * pnorm(abs(zstats$z), lower.tail = FALSE)
hist2 <- hist(pvals2)
```
